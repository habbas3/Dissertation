from collections import Counter
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
import torch

# --- Data transforms ---
class Reshape:
    def __call__(self, x):
        return x.T  # (seq_len, features) -> (features, seq_len)

class Compose:
    def __init__(self, transforms):
        self.transforms = transforms
    def __call__(self, x):
        for t in self.transforms:
            x = t(x)
        return x

# --- Sequence generation ---
def build_sequences(df, feature_cols, label_col, seq_len=32):
    sequences, labels = [], []
    for _, group in df.groupby("filename"):
        group = group.sort_values("cycle_number")
        features = group[feature_cols].values
        labels_all = group[label_col].values
        for i in range(len(features) - seq_len + 1):   # fix: +1 to get last window
            seq = features[i:i+seq_len]                # shape [seq_len, features]
            label = labels_all[i+seq_len-1]            # fix: last label in the window
            sequences.append(seq)
            labels.append(label)
    return np.array(sequences), np.array(labels)


class BatteryDataset(Dataset):
    def __init__(self, sequences, labels, transform=None):
        self.sequences = sequences
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        x = self.sequences[idx]  # shape: (seq_len, n_features) or (n_features,)
        y = self.labels[idx]
        
        if x.ndim == 1:
            x = np.expand_dims(x, axis=1)  
        if x.ndim == 2:
            x = x.T  
        
        x = torch.from_numpy(x).float()
        y = torch.tensor(y).long()
        return x, y


# --- Main loader ---
def load_battery_dataset(
    csv_path,
    source_cathodes,
    target_cathodes=None,
    classification_label="eol_class",
    batch_size=64,
    sequence_length=32,
):
    
    df = pd.read_csv(csv_path)

    if classification_label not in df.columns:
        raise ValueError(f"‚ùå Missing classification label: {classification_label}")
    if "cathode" not in df.columns:
        raise ValueError("‚ùå 'cathode' column missing in CSV.")

    # Encode labels
    df[classification_label + "_encoded"] = LabelEncoder().fit_transform(df[classification_label])

    print("üî¢ Class distribution:\n", df[classification_label + "_encoded"].value_counts())
    print("üî¨ Cathode distribution:\n", df["cathode"].value_counts())
    df["cathode"] = df["cathode"].astype(str).str.strip()

    # ‚úÖ Handle both pretraining and transfer modes
    source_df = df[df["cathode"].isin(source_cathodes)].reset_index(drop=True)
    if source_df.empty:
        raise ValueError(f"‚ùå No rows found for source cathodes: {source_cathodes}")
    if target_cathodes is None or len(target_cathodes) == 0:
        print("üõ† Loading pretraining mode (source only, no target)")
        target_df = pd.DataFrame(columns=df.columns)
    else:
        target_df = df[df["cathode"].isin(target_cathodes)].reset_index(drop=True)
        if target_df.empty:
            print(f"‚ö†Ô∏è No rows found for target cathodes: {target_cathodes}")

    feature_cols = ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge',
                    'capacity_discharge', 'cycle_start', 'cycle_duration']

    scaler = StandardScaler()
    source_df[feature_cols] = scaler.fit_transform(source_df[feature_cols])
    if not target_df.empty:
        target_df[feature_cols] = scaler.transform(target_df[feature_cols])

    # --- Source split ---
    label_col = classification_label + "_encoded"
    file_labels = source_df.groupby("filename")[label_col].first()
    files = file_labels.index.to_numpy()
    labels = file_labels.values

    if len(np.unique(labels)) == 1 or np.min(np.bincount(labels)) < 2:
        print("‚ö†Ô∏è Not enough samples to stratify source files. Using all files for training.")
        train_files = files
        val_files = []

    else:
        train_files, val_files = train_test_split(
            files,
            test_size=0.2,
            stratify=labels,
            random_state=42,
        )

    train_df = source_df[source_df["filename"].isin(train_files)].reset_index(drop=True)
    val_df = source_df[source_df["filename"].isin(val_files)].reset_index(drop=True)
    X_train, y_train = build_sequences(train_df, feature_cols, label_col, seq_len=sequence_length)
    X_val, y_val = build_sequences(val_df, feature_cols, label_col, seq_len=sequence_length)
    
    if len(X_train) == 0:
        max_len = train_df.groupby("filename").size().max()
        if max_len and max_len < sequence_length:
            print(
                f"‚ö†Ô∏è No sequences with length {sequence_length}. Reducing to {max_len}."
            )
            sequence_length = max_len
            X_train, y_train = build_sequences(train_df, feature_cols, label_col, seq_len=sequence_length)
            X_val, y_val = build_sequences(val_df, feature_cols, label_col, seq_len=sequence_length)
        
    if len(X_train) == 0:
        raise ValueError(
            "No training sequences could be generated even after adjusting sequence_length. "
            f"Please check the dataset."
        )

    transform = Compose([Reshape()])  

    source_train = DataLoader(BatteryDataset(X_train, y_train, transform), batch_size=batch_size, shuffle=True)
    source_val   = DataLoader(BatteryDataset(X_val, y_val, transform), batch_size=batch_size, shuffle=False)

    # --- Target split ---
    if not target_df.empty:
        label_col = classification_label + "_encoded"
        tgt_file_labels = target_df.groupby("filename")[label_col].first()
        tgt_files = tgt_file_labels.index.to_numpy()
        tgt_labels = tgt_file_labels.values

        if len(np.unique(tgt_labels)) == 1 or np.min(np.bincount(tgt_labels)) < 2:
            print(f"‚ö†Ô∏è Not enough target samples to stratify. Using full data for training.")
            tgt_train_files = tgt_files
            tgt_val_files = []
        else:
            tgt_train_files, tgt_val_files = train_test_split(
                tgt_files,
                test_size=0.2,
                stratify=tgt_labels,
                random_state=42,
            )

        tgt_train_df = target_df[target_df["filename"].isin(tgt_train_files)].reset_index(drop=True)
        tgt_val_df = target_df[target_df["filename"].isin(tgt_val_files)].reset_index(drop=True)

        X_tgt_train, y_tgt_train = build_sequences(tgt_train_df, feature_cols, label_col, seq_len=sequence_length)
        X_tgt_val, y_tgt_val = build_sequences(tgt_val_df, feature_cols, label_col, seq_len=sequence_length)
        
        if len(X_tgt_train) == 0:
            max_len = tgt_train_df.groupby("filename").size().max()
            if max_len and max_len < sequence_length:
                print(
                    f"‚ö†Ô∏è No target sequences with length {sequence_length}. Reducing to {max_len}."
                )
                sequence_length = max_len
                X_tgt_train, y_tgt_train = build_sequences(tgt_train_df, feature_cols, label_col, seq_len=sequence_length)
                X_tgt_val, y_tgt_val = build_sequences(tgt_val_df, feature_cols, label_col, seq_len=sequence_length)

        
        if len(X_tgt_train) == 0:
            raise ValueError(
                "No target training sequences could be generated even after adjusting sequence_length. "
                "Please check the dataset."
            )
        

        target_train = DataLoader(BatteryDataset(X_tgt_train, y_tgt_train, transform), batch_size=batch_size, shuffle=True)
        target_val   = DataLoader(BatteryDataset(X_tgt_val, y_tgt_val, transform), batch_size=batch_size, shuffle=False)
    else:
        target_train, target_val = None, None

    label_names = sorted(df[classification_label].dropna().unique().tolist())

    print("üìä Source val class counts:", Counter(y_val))
    if not target_df.empty:
        print("üìä Target val class counts:", Counter(y_tgt_val))

    return source_train, source_val, target_train, target_val, label_names, df




12-10 13:33:22 data_name: CWRU_inconsistent
12-10 13:33:22 data_dir: ./my_datasets/CWRU_dataset
12-10 13:33:22 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
12-10 13:33:22 normlizetype: mean-std
12-10 13:33:22 method: sngp
12-10 13:33:22 gp_hidden_dim: 2048
12-10 13:33:22 spectral_norm_bound: 0.95
12-10 13:33:22 n_power_iterations: 1
12-10 13:33:22 nesterov: True
12-10 13:33:22 print_freq: 10
12-10 13:33:22 layers: 16
12-10 13:33:22 widen_factor: 1
12-10 13:33:22 droprate: 0.3
12-10 13:33:22 cuda_device: 0
12-10 13:33:22 checkpoint_dir: ./checkpoint
12-10 13:33:22 pretrained: False
12-10 13:33:22 batch_size: 64
12-10 13:33:22 warmup_epochs: 3
12-10 13:33:22 num_workers: 0
12-10 13:33:22 bottleneck: True
12-10 13:33:22 bottleneck_num: 128
12-10 13:33:22 last_batch: False
12-10 13:33:22 hidden_size: 1024
12-10 13:33:22 trade_off_adversarial: Step
12-10 13:33:22 lam_adversarial: 1
12-10 13:33:22 opt: adam
12-10 13:33:22 lr: 0.001
12-10 13:33:22 momentum: 0.9
12-10 13:33:22 weight_decay: 1e-05
12-10 13:33:22 lr_scheduler: step
12-10 13:33:22 gamma: 0.1
12-10 13:33:22 steps: 150, 250
12-10 13:33:22 middle_epoch: 15
12-10 13:33:22 max_epoch: 50
12-10 13:33:22 print_step: 25
12-10 13:33:22 inconsistent: UAN
12-10 13:33:22 model_name: cnn_openmax
12-10 13:33:22 th: 0.5
12-10 13:33:22 input_channels: 7
12-10 13:33:22 classification_label: eol_class
12-10 13:33:22 sequence_length: 32
12-10 13:33:22 cycles_per_file: 15
12-10 13:33:22 source_cycles_per_file: None
12-10 13:33:22 target_cycles_per_file: None
12-10 13:33:22 cycle_ablation: False
12-10 13:33:22 cycle_ablation_start: 5
12-10 13:33:22 cycle_ablation_step: 10
12-10 13:33:22 cycle_ablation_max: None
12-10 13:33:22 literature_context_file: ./references/joule_s2542-4351-22-00409-3.md
12-10 13:33:22 sample_random_state: 42
12-10 13:33:22 transfer_task: [[1], [0]]
12-10 13:33:22 source_cathode: []
12-10 13:33:22 target_cathode: []
12-10 13:33:22 num_classes: 9
12-10 13:33:22 domain_temperature: 1.0
12-10 13:33:22 class_temperature: 10.0
12-10 13:33:22 lambda_src: 1.0
12-10 13:33:22 lambda_src_decay_patience: 5
12-10 13:33:22 lambda_src_decay_factor: 0.5
12-10 13:33:22 lambda_src_min: 0.0
12-10 13:33:22 lambda_src_warmup: 0
12-10 13:33:22 improvement_metric: accuracy
12-10 13:33:22 skip_retry: False
12-10 13:33:22 auto_select: True
12-10 13:33:22 llm_compare: True
12-10 13:33:22 llm_backend: openai
12-10 13:33:22 llm_model: None
12-10 13:33:22 llm_context: 
12-10 13:33:22 llm_ablation: False
12-10 13:33:22 ablation_cycle_limits: 
12-10 13:33:22 llm_cfg_inputs: {'text_context': 'Dataset: Case Western Reserve University bearing vibration transfer benchmark with label inconsistency handling.\nTransfer from motors [0],[1] to [0],[2]; inconsistency setting UAN.\nWindows are length 32 with 7 vibration channels; task uses None classes.', 'numeric_summary': {'dataset': 'CWRU_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'cwru_bearing', 'transfer_task': [[[0], [1]], [[0], [2]], [[0], [3]], [[1], [0]], [[1], [2]], [[1], [3]], [[2], [0]], [[2], [1]], [[2], [3]], [[3], [0]], [[3], [1]], [[3], [2]]]}}
12-10 13:33:22 llm_cfg: {'architecture': 'cnn_openmax', 'model_name': 'cnn_openmax', 'self_attention': False, 'sngp': True, 'openmax': True, 'use_unknown_head': True, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 1.0, 'warmup_epochs': 5, 'rationale': 'The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation. Transfer 0→0:  (load 0, 0 HP, 1797 rpm) to  (load 0, 0 HP, 1797 rpm). Backbone frozen for 5 epoch(s) before full fine-tuning to stabilise feature reuse. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_openmax",\n  "self_attention": false,\n  "sngp": false,\n  "openmax": true,\n  "use_unknown_head": true,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 1.0,\n  "warmup_epochs": 5,\n  "rationale": "The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation."\n}'}
12-10 13:33:22 llm_cfg_stamp: 20251210_125835
12-10 13:33:22 using 1 cpu
12-10 14:28:21 data_name: CWRU_inconsistent
12-10 14:28:21 data_dir: ./my_datasets/CWRU_dataset
12-10 14:28:21 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
12-10 14:28:21 normlizetype: mean-std
12-10 14:28:21 method: sngp
12-10 14:28:21 gp_hidden_dim: 2048
12-10 14:28:21 spectral_norm_bound: 0.95
12-10 14:28:21 n_power_iterations: 1
12-10 14:28:21 nesterov: True
12-10 14:28:21 print_freq: 10
12-10 14:28:21 layers: 16
12-10 14:28:21 widen_factor: 1
12-10 14:28:21 droprate: 0.3
12-10 14:28:21 cuda_device: 0
12-10 14:28:21 checkpoint_dir: ./checkpoint
12-10 14:28:21 pretrained: False
12-10 14:28:21 batch_size: 64
12-10 14:28:21 warmup_epochs: 3
12-10 14:28:21 num_workers: 0
12-10 14:28:21 bottleneck: True
12-10 14:28:21 bottleneck_num: 128
12-10 14:28:21 last_batch: False
12-10 14:28:21 hidden_size: 1024
12-10 14:28:21 trade_off_adversarial: Step
12-10 14:28:21 lam_adversarial: 1
12-10 14:28:21 opt: adam
12-10 14:28:21 lr: 0.001
12-10 14:28:21 momentum: 0.9
12-10 14:28:21 weight_decay: 1e-05
12-10 14:28:21 lr_scheduler: step
12-10 14:28:21 gamma: 0.1
12-10 14:28:21 steps: 150, 250
12-10 14:28:21 middle_epoch: 15
12-10 14:28:21 max_epoch: 50
12-10 14:28:21 print_step: 25
12-10 14:28:21 inconsistent: UAN
12-10 14:28:21 model_name: cnn_openmax
12-10 14:28:21 th: 0.5
12-10 14:28:21 input_channels: 7
12-10 14:28:21 classification_label: eol_class
12-10 14:28:21 sequence_length: 32
12-10 14:28:21 cycles_per_file: 15
12-10 14:28:21 source_cycles_per_file: None
12-10 14:28:21 target_cycles_per_file: None
12-10 14:28:21 cycle_ablation: False
12-10 14:28:21 cycle_ablation_start: 5
12-10 14:28:21 cycle_ablation_step: 10
12-10 14:28:21 cycle_ablation_max: None
12-10 14:28:21 literature_context_file: ./references/joule_s2542-4351-22-00409-3.md
12-10 14:28:21 sample_random_state: 42
12-10 14:28:21 transfer_task: [[1], [2]]
12-10 14:28:21 source_cathode: []
12-10 14:28:21 target_cathode: []
12-10 14:28:21 num_classes: 9
12-10 14:28:21 domain_temperature: 1.0
12-10 14:28:21 class_temperature: 10.0
12-10 14:28:21 lambda_src: 1.0
12-10 14:28:21 lambda_src_decay_patience: 5
12-10 14:28:21 lambda_src_decay_factor: 0.5
12-10 14:28:21 lambda_src_min: 0.0
12-10 14:28:21 lambda_src_warmup: 0
12-10 14:28:21 improvement_metric: accuracy
12-10 14:28:21 skip_retry: False
12-10 14:28:21 auto_select: True
12-10 14:28:21 llm_compare: True
12-10 14:28:21 llm_backend: openai
12-10 14:28:21 llm_model: None
12-10 14:28:21 llm_context: 
12-10 14:28:21 llm_ablation: False
12-10 14:28:21 ablation_cycle_limits: 
12-10 14:28:21 llm_cfg_inputs: {'text_context': 'Dataset: Case Western Reserve University bearing vibration transfer benchmark with label inconsistency handling.\nTransfer from motors [0],[1] to [0],[2]; inconsistency setting UAN.\nWindows are length 32 with 7 vibration channels; task uses None classes.', 'numeric_summary': {'dataset': 'CWRU_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'cwru_bearing', 'transfer_task': [[[0], [1]], [[0], [2]], [[0], [3]], [[1], [0]], [[1], [2]], [[1], [3]], [[2], [0]], [[2], [1]], [[2], [3]], [[3], [0]], [[3], [1]], [[3], [2]]]}}
12-10 14:28:21 llm_cfg: {'architecture': 'cnn_openmax', 'model_name': 'cnn_openmax', 'self_attention': False, 'sngp': True, 'openmax': True, 'use_unknown_head': True, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 1.0, 'warmup_epochs': 5, 'rationale': 'The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation. Transfer 0→0:  (load 0, 0 HP, 1797 rpm) to  (load 0, 0 HP, 1797 rpm). Backbone frozen for 5 epoch(s) before full fine-tuning to stabilise feature reuse. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_openmax",\n  "self_attention": false,\n  "sngp": false,\n  "openmax": true,\n  "use_unknown_head": true,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 1.0,\n  "warmup_epochs": 5,\n  "rationale": "The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation."\n}'}
12-10 14:28:21 llm_cfg_stamp: 20251210_125835
12-10 14:28:21 using 1 cpu
12-10 14:38:31 data_name: CWRU_inconsistent
12-10 14:38:31 data_dir: ./my_datasets/CWRU_dataset
12-10 14:38:31 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
12-10 14:38:31 normlizetype: mean-std
12-10 14:38:31 method: sngp
12-10 14:38:31 gp_hidden_dim: 2048
12-10 14:38:31 spectral_norm_bound: 0.95
12-10 14:38:31 n_power_iterations: 1
12-10 14:38:31 nesterov: True
12-10 14:38:31 print_freq: 10
12-10 14:38:31 layers: 16
12-10 14:38:31 widen_factor: 1
12-10 14:38:31 droprate: 0.3
12-10 14:38:31 cuda_device: 0
12-10 14:38:31 checkpoint_dir: ./checkpoint
12-10 14:38:31 pretrained: False
12-10 14:38:31 batch_size: 64
12-10 14:38:31 warmup_epochs: 3
12-10 14:38:31 num_workers: 0
12-10 14:38:31 bottleneck: True
12-10 14:38:31 bottleneck_num: 128
12-10 14:38:31 last_batch: False
12-10 14:38:31 hidden_size: 1024
12-10 14:38:31 trade_off_adversarial: Step
12-10 14:38:31 lam_adversarial: 1
12-10 14:38:31 opt: adam
12-10 14:38:31 lr: 0.001
12-10 14:38:31 momentum: 0.9
12-10 14:38:31 weight_decay: 1e-05
12-10 14:38:31 lr_scheduler: step
12-10 14:38:31 gamma: 0.1
12-10 14:38:31 steps: 150, 250
12-10 14:38:31 middle_epoch: 15
12-10 14:38:31 max_epoch: 50
12-10 14:38:31 print_step: 25
12-10 14:38:31 inconsistent: UAN
12-10 14:38:31 model_name: cnn_openmax
12-10 14:38:31 th: 0.5
12-10 14:38:31 input_channels: 7
12-10 14:38:31 classification_label: eol_class
12-10 14:38:31 sequence_length: 32
12-10 14:38:31 cycles_per_file: 15
12-10 14:38:31 source_cycles_per_file: None
12-10 14:38:31 target_cycles_per_file: None
12-10 14:38:31 cycle_ablation: False
12-10 14:38:31 cycle_ablation_start: 5
12-10 14:38:31 cycle_ablation_step: 10
12-10 14:38:31 cycle_ablation_max: None
12-10 14:38:31 literature_context_file: ./references/joule_s2542-4351-22-00409-3.md
12-10 14:38:31 sample_random_state: 42
12-10 14:38:31 transfer_task: [[1], [3]]
12-10 14:38:31 source_cathode: []
12-10 14:38:31 target_cathode: []
12-10 14:38:31 num_classes: 9
12-10 14:38:31 domain_temperature: 1.0
12-10 14:38:31 class_temperature: 10.0
12-10 14:38:31 lambda_src: 1.0
12-10 14:38:31 lambda_src_decay_patience: 5
12-10 14:38:31 lambda_src_decay_factor: 0.5
12-10 14:38:31 lambda_src_min: 0.0
12-10 14:38:31 lambda_src_warmup: 0
12-10 14:38:31 improvement_metric: accuracy
12-10 14:38:31 skip_retry: False
12-10 14:38:31 auto_select: True
12-10 14:38:31 llm_compare: True
12-10 14:38:31 llm_backend: openai
12-10 14:38:31 llm_model: None
12-10 14:38:31 llm_context: 
12-10 14:38:31 llm_ablation: False
12-10 14:38:31 ablation_cycle_limits: 
12-10 14:38:31 llm_cfg_inputs: {'text_context': 'Dataset: Case Western Reserve University bearing vibration transfer benchmark with label inconsistency handling.\nTransfer from motors [0],[1] to [0],[2]; inconsistency setting UAN.\nWindows are length 32 with 7 vibration channels; task uses None classes.', 'numeric_summary': {'dataset': 'CWRU_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'cwru_bearing', 'transfer_task': [[[0], [1]], [[0], [2]], [[0], [3]], [[1], [0]], [[1], [2]], [[1], [3]], [[2], [0]], [[2], [1]], [[2], [3]], [[3], [0]], [[3], [1]], [[3], [2]]]}}
12-10 14:38:31 llm_cfg: {'architecture': 'cnn_openmax', 'model_name': 'cnn_openmax', 'self_attention': False, 'sngp': True, 'openmax': True, 'use_unknown_head': True, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 1.0, 'warmup_epochs': 5, 'rationale': 'The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation. Transfer 0→0:  (load 0, 0 HP, 1797 rpm) to  (load 0, 0 HP, 1797 rpm). Backbone frozen for 5 epoch(s) before full fine-tuning to stabilise feature reuse. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_openmax",\n  "self_attention": false,\n  "sngp": false,\n  "openmax": true,\n  "use_unknown_head": true,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 1.0,\n  "warmup_epochs": 5,\n  "rationale": "The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation."\n}'}
12-10 14:38:31 llm_cfg_stamp: 20251210_125835
12-10 14:38:31 using 1 cpu
12-10 19:20:56 data_name: CWRU_inconsistent
12-10 19:20:56 data_dir: ./my_datasets/CWRU_dataset
12-10 19:20:56 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
12-10 19:20:56 normlizetype: mean-std
12-10 19:20:56 method: deterministic
12-10 19:20:56 gp_hidden_dim: 2048
12-10 19:20:56 spectral_norm_bound: 0.95
12-10 19:20:56 n_power_iterations: 1
12-10 19:20:56 nesterov: True
12-10 19:20:56 print_freq: 10
12-10 19:20:56 layers: 16
12-10 19:20:56 widen_factor: 1
12-10 19:20:56 droprate: 0.3
12-10 19:20:56 cuda_device: 0
12-10 19:20:56 checkpoint_dir: ./checkpoint
12-10 19:20:56 pretrained: False
12-10 19:20:56 batch_size: 64
12-10 19:20:56 warmup_epochs: 3
12-10 19:20:56 num_workers: 0
12-10 19:20:56 bottleneck: True
12-10 19:20:56 bottleneck_num: 128
12-10 19:20:56 last_batch: False
12-10 19:20:56 hidden_size: 1024
12-10 19:20:56 trade_off_adversarial: Step
12-10 19:20:56 lam_adversarial: 1
12-10 19:20:56 opt: adam
12-10 19:20:56 lr: 0.001
12-10 19:20:56 momentum: 0.9
12-10 19:20:56 weight_decay: 1e-05
12-10 19:20:56 lr_scheduler: step
12-10 19:20:56 gamma: 0.1
12-10 19:20:56 steps: 150, 250
12-10 19:20:56 middle_epoch: 15
12-10 19:20:56 max_epoch: 50
12-10 19:20:56 print_step: 25
12-10 19:20:56 inconsistent: UAN
12-10 19:20:56 model_name: cnn_openmax
12-10 19:20:56 th: 0.5
12-10 19:20:56 input_channels: 7
12-10 19:20:56 classification_label: eol_class
12-10 19:20:56 sequence_length: 32
12-10 19:20:56 cycles_per_file: 15
12-10 19:20:56 source_cycles_per_file: None
12-10 19:20:56 target_cycles_per_file: None
12-10 19:20:56 cycle_ablation: False
12-10 19:20:56 cycle_ablation_start: 5
12-10 19:20:56 cycle_ablation_step: 10
12-10 19:20:56 cycle_ablation_max: None
12-10 19:20:56 literature_context_file: ./references/joule_s2542-4351-22-00409-3.md
12-10 19:20:56 sample_random_state: 42
12-10 19:20:56 transfer_task: [[1], [0]]
12-10 19:20:56 source_cathode: []
12-10 19:20:56 target_cathode: []
12-10 19:20:56 num_classes: 9
12-10 19:20:56 domain_temperature: 1.0
12-10 19:20:56 class_temperature: 10.0
12-10 19:20:56 lambda_src: 1.0
12-10 19:20:56 lambda_src_decay_patience: 5
12-10 19:20:56 lambda_src_decay_factor: 0.5
12-10 19:20:56 lambda_src_min: 0.0
12-10 19:20:56 lambda_src_warmup: 0
12-10 19:20:56 improvement_metric: accuracy
12-10 19:20:56 skip_retry: False
12-10 19:20:56 auto_select: True
12-10 19:20:56 llm_compare: True
12-10 19:20:56 llm_backend: openai
12-10 19:20:56 llm_model: None
12-10 19:20:56 llm_context: 
12-10 19:20:56 llm_ablation: False
12-10 19:20:56 ablation_cycle_limits: 
12-10 19:20:56 llm_cfg_inputs: {'text_context': 'Dataset: Case Western Reserve University bearing vibration transfer benchmark with label inconsistency handling.\nTransfer from motors [0],[1] to [0],[2]; inconsistency setting UAN.\nWindows are length 32 with 7 vibration channels; task uses None classes.', 'numeric_summary': {'dataset': 'CWRU_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'cwru_bearing', 'transfer_task': [[[0], [1]], [[0], [2]], [[0], [3]], [[1], [0]], [[1], [2]], [[1], [3]], [[2], [0]], [[2], [1]], [[2], [3]], [[3], [0]], [[3], [1]], [[3], [2]]]}}
12-10 19:20:56 llm_cfg: {'architecture': 'cnn_openmax', 'model_name': 'cnn_openmax', 'self_attention': False, 'sngp': True, 'openmax': True, 'use_unknown_head': True, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 1.0, 'warmup_epochs': 5, 'rationale': 'The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation. Transfer 0→0:  (load 0, 0 HP, 1797 rpm) to  (load 0, 0 HP, 1797 rpm). Backbone frozen for 5 epoch(s) before full fine-tuning to stabilise feature reuse. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_openmax",\n  "self_attention": false,\n  "sngp": false,\n  "openmax": true,\n  "use_unknown_head": true,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 1.0,\n  "warmup_epochs": 5,\n  "rationale": "The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation."\n}'}
12-10 19:20:56 llm_cfg_stamp: 20251210_125835
12-10 19:20:56 tag: no_sngp_20251210_125835
12-10 19:20:56 using 1 cpu
12-10 19:34:00 data_name: CWRU_inconsistent
12-10 19:34:00 data_dir: ./my_datasets/CWRU_dataset
12-10 19:34:00 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
12-10 19:34:00 normlizetype: mean-std
12-10 19:34:00 method: deterministic
12-10 19:34:00 gp_hidden_dim: 2048
12-10 19:34:00 spectral_norm_bound: 0.95
12-10 19:34:00 n_power_iterations: 1
12-10 19:34:00 nesterov: True
12-10 19:34:00 print_freq: 10
12-10 19:34:00 layers: 16
12-10 19:34:00 widen_factor: 1
12-10 19:34:00 droprate: 0.3
12-10 19:34:00 cuda_device: 0
12-10 19:34:00 checkpoint_dir: ./checkpoint
12-10 19:34:00 pretrained: False
12-10 19:34:00 batch_size: 64
12-10 19:34:00 warmup_epochs: 3
12-10 19:34:00 num_workers: 0
12-10 19:34:00 bottleneck: True
12-10 19:34:00 bottleneck_num: 128
12-10 19:34:00 last_batch: False
12-10 19:34:00 hidden_size: 1024
12-10 19:34:00 trade_off_adversarial: Step
12-10 19:34:00 lam_adversarial: 1
12-10 19:34:00 opt: adam
12-10 19:34:00 lr: 0.001
12-10 19:34:00 momentum: 0.9
12-10 19:34:00 weight_decay: 1e-05
12-10 19:34:00 lr_scheduler: step
12-10 19:34:00 gamma: 0.1
12-10 19:34:00 steps: 150, 250
12-10 19:34:00 middle_epoch: 15
12-10 19:34:00 max_epoch: 50
12-10 19:34:00 print_step: 25
12-10 19:34:00 inconsistent: UAN
12-10 19:34:00 model_name: cnn_openmax
12-10 19:34:00 th: 0.5
12-10 19:34:00 input_channels: 7
12-10 19:34:00 classification_label: eol_class
12-10 19:34:00 sequence_length: 32
12-10 19:34:00 cycles_per_file: 15
12-10 19:34:00 source_cycles_per_file: None
12-10 19:34:00 target_cycles_per_file: None
12-10 19:34:00 cycle_ablation: False
12-10 19:34:00 cycle_ablation_start: 5
12-10 19:34:00 cycle_ablation_step: 10
12-10 19:34:00 cycle_ablation_max: None
12-10 19:34:00 literature_context_file: ./references/joule_s2542-4351-22-00409-3.md
12-10 19:34:00 sample_random_state: 42
12-10 19:34:00 transfer_task: [[1], [2]]
12-10 19:34:00 source_cathode: []
12-10 19:34:00 target_cathode: []
12-10 19:34:00 num_classes: 9
12-10 19:34:00 domain_temperature: 1.0
12-10 19:34:00 class_temperature: 10.0
12-10 19:34:00 lambda_src: 1.0
12-10 19:34:00 lambda_src_decay_patience: 5
12-10 19:34:00 lambda_src_decay_factor: 0.5
12-10 19:34:00 lambda_src_min: 0.0
12-10 19:34:00 lambda_src_warmup: 0
12-10 19:34:00 improvement_metric: accuracy
12-10 19:34:00 skip_retry: False
12-10 19:34:00 auto_select: True
12-10 19:34:00 llm_compare: True
12-10 19:34:00 llm_backend: openai
12-10 19:34:00 llm_model: None
12-10 19:34:00 llm_context: 
12-10 19:34:00 llm_ablation: False
12-10 19:34:00 ablation_cycle_limits: 
12-10 19:34:00 llm_cfg_inputs: {'text_context': 'Dataset: Case Western Reserve University bearing vibration transfer benchmark with label inconsistency handling.\nTransfer from motors [0],[1] to [0],[2]; inconsistency setting UAN.\nWindows are length 32 with 7 vibration channels; task uses None classes.', 'numeric_summary': {'dataset': 'CWRU_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'cwru_bearing', 'transfer_task': [[[0], [1]], [[0], [2]], [[0], [3]], [[1], [0]], [[1], [2]], [[1], [3]], [[2], [0]], [[2], [1]], [[2], [3]], [[3], [0]], [[3], [1]], [[3], [2]]]}}
12-10 19:34:00 llm_cfg: {'architecture': 'cnn_openmax', 'model_name': 'cnn_openmax', 'self_attention': False, 'sngp': True, 'openmax': True, 'use_unknown_head': True, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 1.0, 'warmup_epochs': 5, 'rationale': 'The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation. Transfer 0→0:  (load 0, 0 HP, 1797 rpm) to  (load 0, 0 HP, 1797 rpm). Backbone frozen for 5 epoch(s) before full fine-tuning to stabilise feature reuse. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_openmax",\n  "self_attention": false,\n  "sngp": false,\n  "openmax": true,\n  "use_unknown_head": true,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 1.0,\n  "warmup_epochs": 5,\n  "rationale": "The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation."\n}'}
12-10 19:34:00 llm_cfg_stamp: 20251210_125835
12-10 19:34:00 tag: no_sngp_20251210_125835
12-10 19:34:00 using 1 cpu
12-10 19:44:04 data_name: CWRU_inconsistent
12-10 19:44:04 data_dir: ./my_datasets/CWRU_dataset
12-10 19:44:04 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
12-10 19:44:04 normlizetype: mean-std
12-10 19:44:04 method: deterministic
12-10 19:44:04 gp_hidden_dim: 2048
12-10 19:44:04 spectral_norm_bound: 0.95
12-10 19:44:04 n_power_iterations: 1
12-10 19:44:04 nesterov: True
12-10 19:44:04 print_freq: 10
12-10 19:44:04 layers: 16
12-10 19:44:04 widen_factor: 1
12-10 19:44:04 droprate: 0.3
12-10 19:44:04 cuda_device: 0
12-10 19:44:04 checkpoint_dir: ./checkpoint
12-10 19:44:04 pretrained: False
12-10 19:44:04 batch_size: 64
12-10 19:44:04 warmup_epochs: 3
12-10 19:44:04 num_workers: 0
12-10 19:44:04 bottleneck: True
12-10 19:44:04 bottleneck_num: 128
12-10 19:44:04 last_batch: False
12-10 19:44:04 hidden_size: 1024
12-10 19:44:04 trade_off_adversarial: Step
12-10 19:44:04 lam_adversarial: 1
12-10 19:44:04 opt: adam
12-10 19:44:04 lr: 0.001
12-10 19:44:04 momentum: 0.9
12-10 19:44:04 weight_decay: 1e-05
12-10 19:44:04 lr_scheduler: step
12-10 19:44:04 gamma: 0.1
12-10 19:44:04 steps: 150, 250
12-10 19:44:04 middle_epoch: 15
12-10 19:44:04 max_epoch: 50
12-10 19:44:04 print_step: 25
12-10 19:44:04 inconsistent: UAN
12-10 19:44:04 model_name: cnn_openmax
12-10 19:44:04 th: 0.5
12-10 19:44:04 input_channels: 7
12-10 19:44:04 classification_label: eol_class
12-10 19:44:04 sequence_length: 32
12-10 19:44:04 cycles_per_file: 15
12-10 19:44:04 source_cycles_per_file: None
12-10 19:44:04 target_cycles_per_file: None
12-10 19:44:04 cycle_ablation: False
12-10 19:44:04 cycle_ablation_start: 5
12-10 19:44:04 cycle_ablation_step: 10
12-10 19:44:04 cycle_ablation_max: None
12-10 19:44:04 literature_context_file: ./references/joule_s2542-4351-22-00409-3.md
12-10 19:44:04 sample_random_state: 42
12-10 19:44:04 transfer_task: [[1], [3]]
12-10 19:44:04 source_cathode: []
12-10 19:44:04 target_cathode: []
12-10 19:44:04 num_classes: 9
12-10 19:44:04 domain_temperature: 1.0
12-10 19:44:04 class_temperature: 10.0
12-10 19:44:04 lambda_src: 1.0
12-10 19:44:04 lambda_src_decay_patience: 5
12-10 19:44:04 lambda_src_decay_factor: 0.5
12-10 19:44:04 lambda_src_min: 0.0
12-10 19:44:04 lambda_src_warmup: 0
12-10 19:44:04 improvement_metric: accuracy
12-10 19:44:04 skip_retry: False
12-10 19:44:04 auto_select: True
12-10 19:44:04 llm_compare: True
12-10 19:44:04 llm_backend: openai
12-10 19:44:04 llm_model: None
12-10 19:44:04 llm_context: 
12-10 19:44:04 llm_ablation: False
12-10 19:44:04 ablation_cycle_limits: 
12-10 19:44:04 llm_cfg_inputs: {'text_context': 'Dataset: Case Western Reserve University bearing vibration transfer benchmark with label inconsistency handling.\nTransfer from motors [0],[1] to [0],[2]; inconsistency setting UAN.\nWindows are length 32 with 7 vibration channels; task uses None classes.', 'numeric_summary': {'dataset': 'CWRU_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'cwru_bearing', 'transfer_task': [[[0], [1]], [[0], [2]], [[0], [3]], [[1], [0]], [[1], [2]], [[1], [3]], [[2], [0]], [[2], [1]], [[2], [3]], [[3], [0]], [[3], [1]], [[3], [2]]]}}
12-10 19:44:04 llm_cfg: {'architecture': 'cnn_openmax', 'model_name': 'cnn_openmax', 'self_attention': False, 'sngp': True, 'openmax': True, 'use_unknown_head': True, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 1.0, 'warmup_epochs': 5, 'rationale': 'The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation. Transfer 0→0:  (load 0, 0 HP, 1797 rpm) to  (load 0, 0 HP, 1797 rpm). Backbone frozen for 5 epoch(s) before full fine-tuning to stabilise feature reuse. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_openmax",\n  "self_attention": false,\n  "sngp": false,\n  "openmax": true,\n  "use_unknown_head": true,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 1.0,\n  "warmup_epochs": 5,\n  "rationale": "The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation."\n}'}
12-10 19:44:04 llm_cfg_stamp: 20251210_125835
12-10 19:44:04 tag: no_sngp_20251210_125835
12-10 19:44:04 using 1 cpu

11-15 22:26:10 data_name: CWRU_inconsistent
11-15 22:26:10 data_dir: ./my_datasets/CWRU_dataset
11-15 22:26:10 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
11-15 22:26:10 normlizetype: mean-std
11-15 22:26:10 method: deterministic
11-15 22:26:10 gp_hidden_dim: 2048
11-15 22:26:10 spectral_norm_bound: 0.95
11-15 22:26:10 n_power_iterations: 1
11-15 22:26:10 nesterov: True
11-15 22:26:10 print_freq: 10
11-15 22:26:10 layers: 16
11-15 22:26:10 widen_factor: 1
11-15 22:26:10 droprate: 0.3
11-15 22:26:10 cuda_device: 0
11-15 22:26:10 checkpoint_dir: ./checkpoint
11-15 22:26:10 pretrained: False
11-15 22:26:10 batch_size: 64
11-15 22:26:10 num_workers: 0
11-15 22:26:10 bottleneck: True
11-15 22:26:10 bottleneck_num: 128
11-15 22:26:10 last_batch: False
11-15 22:26:10 hidden_size: 1024
11-15 22:26:10 trade_off_adversarial: Step
11-15 22:26:10 lam_adversarial: 1
11-15 22:26:10 opt: adam
11-15 22:26:10 lr: 0.0003
11-15 22:26:10 momentum: 0.9
11-15 22:26:10 weight_decay: 1e-05
11-15 22:26:10 lr_scheduler: step
11-15 22:26:10 gamma: 0.1
11-15 22:26:10 steps: 150, 250
11-15 22:26:10 middle_epoch: 15
11-15 22:26:10 max_epoch: 50
11-15 22:26:10 print_step: 25
11-15 22:26:10 inconsistent: UAN
11-15 22:26:10 model_name: cnn_features_1d
11-15 22:26:10 th: 0.5
11-15 22:26:10 input_channels: 7
11-15 22:26:10 classification_label: eol_class
11-15 22:26:10 sequence_length: 32
11-15 22:26:10 cycles_per_file: 50
11-15 22:26:10 sample_random_state: 42
11-15 22:26:10 transfer_task: [[1], [1]]
11-15 22:26:10 source_cathode: []
11-15 22:26:10 target_cathode: []
11-15 22:26:10 num_classes: 9
11-15 22:26:10 domain_temperature: 1.0
11-15 22:26:10 class_temperature: 10.0
11-15 22:26:10 lambda_src: 0.0
11-15 22:26:10 lambda_src_decay_patience: 5
11-15 22:26:10 lambda_src_decay_factor: 0.5
11-15 22:26:10 lambda_src_min: 0.0
11-15 22:26:10 lambda_src_warmup: 0
11-15 22:26:10 improvement_metric: common
11-15 22:26:10 auto_select: True
11-15 22:26:10 llm_compare: True
11-15 22:26:10 llm_backend: openai
11-15 22:26:10 llm_model: None
11-15 22:26:10 llm_context: 
11-15 22:26:10 llm_cfg_inputs: {'text_context': 'Dataset: Case Western Reserve University bearing vibration transfer benchmark with label inconsistency handling.\nTransfer from motors 0 to 1; inconsistency setting UAN.\nWindows are length 32 with 7 vibration channels; task uses None classes.', 'numeric_summary': {'dataset': 'CWRU_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'cwru_bearing', 'transfer_task': [[0], [1]]}}
11-15 22:26:10 llm_cfg: {'architecture': 'cnn_openmax', 'model_name': 'cnn_openmax', 'self_attention': False, 'sngp': True, 'openmax': True, 'use_unknown_head': True, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 1.0, 'rationale': "The dataset has short sequences (32 steps) and 7 channels, suitable for a 1-D CNN backbone. Label inconsistency and unknown classes motivate using OpenMax for explicit unknown rejection. The dropout of 0.3 balances overfitting risk due to label noise. Batch size 64 offers stable training without excessive memory use. Lambda_src at 1.0 encourages domain adaptation for transfer from motor 0 to 1. Label inconsistency flagged â†’ calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness. Benchmarking against Zhao et al.'s CNN baseline to highlight transfer-aware gains. CWRU bearings rewarded wideresnet capacity and SNGP calibration over the Zhao CNN baseline.", '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_openmax",\n  "self_attention": false,\n  "sngp": false,\n  "openmax": true,\n  "use_unknown_head": true,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 1.0,\n  "rationale": "The dataset has short sequences (32 steps) and 7 channels, suitable for a 1-D CNN backbone. Label inconsistency and unknown classes motivate using OpenMax for explicit unknown rejection. The dropout of 0.3 balances overfitting risk due to label noise. Batch size 64 offers stable training without excessive memory use. Lambda_src at 1.0 encourages domain adaptation for transfer from motor 0 to 1."\n}'}
11-15 22:26:10 llm_cfg_stamp: 20251115_222238
11-15 22:26:10 sngp: False
11-15 22:26:10 openmax: False
11-15 22:26:10 use_unknown_head: False
11-15 22:26:10 pretrained_model_path: None
11-15 22:26:10 using 1 cpu

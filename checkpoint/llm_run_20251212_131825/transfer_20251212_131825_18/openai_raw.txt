{
  "model_name": "cnn_features_1d_sa",
  "self_attention": true,
  "sngp": false,
  "openmax": false,
  "use_unknown_head": false,
  "bottleneck": 256,
  "dropout": 0.35,
  "learning_rate": 0.0005,
  "batch_size": 8,
  "lambda_src": 1.0,
  "warmup_epochs": 10,
  "rationale": "The dataset has 21 channels and short sequence length (32), favoring a CNN with self-attention to capture local and some global dependencies. Self-attention helps adapt to domain shift between source and target cathodes. Moderate bottleneck and dropout balance capacity and regularization given label inconsistency. OpenMax or SNGP are less critical here since the main challenge is transfer learning rather than open-set detection. Batch size 8 matches seen data, and warmup aids stable transfer."
}
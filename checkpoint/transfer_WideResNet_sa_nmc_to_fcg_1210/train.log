12-10 11:59:21 data_name: Battery_inconsistent
12-10 11:59:21 data_dir: ./my_datasets/Battery
12-10 11:59:21 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
12-10 11:59:21 normlizetype: mean-std
12-10 11:59:21 method: sngp
12-10 11:59:21 gp_hidden_dim: 2048
12-10 11:59:21 spectral_norm_bound: 0.95
12-10 11:59:21 n_power_iterations: 1
12-10 11:59:21 nesterov: True
12-10 11:59:21 print_freq: 10
12-10 11:59:21 layers: 16
12-10 11:59:21 widen_factor: 1
12-10 11:59:21 droprate: 0.3
12-10 11:59:21 cuda_device: 0
12-10 11:59:21 checkpoint_dir: ./checkpoint
12-10 11:59:21 pretrained: True
12-10 11:59:21 batch_size: 64
12-10 11:59:21 warmup_epochs: 4
12-10 11:59:21 num_workers: 0
12-10 11:59:21 bottleneck: True
12-10 11:59:21 bottleneck_num: 256
12-10 11:59:21 last_batch: False
12-10 11:59:21 hidden_size: 1024
12-10 11:59:21 trade_off_adversarial: Step
12-10 11:59:21 lam_adversarial: 1
12-10 11:59:21 opt: adam
12-10 11:59:21 lr: 0.0005
12-10 11:59:21 momentum: 0.9
12-10 11:59:21 weight_decay: 1e-05
12-10 11:59:21 lr_scheduler: step
12-10 11:59:21 gamma: 0.1
12-10 11:59:21 steps: 150, 250
12-10 11:59:21 middle_epoch: 15
12-10 11:59:21 max_epoch: 66
12-10 11:59:21 print_step: 25
12-10 11:59:21 inconsistent: UAN
12-10 11:59:21 model_name: WideResNet_sa
12-10 11:59:21 th: 0.5
12-10 11:59:21 input_channels: 7
12-10 11:59:21 classification_label: eol_class
12-10 11:59:21 sequence_length: 32
12-10 11:59:21 cycles_per_file: 15
12-10 11:59:21 source_cycles_per_file: None
12-10 11:59:21 target_cycles_per_file: None
12-10 11:59:21 cycle_ablation: False
12-10 11:59:21 cycle_ablation_start: 5
12-10 11:59:21 cycle_ablation_step: 10
12-10 11:59:21 cycle_ablation_max: None
12-10 11:59:21 literature_context_file: ./references/joule_s2542-4351-22-00409-3.md
12-10 11:59:21 sample_random_state: 42
12-10 11:59:21 transfer_task: [[[0], [1]], [[0], [2]], [[0], [3]], [[1], [0]], [[1], [2]], [[1], [3]], [[2], [0]], [[2], [1]], [[2], [3]], [[3], [0]], [[3], [1]], [[3], [2]]]
12-10 11:59:21 source_cathode: ['HE5050', 'NMC111', 'NMC532', 'NMC622', 'NMC811']
12-10 11:59:21 target_cathode: ['FCG']
12-10 11:59:21 num_classes: None
12-10 11:59:21 domain_temperature: 1.0
12-10 11:59:21 class_temperature: 10.0
12-10 11:59:21 lambda_src: 0.6
12-10 11:59:21 lambda_src_decay_patience: 2
12-10 11:59:21 lambda_src_decay_factor: 0.5
12-10 11:59:21 lambda_src_min: 0.0
12-10 11:59:21 lambda_src_warmup: 3
12-10 11:59:21 improvement_metric: accuracy
12-10 11:59:21 skip_retry: False
12-10 11:59:21 auto_select: True
12-10 11:59:21 llm_compare: True
12-10 11:59:21 llm_backend: openai
12-10 11:59:21 llm_model: None
12-10 11:59:21 llm_context: 
12-10 11:59:21 llm_ablation: False
12-10 11:59:21 ablation_cycle_limits: 
12-10 11:59:21 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: (all available); target cathodes: (none specified).\nLabel column 'eol_class' with ~None classes; sequence length 32; 7 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.\nLiterature cues (Joule S2542-4351(22)00409-3, chemistry-sensitive ablations):\nJoule 2022 (S2542-4351(22)00409-3) key cues for chemistry-aware ablation:\n- Early-cycle capacity fade slopes and coulombic-efficiency stabilization highlight whether aging is lithium-inventory loss (graphite/anode-electrolyte limited) versus cathode structural decay (e.g., Ni-rich NMC cathodes with electrolyte oxidation risk).\n- High initial impedance growth, thick SEI signatures, or electrolyte oxidation markers flag chemistries that benefit from uncertainty-aware heads (SNGP/OpenMax) to avoid overconfident extrapolation on outlier cycles.\n- LFP/graphite pairs tend to show flatter voltage plateaus and slower early fade; Ni-rich NMC chemistries degrade faster under aggressive cycling and need stronger regularization or shorter cycle horizons in ablations.\n- Transfer between mismatched chemistries (e.g., NMC → LFP) should down-weight source loss and favor architectures that adapt quickly with self-attention or wider bottlenecks; closely matched chemistries can rely more on convolutional baselines.", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': [], 'target_cathodes': [], 'label_column': 'eol_class'}}
12-10 11:59:21 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 256, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 1.0, 'warmup_epochs': 10, 'rationale': 'The dataset has short sequences (32 steps) and 7 channels, suitable for 1D CNN with self-attention to capture local and multi-cycle correlations. Label inconsistency and potential outliers suggest using SNGP and unknown head for uncertainty-aware predictions. Moderate bottleneck (256) balances capacity and overfitting risk. Dropout 0.3 prevents overfitting given label noise. Learning rate 0.001 and batch size 64 balance convergence and memory. Warmup epochs 10 help stabilize training with uncertainty modules. Backbone frozen for 10 epoch(s) before full fine-tuning to stabilise feature reuse. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": true,\n  "openmax": false,\n  "use_unknown_head": true,\n  "bottleneck": 256,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 1.0,\n  "warmup_epochs": 10,\n  "rationale": "The dataset has short sequences (32 steps) and 7 channels, suitable for 1D CNN with self-attention to capture local and multi-cycle correlations. Label inconsistency and potential outliers suggest using SNGP and unknown head for uncertainty-aware predictions. Moderate bottleneck (256) balances capacity and overfitting risk. Dropout 0.3 prevents overfitting given label noise. Learning rate 0.001 and batch size 64 balance convergence and memory. Warmup epochs 10 help stabilize training with uncertainty modules."\n}'}
12-10 11:59:21 llm_cfg_stamp: 20251210_111212
12-10 11:59:21 tag: sngp_wrn_sa_20251210_111212
12-10 11:59:21 pretrained_model_path: ./checkpoint/pretrain_WideResNet_sa_nmc_1210/best_model.pth
12-10 11:59:21 using 1 cpu
12-10 11:59:21 Reducing learning rate to 0.0001 for 5 target samples
12-10 11:59:21 Freezing early layers of backbone for 5 target samples

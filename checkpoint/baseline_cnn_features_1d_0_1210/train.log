12-10 17:02:25 data_name: CWRU_inconsistent
12-10 17:02:25 data_dir: ./my_datasets/CWRU_dataset
12-10 17:02:25 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
12-10 17:02:25 normlizetype: mean-std
12-10 17:02:25 method: deterministic
12-10 17:02:25 gp_hidden_dim: 2048
12-10 17:02:25 spectral_norm_bound: 0.95
12-10 17:02:25 n_power_iterations: 1
12-10 17:02:25 nesterov: True
12-10 17:02:25 print_freq: 10
12-10 17:02:25 layers: 16
12-10 17:02:25 widen_factor: 1
12-10 17:02:25 droprate: 0.3
12-10 17:02:25 cuda_device: 0
12-10 17:02:25 checkpoint_dir: ./checkpoint
12-10 17:02:25 pretrained: False
12-10 17:02:25 batch_size: 64
12-10 17:02:25 warmup_epochs: 3
12-10 17:02:25 num_workers: 0
12-10 17:02:25 bottleneck: True
12-10 17:02:25 bottleneck_num: 128
12-10 17:02:25 last_batch: False
12-10 17:02:25 hidden_size: 1024
12-10 17:02:25 trade_off_adversarial: Step
12-10 17:02:25 lam_adversarial: 1
12-10 17:02:25 opt: adam
12-10 17:02:25 lr: 0.0003
12-10 17:02:25 momentum: 0.9
12-10 17:02:25 weight_decay: 1e-05
12-10 17:02:25 lr_scheduler: step
12-10 17:02:25 gamma: 0.1
12-10 17:02:25 steps: 150, 250
12-10 17:02:25 middle_epoch: 15
12-10 17:02:25 max_epoch: 50
12-10 17:02:25 print_step: 25
12-10 17:02:25 inconsistent: UAN
12-10 17:02:25 model_name: cnn_features_1d
12-10 17:02:25 th: 0.5
12-10 17:02:25 input_channels: 7
12-10 17:02:25 classification_label: eol_class
12-10 17:02:25 sequence_length: 32
12-10 17:02:25 cycles_per_file: 15
12-10 17:02:25 source_cycles_per_file: None
12-10 17:02:25 target_cycles_per_file: None
12-10 17:02:25 cycle_ablation: False
12-10 17:02:25 cycle_ablation_start: 5
12-10 17:02:25 cycle_ablation_step: 10
12-10 17:02:25 cycle_ablation_max: None
12-10 17:02:25 literature_context_file: ./references/joule_s2542-4351-22-00409-3.md
12-10 17:02:25 sample_random_state: 42
12-10 17:02:25 transfer_task: [[0], [0]]
12-10 17:02:25 source_cathode: []
12-10 17:02:25 target_cathode: []
12-10 17:02:25 num_classes: 9
12-10 17:02:25 domain_temperature: 1.0
12-10 17:02:25 class_temperature: 10.0
12-10 17:02:25 lambda_src: 0.0
12-10 17:02:25 lambda_src_decay_patience: 5
12-10 17:02:25 lambda_src_decay_factor: 0.5
12-10 17:02:25 lambda_src_min: 0.0
12-10 17:02:25 lambda_src_warmup: 0
12-10 17:02:25 improvement_metric: accuracy
12-10 17:02:25 skip_retry: False
12-10 17:02:25 auto_select: True
12-10 17:02:25 llm_compare: True
12-10 17:02:25 llm_backend: openai
12-10 17:02:25 llm_model: None
12-10 17:02:25 llm_context: 
12-10 17:02:25 llm_ablation: False
12-10 17:02:25 ablation_cycle_limits: 
12-10 17:02:25 llm_cfg_inputs: {'text_context': 'Dataset: Case Western Reserve University bearing vibration transfer benchmark with label inconsistency handling.\nTransfer from motors [0],[1] to [0],[2]; inconsistency setting UAN.\nWindows are length 32 with 7 vibration channels; task uses None classes.', 'numeric_summary': {'dataset': 'CWRU_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'cwru_bearing', 'transfer_task': [[[0], [1]], [[0], [2]], [[0], [3]], [[1], [0]], [[1], [2]], [[1], [3]], [[2], [0]], [[2], [1]], [[2], [3]], [[3], [0]], [[3], [1]], [[3], [2]]]}}
12-10 17:02:25 llm_cfg: {'architecture': 'cnn_openmax', 'model_name': 'cnn_openmax', 'self_attention': False, 'sngp': True, 'openmax': True, 'use_unknown_head': True, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 1.0, 'warmup_epochs': 5, 'rationale': 'The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation. Transfer 0→0:  (load 0, 0 HP, 1797 rpm) to  (load 0, 0 HP, 1797 rpm). Backbone frozen for 5 epoch(s) before full fine-tuning to stabilise feature reuse. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_openmax",\n  "self_attention": false,\n  "sngp": false,\n  "openmax": true,\n  "use_unknown_head": true,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 1.0,\n  "warmup_epochs": 5,\n  "rationale": "The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation."\n}'}
12-10 17:02:25 llm_cfg_stamp: 20251210_125835
12-10 17:02:25 tag: no_openmax_20251210_125835
12-10 17:02:25 sngp: False
12-10 17:02:25 openmax: False
12-10 17:02:25 use_unknown_head: False
12-10 17:02:25 pretrained_model_path: None
12-10 17:02:25 using 1 cpu
12-10 17:35:22 data_name: CWRU_inconsistent
12-10 17:35:22 data_dir: ./my_datasets/CWRU_dataset
12-10 17:35:22 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
12-10 17:35:22 normlizetype: mean-std
12-10 17:35:22 method: deterministic
12-10 17:35:22 gp_hidden_dim: 2048
12-10 17:35:22 spectral_norm_bound: 0.95
12-10 17:35:22 n_power_iterations: 1
12-10 17:35:22 nesterov: True
12-10 17:35:22 print_freq: 10
12-10 17:35:22 layers: 16
12-10 17:35:22 widen_factor: 1
12-10 17:35:22 droprate: 0.3
12-10 17:35:22 cuda_device: 0
12-10 17:35:22 checkpoint_dir: ./checkpoint
12-10 17:35:22 pretrained: False
12-10 17:35:22 batch_size: 64
12-10 17:35:22 warmup_epochs: 3
12-10 17:35:22 num_workers: 0
12-10 17:35:22 bottleneck: True
12-10 17:35:22 bottleneck_num: 128
12-10 17:35:22 last_batch: False
12-10 17:35:22 hidden_size: 1024
12-10 17:35:22 trade_off_adversarial: Step
12-10 17:35:22 lam_adversarial: 1
12-10 17:35:22 opt: adam
12-10 17:35:22 lr: 0.0003
12-10 17:35:22 momentum: 0.9
12-10 17:35:22 weight_decay: 1e-05
12-10 17:35:22 lr_scheduler: step
12-10 17:35:22 gamma: 0.1
12-10 17:35:22 steps: 150, 250
12-10 17:35:22 middle_epoch: 15
12-10 17:35:22 max_epoch: 50
12-10 17:35:22 print_step: 25
12-10 17:35:22 inconsistent: UAN
12-10 17:35:22 model_name: cnn_features_1d
12-10 17:35:22 th: 0.5
12-10 17:35:22 input_channels: 7
12-10 17:35:22 classification_label: eol_class
12-10 17:35:22 sequence_length: 32
12-10 17:35:22 cycles_per_file: 15
12-10 17:35:22 source_cycles_per_file: None
12-10 17:35:22 target_cycles_per_file: None
12-10 17:35:22 cycle_ablation: False
12-10 17:35:22 cycle_ablation_start: 5
12-10 17:35:22 cycle_ablation_step: 10
12-10 17:35:22 cycle_ablation_max: None
12-10 17:35:22 literature_context_file: ./references/joule_s2542-4351-22-00409-3.md
12-10 17:35:22 sample_random_state: 42
12-10 17:35:22 transfer_task: [[0], [0]]
12-10 17:35:22 source_cathode: []
12-10 17:35:22 target_cathode: []
12-10 17:35:22 num_classes: 9
12-10 17:35:22 domain_temperature: 1.0
12-10 17:35:22 class_temperature: 10.0
12-10 17:35:22 lambda_src: 0.0
12-10 17:35:22 lambda_src_decay_patience: 5
12-10 17:35:22 lambda_src_decay_factor: 0.5
12-10 17:35:22 lambda_src_min: 0.0
12-10 17:35:22 lambda_src_warmup: 0
12-10 17:35:22 improvement_metric: accuracy
12-10 17:35:22 skip_retry: False
12-10 17:35:22 auto_select: True
12-10 17:35:22 llm_compare: True
12-10 17:35:22 llm_backend: openai
12-10 17:35:22 llm_model: None
12-10 17:35:22 llm_context: 
12-10 17:35:22 llm_ablation: False
12-10 17:35:22 ablation_cycle_limits: 
12-10 17:35:22 llm_cfg_inputs: {'text_context': 'Dataset: Case Western Reserve University bearing vibration transfer benchmark with label inconsistency handling.\nTransfer from motors [0],[1] to [0],[2]; inconsistency setting UAN.\nWindows are length 32 with 7 vibration channels; task uses None classes.', 'numeric_summary': {'dataset': 'CWRU_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'cwru_bearing', 'transfer_task': [[[0], [1]], [[0], [2]], [[0], [3]], [[1], [0]], [[1], [2]], [[1], [3]], [[2], [0]], [[2], [1]], [[2], [3]], [[3], [0]], [[3], [1]], [[3], [2]]]}}
12-10 17:35:22 llm_cfg: {'architecture': 'cnn_openmax', 'model_name': 'cnn_openmax', 'self_attention': False, 'sngp': True, 'openmax': True, 'use_unknown_head': True, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 1.0, 'warmup_epochs': 5, 'rationale': 'The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation. Transfer 0→0:  (load 0, 0 HP, 1797 rpm) to  (load 0, 0 HP, 1797 rpm). Backbone frozen for 5 epoch(s) before full fine-tuning to stabilise feature reuse. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_openmax",\n  "self_attention": false,\n  "sngp": false,\n  "openmax": true,\n  "use_unknown_head": true,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 1.0,\n  "warmup_epochs": 5,\n  "rationale": "The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation."\n}'}
12-10 17:35:22 llm_cfg_stamp: 20251210_125835
12-10 17:35:22 tag: no_openmax_20251210_125835
12-10 17:35:22 sngp: False
12-10 17:35:22 openmax: False
12-10 17:35:22 use_unknown_head: False
12-10 17:35:22 pretrained_model_path: None
12-10 17:35:22 using 1 cpu
12-10 18:17:10 data_name: CWRU_inconsistent
12-10 18:17:10 data_dir: ./my_datasets/CWRU_dataset
12-10 18:17:10 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
12-10 18:17:10 normlizetype: mean-std
12-10 18:17:10 method: deterministic
12-10 18:17:10 gp_hidden_dim: 2048
12-10 18:17:10 spectral_norm_bound: 0.95
12-10 18:17:10 n_power_iterations: 1
12-10 18:17:10 nesterov: True
12-10 18:17:10 print_freq: 10
12-10 18:17:10 layers: 16
12-10 18:17:10 widen_factor: 1
12-10 18:17:10 droprate: 0.3
12-10 18:17:10 cuda_device: 0
12-10 18:17:10 checkpoint_dir: ./checkpoint
12-10 18:17:10 pretrained: False
12-10 18:17:10 batch_size: 64
12-10 18:17:10 warmup_epochs: 3
12-10 18:17:10 num_workers: 0
12-10 18:17:10 bottleneck: True
12-10 18:17:10 bottleneck_num: 128
12-10 18:17:10 last_batch: False
12-10 18:17:10 hidden_size: 1024
12-10 18:17:10 trade_off_adversarial: Step
12-10 18:17:10 lam_adversarial: 1
12-10 18:17:10 opt: adam
12-10 18:17:10 lr: 0.0003
12-10 18:17:10 momentum: 0.9
12-10 18:17:10 weight_decay: 1e-05
12-10 18:17:10 lr_scheduler: step
12-10 18:17:10 gamma: 0.1
12-10 18:17:10 steps: 150, 250
12-10 18:17:10 middle_epoch: 15
12-10 18:17:10 max_epoch: 50
12-10 18:17:10 print_step: 25
12-10 18:17:10 inconsistent: UAN
12-10 18:17:10 model_name: cnn_features_1d
12-10 18:17:10 th: 0.5
12-10 18:17:10 input_channels: 7
12-10 18:17:10 classification_label: eol_class
12-10 18:17:10 sequence_length: 32
12-10 18:17:10 cycles_per_file: 15
12-10 18:17:10 source_cycles_per_file: None
12-10 18:17:10 target_cycles_per_file: None
12-10 18:17:10 cycle_ablation: False
12-10 18:17:10 cycle_ablation_start: 5
12-10 18:17:10 cycle_ablation_step: 10
12-10 18:17:10 cycle_ablation_max: None
12-10 18:17:10 literature_context_file: ./references/joule_s2542-4351-22-00409-3.md
12-10 18:17:10 sample_random_state: 42
12-10 18:17:10 transfer_task: [[0], [0]]
12-10 18:17:10 source_cathode: []
12-10 18:17:10 target_cathode: []
12-10 18:17:10 num_classes: 9
12-10 18:17:10 domain_temperature: 1.0
12-10 18:17:10 class_temperature: 10.0
12-10 18:17:10 lambda_src: 0.0
12-10 18:17:10 lambda_src_decay_patience: 5
12-10 18:17:10 lambda_src_decay_factor: 0.5
12-10 18:17:10 lambda_src_min: 0.0
12-10 18:17:10 lambda_src_warmup: 0
12-10 18:17:10 improvement_metric: accuracy
12-10 18:17:10 skip_retry: False
12-10 18:17:10 auto_select: True
12-10 18:17:10 llm_compare: True
12-10 18:17:10 llm_backend: openai
12-10 18:17:10 llm_model: None
12-10 18:17:10 llm_context: 
12-10 18:17:10 llm_ablation: False
12-10 18:17:10 ablation_cycle_limits: 
12-10 18:17:10 llm_cfg_inputs: {'text_context': 'Dataset: Case Western Reserve University bearing vibration transfer benchmark with label inconsistency handling.\nTransfer from motors [0],[1] to [0],[2]; inconsistency setting UAN.\nWindows are length 32 with 7 vibration channels; task uses None classes.', 'numeric_summary': {'dataset': 'CWRU_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'cwru_bearing', 'transfer_task': [[[0], [1]], [[0], [2]], [[0], [3]], [[1], [0]], [[1], [2]], [[1], [3]], [[2], [0]], [[2], [1]], [[2], [3]], [[3], [0]], [[3], [1]], [[3], [2]]]}}
12-10 18:17:10 llm_cfg: {'architecture': 'cnn_openmax', 'model_name': 'cnn_openmax', 'self_attention': False, 'sngp': True, 'openmax': True, 'use_unknown_head': True, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 1.0, 'warmup_epochs': 5, 'rationale': 'The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation. Transfer 0→0:  (load 0, 0 HP, 1797 rpm) to  (load 0, 0 HP, 1797 rpm). Backbone frozen for 5 epoch(s) before full fine-tuning to stabilise feature reuse. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_openmax",\n  "self_attention": false,\n  "sngp": false,\n  "openmax": true,\n  "use_unknown_head": true,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 1.0,\n  "warmup_epochs": 5,\n  "rationale": "The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation."\n}'}
12-10 18:17:10 llm_cfg_stamp: 20251210_125835
12-10 18:17:10 tag: no_openmax_20251210_125835
12-10 18:17:10 sngp: False
12-10 18:17:10 openmax: False
12-10 18:17:10 use_unknown_head: False
12-10 18:17:10 pretrained_model_path: None
12-10 18:17:10 using 1 cpu
12-10 21:38:00 data_name: CWRU_inconsistent
12-10 21:38:00 data_dir: ./my_datasets/CWRU_dataset
12-10 21:38:00 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
12-10 21:38:00 normlizetype: mean-std
12-10 21:38:00 method: deterministic
12-10 21:38:00 gp_hidden_dim: 2048
12-10 21:38:00 spectral_norm_bound: 0.95
12-10 21:38:00 n_power_iterations: 1
12-10 21:38:00 nesterov: True
12-10 21:38:00 print_freq: 10
12-10 21:38:00 layers: 16
12-10 21:38:00 widen_factor: 1
12-10 21:38:00 droprate: 0.3
12-10 21:38:00 cuda_device: 0
12-10 21:38:00 checkpoint_dir: ./checkpoint
12-10 21:38:00 pretrained: False
12-10 21:38:00 batch_size: 64
12-10 21:38:00 warmup_epochs: 3
12-10 21:38:00 num_workers: 0
12-10 21:38:00 bottleneck: True
12-10 21:38:00 bottleneck_num: 128
12-10 21:38:00 last_batch: False
12-10 21:38:00 hidden_size: 1024
12-10 21:38:00 trade_off_adversarial: Step
12-10 21:38:00 lam_adversarial: 1
12-10 21:38:00 opt: adam
12-10 21:38:00 lr: 0.0003
12-10 21:38:00 momentum: 0.9
12-10 21:38:00 weight_decay: 1e-05
12-10 21:38:00 lr_scheduler: step
12-10 21:38:00 gamma: 0.1
12-10 21:38:00 steps: 150, 250
12-10 21:38:00 middle_epoch: 15
12-10 21:38:00 max_epoch: 50
12-10 21:38:00 print_step: 25
12-10 21:38:00 inconsistent: UAN
12-10 21:38:00 model_name: cnn_features_1d
12-10 21:38:00 th: 0.5
12-10 21:38:00 input_channels: 7
12-10 21:38:00 classification_label: eol_class
12-10 21:38:00 sequence_length: 32
12-10 21:38:00 cycles_per_file: 15
12-10 21:38:00 source_cycles_per_file: None
12-10 21:38:00 target_cycles_per_file: None
12-10 21:38:00 cycle_ablation: False
12-10 21:38:00 cycle_ablation_start: 5
12-10 21:38:00 cycle_ablation_step: 10
12-10 21:38:00 cycle_ablation_max: None
12-10 21:38:00 literature_context_file: ./references/joule_s2542-4351-22-00409-3.md
12-10 21:38:00 sample_random_state: 42
12-10 21:38:00 transfer_task: [[0], [0]]
12-10 21:38:00 source_cathode: []
12-10 21:38:00 target_cathode: []
12-10 21:38:00 num_classes: 9
12-10 21:38:00 domain_temperature: 1.0
12-10 21:38:00 class_temperature: 10.0
12-10 21:38:00 lambda_src: 0.0
12-10 21:38:00 lambda_src_decay_patience: 5
12-10 21:38:00 lambda_src_decay_factor: 0.5
12-10 21:38:00 lambda_src_min: 0.0
12-10 21:38:00 lambda_src_warmup: 0
12-10 21:38:00 improvement_metric: accuracy
12-10 21:38:00 skip_retry: False
12-10 21:38:00 auto_select: True
12-10 21:38:00 llm_compare: True
12-10 21:38:00 llm_backend: openai
12-10 21:38:00 llm_model: None
12-10 21:38:00 llm_context: 
12-10 21:38:00 llm_ablation: False
12-10 21:38:00 ablation_cycle_limits: 
12-10 21:38:00 llm_cfg_inputs: {'text_context': 'Dataset: Case Western Reserve University bearing vibration transfer benchmark with label inconsistency handling.\nTransfer from motors [0],[1] to [0],[2]; inconsistency setting UAN.\nWindows are length 32 with 7 vibration channels; task uses None classes.', 'numeric_summary': {'dataset': 'CWRU_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'cwru_bearing', 'transfer_task': [[[0], [1]], [[0], [2]], [[0], [3]], [[1], [0]], [[1], [2]], [[1], [3]], [[2], [0]], [[2], [1]], [[2], [3]], [[3], [0]], [[3], [1]], [[3], [2]]]}}
12-10 21:38:00 llm_cfg: {'architecture': 'cnn_openmax', 'model_name': 'cnn_openmax', 'self_attention': False, 'sngp': True, 'openmax': True, 'use_unknown_head': True, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 1.0, 'warmup_epochs': 5, 'rationale': 'The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation. Transfer 0→0:  (load 0, 0 HP, 1797 rpm) to  (load 0, 0 HP, 1797 rpm). Backbone frozen for 5 epoch(s) before full fine-tuning to stabilise feature reuse. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_openmax",\n  "self_attention": false,\n  "sngp": false,\n  "openmax": true,\n  "use_unknown_head": true,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 1.0,\n  "warmup_epochs": 5,\n  "rationale": "The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation."\n}'}
12-10 21:38:00 llm_cfg_stamp: 20251210_125835
12-10 21:38:00 tag: detcnn_20251210_125835
12-10 21:38:00 sngp: False
12-10 21:38:00 openmax: False
12-10 21:38:00 use_unknown_head: False
12-10 21:38:00 pretrained_model_path: None
12-10 21:38:00 using 1 cpu
12-10 21:50:59 data_name: CWRU_inconsistent
12-10 21:50:59 data_dir: ./my_datasets/CWRU_dataset
12-10 21:50:59 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
12-10 21:50:59 normlizetype: mean-std
12-10 21:50:59 method: deterministic
12-10 21:50:59 gp_hidden_dim: 2048
12-10 21:50:59 spectral_norm_bound: 0.95
12-10 21:50:59 n_power_iterations: 1
12-10 21:50:59 nesterov: True
12-10 21:50:59 print_freq: 10
12-10 21:50:59 layers: 16
12-10 21:50:59 widen_factor: 1
12-10 21:50:59 droprate: 0.3
12-10 21:50:59 cuda_device: 0
12-10 21:50:59 checkpoint_dir: ./checkpoint
12-10 21:50:59 pretrained: False
12-10 21:50:59 batch_size: 64
12-10 21:50:59 warmup_epochs: 3
12-10 21:50:59 num_workers: 0
12-10 21:50:59 bottleneck: True
12-10 21:50:59 bottleneck_num: 128
12-10 21:50:59 last_batch: False
12-10 21:50:59 hidden_size: 1024
12-10 21:50:59 trade_off_adversarial: Step
12-10 21:50:59 lam_adversarial: 1
12-10 21:50:59 opt: adam
12-10 21:50:59 lr: 0.0003
12-10 21:50:59 momentum: 0.9
12-10 21:50:59 weight_decay: 1e-05
12-10 21:50:59 lr_scheduler: step
12-10 21:50:59 gamma: 0.1
12-10 21:50:59 steps: 150, 250
12-10 21:50:59 middle_epoch: 15
12-10 21:50:59 max_epoch: 50
12-10 21:50:59 print_step: 25
12-10 21:50:59 inconsistent: UAN
12-10 21:50:59 model_name: cnn_features_1d
12-10 21:50:59 th: 0.5
12-10 21:50:59 input_channels: 7
12-10 21:50:59 classification_label: eol_class
12-10 21:50:59 sequence_length: 32
12-10 21:50:59 cycles_per_file: 15
12-10 21:50:59 source_cycles_per_file: None
12-10 21:50:59 target_cycles_per_file: None
12-10 21:50:59 cycle_ablation: False
12-10 21:50:59 cycle_ablation_start: 5
12-10 21:50:59 cycle_ablation_step: 10
12-10 21:50:59 cycle_ablation_max: None
12-10 21:50:59 literature_context_file: ./references/joule_s2542-4351-22-00409-3.md
12-10 21:50:59 sample_random_state: 42
12-10 21:50:59 transfer_task: [[0], [0]]
12-10 21:50:59 source_cathode: []
12-10 21:50:59 target_cathode: []
12-10 21:50:59 num_classes: 9
12-10 21:50:59 domain_temperature: 1.0
12-10 21:50:59 class_temperature: 10.0
12-10 21:50:59 lambda_src: 0.0
12-10 21:50:59 lambda_src_decay_patience: 5
12-10 21:50:59 lambda_src_decay_factor: 0.5
12-10 21:50:59 lambda_src_min: 0.0
12-10 21:50:59 lambda_src_warmup: 0
12-10 21:50:59 improvement_metric: accuracy
12-10 21:50:59 skip_retry: False
12-10 21:50:59 auto_select: True
12-10 21:50:59 llm_compare: True
12-10 21:50:59 llm_backend: openai
12-10 21:50:59 llm_model: None
12-10 21:50:59 llm_context: 
12-10 21:50:59 llm_ablation: False
12-10 21:50:59 ablation_cycle_limits: 
12-10 21:50:59 llm_cfg_inputs: {'text_context': 'Dataset: Case Western Reserve University bearing vibration transfer benchmark with label inconsistency handling.\nTransfer from motors [0],[1] to [0],[2]; inconsistency setting UAN.\nWindows are length 32 with 7 vibration channels; task uses None classes.', 'numeric_summary': {'dataset': 'CWRU_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'cwru_bearing', 'transfer_task': [[[0], [1]], [[0], [2]], [[0], [3]], [[1], [0]], [[1], [2]], [[1], [3]], [[2], [0]], [[2], [1]], [[2], [3]], [[3], [0]], [[3], [1]], [[3], [2]]]}}
12-10 21:50:59 llm_cfg: {'architecture': 'cnn_openmax', 'model_name': 'cnn_openmax', 'self_attention': False, 'sngp': True, 'openmax': True, 'use_unknown_head': True, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 1.0, 'warmup_epochs': 5, 'rationale': 'The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation. Transfer 0→0:  (load 0, 0 HP, 1797 rpm) to  (load 0, 0 HP, 1797 rpm). Backbone frozen for 5 epoch(s) before full fine-tuning to stabilise feature reuse. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_openmax",\n  "self_attention": false,\n  "sngp": false,\n  "openmax": true,\n  "use_unknown_head": true,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 1.0,\n  "warmup_epochs": 5,\n  "rationale": "The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation."\n}'}
12-10 21:50:59 llm_cfg_stamp: 20251210_125835
12-10 21:50:59 tag: detcnn_20251210_125835
12-10 21:50:59 sngp: False
12-10 21:50:59 openmax: False
12-10 21:50:59 use_unknown_head: False
12-10 21:50:59 pretrained_model_path: None
12-10 21:50:59 using 1 cpu
12-10 22:12:27 data_name: CWRU_inconsistent
12-10 22:12:27 data_dir: ./my_datasets/CWRU_dataset
12-10 22:12:27 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
12-10 22:12:27 normlizetype: mean-std
12-10 22:12:27 method: deterministic
12-10 22:12:27 gp_hidden_dim: 2048
12-10 22:12:27 spectral_norm_bound: 0.95
12-10 22:12:27 n_power_iterations: 1
12-10 22:12:27 nesterov: True
12-10 22:12:27 print_freq: 10
12-10 22:12:27 layers: 16
12-10 22:12:27 widen_factor: 1
12-10 22:12:27 droprate: 0.3
12-10 22:12:27 cuda_device: 0
12-10 22:12:27 checkpoint_dir: ./checkpoint
12-10 22:12:27 pretrained: False
12-10 22:12:27 batch_size: 64
12-10 22:12:27 warmup_epochs: 3
12-10 22:12:27 num_workers: 0
12-10 22:12:27 bottleneck: True
12-10 22:12:27 bottleneck_num: 128
12-10 22:12:27 last_batch: False
12-10 22:12:27 hidden_size: 1024
12-10 22:12:27 trade_off_adversarial: Step
12-10 22:12:27 lam_adversarial: 1
12-10 22:12:27 opt: adam
12-10 22:12:27 lr: 0.0003
12-10 22:12:27 momentum: 0.9
12-10 22:12:27 weight_decay: 1e-05
12-10 22:12:27 lr_scheduler: step
12-10 22:12:27 gamma: 0.1
12-10 22:12:27 steps: 150, 250
12-10 22:12:27 middle_epoch: 15
12-10 22:12:27 max_epoch: 50
12-10 22:12:27 print_step: 25
12-10 22:12:27 inconsistent: UAN
12-10 22:12:27 model_name: cnn_features_1d
12-10 22:12:27 th: 0.5
12-10 22:12:27 input_channels: 7
12-10 22:12:27 classification_label: eol_class
12-10 22:12:27 sequence_length: 32
12-10 22:12:27 cycles_per_file: 15
12-10 22:12:27 source_cycles_per_file: None
12-10 22:12:27 target_cycles_per_file: None
12-10 22:12:27 cycle_ablation: False
12-10 22:12:27 cycle_ablation_start: 5
12-10 22:12:27 cycle_ablation_step: 10
12-10 22:12:27 cycle_ablation_max: None
12-10 22:12:27 literature_context_file: ./references/joule_s2542-4351-22-00409-3.md
12-10 22:12:27 sample_random_state: 42
12-10 22:12:27 transfer_task: [[0], [0]]
12-10 22:12:27 source_cathode: []
12-10 22:12:27 target_cathode: []
12-10 22:12:27 num_classes: 9
12-10 22:12:27 domain_temperature: 1.0
12-10 22:12:27 class_temperature: 10.0
12-10 22:12:27 lambda_src: 0.0
12-10 22:12:27 lambda_src_decay_patience: 5
12-10 22:12:27 lambda_src_decay_factor: 0.5
12-10 22:12:27 lambda_src_min: 0.0
12-10 22:12:27 lambda_src_warmup: 0
12-10 22:12:27 improvement_metric: accuracy
12-10 22:12:27 skip_retry: False
12-10 22:12:27 auto_select: True
12-10 22:12:27 llm_compare: True
12-10 22:12:27 llm_backend: openai
12-10 22:12:27 llm_model: None
12-10 22:12:27 llm_context: 
12-10 22:12:27 llm_ablation: False
12-10 22:12:27 ablation_cycle_limits: 
12-10 22:12:27 llm_cfg_inputs: {'text_context': 'Dataset: Case Western Reserve University bearing vibration transfer benchmark with label inconsistency handling.\nTransfer from motors [0],[1] to [0],[2]; inconsistency setting UAN.\nWindows are length 32 with 7 vibration channels; task uses None classes.', 'numeric_summary': {'dataset': 'CWRU_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'cwru_bearing', 'transfer_task': [[[0], [1]], [[0], [2]], [[0], [3]], [[1], [0]], [[1], [2]], [[1], [3]], [[2], [0]], [[2], [1]], [[2], [3]], [[3], [0]], [[3], [1]], [[3], [2]]]}}
12-10 22:12:27 llm_cfg: {'architecture': 'cnn_openmax', 'model_name': 'cnn_openmax', 'self_attention': False, 'sngp': True, 'openmax': True, 'use_unknown_head': True, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 1.0, 'warmup_epochs': 5, 'rationale': 'The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation. Transfer 0→0:  (load 0, 0 HP, 1797 rpm) to  (load 0, 0 HP, 1797 rpm). Backbone frozen for 5 epoch(s) before full fine-tuning to stabilise feature reuse. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_openmax",\n  "self_attention": false,\n  "sngp": false,\n  "openmax": true,\n  "use_unknown_head": true,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 1.0,\n  "warmup_epochs": 5,\n  "rationale": "The dataset has short sequences (32 steps) with 7 channels and label inconsistency requiring open-set handling. cnn_openmax adds calibrated unknown rejection suitable for label-inconsistent CWRU data, improving over the baseline deterministic CNN. Moderate bottleneck and dropout prevent overfitting, while a batch size of 64 balances compute and stability. Warmup and lambda_src support transfer learning adaptation."\n}'}
12-10 22:12:27 llm_cfg_stamp: 20251210_125835
12-10 22:12:27 tag: detcnn_20251210_125835
12-10 22:12:27 sngp: False
12-10 22:12:27 openmax: False
12-10 22:12:27 use_unknown_head: False
12-10 22:12:27 pretrained_model_path: None
12-10 22:12:27 using 1 cpu

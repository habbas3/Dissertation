11-23 11:13:16 data_name: Battery_inconsistent
11-23 11:13:16 data_dir: ./my_datasets/Battery
11-23 11:13:16 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
11-23 11:13:16 normlizetype: mean-std
11-23 11:13:16 method: sngp
11-23 11:13:16 gp_hidden_dim: 2048
11-23 11:13:16 spectral_norm_bound: 0.95
11-23 11:13:16 n_power_iterations: 1
11-23 11:13:16 nesterov: True
11-23 11:13:16 print_freq: 10
11-23 11:13:16 layers: 16
11-23 11:13:16 widen_factor: 1
11-23 11:13:16 droprate: 0.3
11-23 11:13:16 cuda_device: 0
11-23 11:13:16 checkpoint_dir: ./checkpoint
11-23 11:13:16 pretrained: False
11-23 11:13:16 batch_size: 64
11-23 11:13:16 warmup_epochs: 3
11-23 11:13:16 num_workers: 0
11-23 11:13:16 bottleneck: True
11-23 11:13:16 bottleneck_num: 128
11-23 11:13:16 last_batch: False
11-23 11:13:16 hidden_size: 1024
11-23 11:13:16 trade_off_adversarial: Step
11-23 11:13:16 lam_adversarial: 1
11-23 11:13:16 opt: adam
11-23 11:13:16 lr: 0.001
11-23 11:13:16 momentum: 0.9
11-23 11:13:16 weight_decay: 1e-05
11-23 11:13:16 lr_scheduler: step
11-23 11:13:16 gamma: 0.1
11-23 11:13:16 steps: 150, 250
11-23 11:13:16 middle_epoch: 15
11-23 11:13:16 max_epoch: 50
11-23 11:13:16 print_step: 25
11-23 11:13:16 inconsistent: UAN
11-23 11:13:16 model_name: cnn_features_1d_sa
11-23 11:13:16 th: 0.5
11-23 11:13:16 input_channels: 7
11-23 11:13:16 classification_label: eol_class
11-23 11:13:16 sequence_length: 32
11-23 11:13:16 cycles_per_file: 15
11-23 11:13:16 source_cycles_per_file: None
11-23 11:13:16 target_cycles_per_file: None
11-23 11:13:16 sample_random_state: 42
11-23 11:13:16 transfer_task: [[0], [1]]
11-23 11:13:16 source_cathode: ['5Vspinel']
11-23 11:13:16 target_cathode: []
11-23 11:13:16 num_classes: None
11-23 11:13:16 domain_temperature: 1.0
11-23 11:13:16 class_temperature: 10.0
11-23 11:13:16 lambda_src: 0.0
11-23 11:13:16 lambda_src_decay_patience: 5
11-23 11:13:16 lambda_src_decay_factor: 0.5
11-23 11:13:16 lambda_src_min: 0.0
11-23 11:13:16 lambda_src_warmup: 0
11-23 11:13:16 improvement_metric: common
11-23 11:13:16 auto_select: True
11-23 11:13:16 llm_compare: True
11-23 11:13:16 llm_backend: openai
11-23 11:13:16 llm_model: None
11-23 11:13:16 llm_context: 
11-23 11:13:16 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: (all available); target cathodes: (none specified).\nLabel column 'eol_class' with ~None classes; sequence length 32; 7 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': [], 'target_cathodes': [], 'label_column': 'eol_class'}}
11-23 11:13:16 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 0.0, 'warmup_epochs': 5, 'rationale': 'The dataset has short sequences (32 steps) and 7 channels, making a lightweight CNN with self-attention ideal to capture local and some longer-range dependencies without excessive compute. Label inconsistency suggests avoiding openmax or unknown heads to reduce complexity. Moderate dropout (0.3) helps prevent overfitting given the small sequence length and label noise. The learning rate and batch size balance stable training and efficiency, while warmup epochs support gradual adaptation. Backbone frozen for 5 epoch(s) before full fine-tuning to stabilise feature reuse. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness. Enabled SNGP to stabilise open-set transfer risk detected in metadata.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": false,\n  "openmax": false,\n  "use_unknown_head": false,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 0.0,\n  "warmup_epochs": 5,\n  "rationale": "The dataset has short sequences (32 steps) and 7 channels, making a lightweight CNN with self-attention ideal to capture local and some longer-range dependencies without excessive compute. Label inconsistency suggests avoiding openmax or unknown heads to reduce complexity. Moderate dropout (0.3) helps prevent overfitting given the small sequence length and label noise. The learning rate and batch size balance stable training and efficiency, while warmup epochs support gradual adaptation."\n}'}
11-23 11:13:16 llm_cfg_stamp: 20251123_111155
11-23 11:13:16 pretrained_model_path: None
11-23 11:13:18 using 1 cpu
11-23 11:13:31 data_name: Battery_inconsistent
11-23 11:13:31 data_dir: ./my_datasets/Battery
11-23 11:13:31 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
11-23 11:13:31 normlizetype: mean-std
11-23 11:13:31 method: sngp
11-23 11:13:31 gp_hidden_dim: 2048
11-23 11:13:31 spectral_norm_bound: 0.95
11-23 11:13:31 n_power_iterations: 1
11-23 11:13:31 nesterov: True
11-23 11:13:31 print_freq: 10
11-23 11:13:31 layers: 16
11-23 11:13:31 widen_factor: 1
11-23 11:13:31 droprate: 0.3
11-23 11:13:31 cuda_device: 0
11-23 11:13:31 checkpoint_dir: ./checkpoint
11-23 11:13:31 pretrained: False
11-23 11:13:31 batch_size: 64
11-23 11:13:31 warmup_epochs: 3
11-23 11:13:31 num_workers: 0
11-23 11:13:31 bottleneck: True
11-23 11:13:31 bottleneck_num: 128
11-23 11:13:31 last_batch: False
11-23 11:13:31 hidden_size: 1024
11-23 11:13:31 trade_off_adversarial: Step
11-23 11:13:31 lam_adversarial: 1
11-23 11:13:31 opt: adam
11-23 11:13:32 lr: 0.001
11-23 11:13:32 momentum: 0.9
11-23 11:13:32 weight_decay: 1e-05
11-23 11:13:32 lr_scheduler: step
11-23 11:13:32 gamma: 0.1
11-23 11:13:32 steps: 150, 250
11-23 11:13:32 middle_epoch: 15
11-23 11:13:32 max_epoch: 50
11-23 11:13:32 print_step: 25
11-23 11:13:32 inconsistent: UAN
11-23 11:13:32 model_name: cnn_features_1d_sa
11-23 11:13:32 th: 0.5
11-23 11:13:32 input_channels: 7
11-23 11:13:32 classification_label: eol_class
11-23 11:13:32 sequence_length: 32
11-23 11:13:32 cycles_per_file: 15
11-23 11:13:32 source_cycles_per_file: None
11-23 11:13:32 target_cycles_per_file: None
11-23 11:13:32 sample_random_state: 42
11-23 11:13:32 transfer_task: [[0], [1]]
11-23 11:13:32 source_cathode: ['5Vspinel']
11-23 11:13:32 target_cathode: []
11-23 11:13:32 num_classes: None
11-23 11:13:32 domain_temperature: 1.0
11-23 11:13:32 class_temperature: 10.0
11-23 11:13:32 lambda_src: 0.0
11-23 11:13:32 lambda_src_decay_patience: 5
11-23 11:13:32 lambda_src_decay_factor: 0.5
11-23 11:13:32 lambda_src_min: 0.0
11-23 11:13:32 lambda_src_warmup: 0
11-23 11:13:32 improvement_metric: common
11-23 11:13:32 auto_select: True
11-23 11:13:32 llm_compare: True
11-23 11:13:32 llm_backend: openai
11-23 11:13:32 llm_model: None
11-23 11:13:32 llm_context: 
11-23 11:13:32 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: (all available); target cathodes: (none specified).\nLabel column 'eol_class' with ~None classes; sequence length 32; 7 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': [], 'target_cathodes': [], 'label_column': 'eol_class'}}
11-23 11:13:32 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 0.0, 'warmup_epochs': 5, 'rationale': 'The dataset has short sequences (32 steps) and 7 channels, making a lightweight CNN with self-attention ideal to capture local and some longer-range dependencies without excessive compute. Label inconsistency suggests avoiding openmax or unknown heads to reduce complexity. Moderate dropout (0.3) helps prevent overfitting given the small sequence length and label noise. The learning rate and batch size balance stable training and efficiency, while warmup epochs support gradual adaptation. Backbone frozen for 5 epoch(s) before full fine-tuning to stabilise feature reuse. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness. Enabled SNGP to stabilise open-set transfer risk detected in metadata.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": false,\n  "openmax": false,\n  "use_unknown_head": false,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 0.0,\n  "warmup_epochs": 5,\n  "rationale": "The dataset has short sequences (32 steps) and 7 channels, making a lightweight CNN with self-attention ideal to capture local and some longer-range dependencies without excessive compute. Label inconsistency suggests avoiding openmax or unknown heads to reduce complexity. Moderate dropout (0.3) helps prevent overfitting given the small sequence length and label noise. The learning rate and batch size balance stable training and efficiency, while warmup epochs support gradual adaptation."\n}'}
11-23 11:13:32 llm_cfg_stamp: 20251123_111155
11-23 11:13:32 pretrained_model_path: None
11-23 11:13:34 using 1 cpu
11-23 13:07:00 data_name: Battery_inconsistent
11-23 13:07:00 data_dir: ./my_datasets/Battery
11-23 13:07:00 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
11-23 13:07:00 normlizetype: mean-std
11-23 13:07:00 method: sngp
11-23 13:07:00 gp_hidden_dim: 2048
11-23 13:07:00 spectral_norm_bound: 0.95
11-23 13:07:00 n_power_iterations: 1
11-23 13:07:00 nesterov: True
11-23 13:07:00 print_freq: 10
11-23 13:07:00 layers: 16
11-23 13:07:00 widen_factor: 1
11-23 13:07:00 droprate: 0.3
11-23 13:07:00 cuda_device: 0
11-23 13:07:00 checkpoint_dir: ./checkpoint
11-23 13:07:00 pretrained: False
11-23 13:07:00 batch_size: 64
11-23 13:07:00 warmup_epochs: 3
11-23 13:07:00 num_workers: 0
11-23 13:07:00 bottleneck: True
11-23 13:07:00 bottleneck_num: 128
11-23 13:07:00 last_batch: False
11-23 13:07:00 hidden_size: 1024
11-23 13:07:00 trade_off_adversarial: Step
11-23 13:07:00 lam_adversarial: 1
11-23 13:07:00 opt: adam
11-23 13:07:00 lr: 0.001
11-23 13:07:00 momentum: 0.9
11-23 13:07:00 weight_decay: 1e-05
11-23 13:07:00 lr_scheduler: step
11-23 13:07:00 gamma: 0.1
11-23 13:07:00 steps: 150, 250
11-23 13:07:00 middle_epoch: 15
11-23 13:07:00 max_epoch: 50
11-23 13:07:00 print_step: 25
11-23 13:07:00 inconsistent: UAN
11-23 13:07:00 model_name: cnn_features_1d_sa
11-23 13:07:00 th: 0.5
11-23 13:07:00 input_channels: 7
11-23 13:07:00 classification_label: eol_class
11-23 13:07:00 sequence_length: 32
11-23 13:07:00 cycles_per_file: 15
11-23 13:07:00 source_cycles_per_file: None
11-23 13:07:00 target_cycles_per_file: None
11-23 13:07:00 sample_random_state: 42
11-23 13:07:00 transfer_task: [[0], [1]]
11-23 13:07:00 source_cathode: ['5Vspinel']
11-23 13:07:00 target_cathode: []
11-23 13:07:00 num_classes: None
11-23 13:07:00 domain_temperature: 1.0
11-23 13:07:00 class_temperature: 10.0
11-23 13:07:00 lambda_src: 0.0
11-23 13:07:00 lambda_src_decay_patience: 5
11-23 13:07:00 lambda_src_decay_factor: 0.5
11-23 13:07:00 lambda_src_min: 0.0
11-23 13:07:00 lambda_src_warmup: 0
11-23 13:07:00 improvement_metric: common
11-23 13:07:00 auto_select: True
11-23 13:07:00 llm_compare: True
11-23 13:07:00 llm_backend: openai
11-23 13:07:00 llm_model: None
11-23 13:07:00 llm_context: 
11-23 13:07:00 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: (all available); target cathodes: (none specified).\nLabel column 'eol_class' with ~None classes; sequence length 32; 7 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': [], 'target_cathodes': [], 'label_column': 'eol_class'}}
11-23 13:07:00 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 0.0, 'warmup_epochs': 5, 'rationale': 'The dataset has short sequences (32 steps) and 7 channels, making a lightweight CNN with self-attention ideal to capture local and some global dependencies without excessive compute. Label inconsistency suggests avoiding openmax to reduce tuning complexity, while dropout at 0.3 helps prevent overfitting. Moderate bottleneck size balances capacity and regularization, and a learning rate of 0.001 with warmup stabilizes training on this battery aging data. Backbone frozen for 5 epoch(s) before full fine-tuning to stabilise feature reuse. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness. Enabled SNGP to stabilise open-set transfer risk detected in metadata.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": false,\n  "openmax": false,\n  "use_unknown_head": false,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 0.0,\n  "warmup_epochs": 5,\n  "rationale": "The dataset has short sequences (32 steps) and 7 channels, making a lightweight CNN with self-attention ideal to capture local and some global dependencies without excessive compute. Label inconsistency suggests avoiding openmax to reduce tuning complexity, while dropout at 0.3 helps prevent overfitting. Moderate bottleneck size balances capacity and regularization, and a learning rate of 0.001 with warmup stabilizes training on this battery aging data."\n}'}
11-23 13:07:00 llm_cfg_stamp: 20251123_130533
11-23 13:07:00 pretrained_model_path: None
11-23 13:07:02 using 1 cpu
11-23 13:07:17 data_name: Battery_inconsistent
11-23 13:07:17 data_dir: ./my_datasets/Battery
11-23 13:07:17 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
11-23 13:07:17 normlizetype: mean-std
11-23 13:07:17 method: sngp
11-23 13:07:17 gp_hidden_dim: 2048
11-23 13:07:17 spectral_norm_bound: 0.95
11-23 13:07:17 n_power_iterations: 1
11-23 13:07:17 nesterov: True
11-23 13:07:17 print_freq: 10
11-23 13:07:17 layers: 16
11-23 13:07:17 widen_factor: 1
11-23 13:07:17 droprate: 0.3
11-23 13:07:17 cuda_device: 0
11-23 13:07:17 checkpoint_dir: ./checkpoint
11-23 13:07:17 pretrained: False
11-23 13:07:17 batch_size: 64
11-23 13:07:17 warmup_epochs: 3
11-23 13:07:17 num_workers: 0
11-23 13:07:17 bottleneck: True
11-23 13:07:17 bottleneck_num: 128
11-23 13:07:17 last_batch: False
11-23 13:07:17 hidden_size: 1024
11-23 13:07:17 trade_off_adversarial: Step
11-23 13:07:17 lam_adversarial: 1
11-23 13:07:17 opt: adam
11-23 13:07:17 lr: 0.001
11-23 13:07:17 momentum: 0.9
11-23 13:07:17 weight_decay: 1e-05
11-23 13:07:17 lr_scheduler: step
11-23 13:07:17 gamma: 0.1
11-23 13:07:17 steps: 150, 250
11-23 13:07:17 middle_epoch: 15
11-23 13:07:17 max_epoch: 50
11-23 13:07:17 print_step: 25
11-23 13:07:17 inconsistent: UAN
11-23 13:07:17 model_name: cnn_features_1d_sa
11-23 13:07:17 th: 0.5
11-23 13:07:17 input_channels: 7
11-23 13:07:17 classification_label: eol_class
11-23 13:07:17 sequence_length: 32
11-23 13:07:17 cycles_per_file: 15
11-23 13:07:17 source_cycles_per_file: None
11-23 13:07:17 target_cycles_per_file: None
11-23 13:07:17 sample_random_state: 42
11-23 13:07:17 transfer_task: [[0], [1]]
11-23 13:07:17 source_cathode: ['5Vspinel']
11-23 13:07:17 target_cathode: []
11-23 13:07:17 num_classes: None
11-23 13:07:17 domain_temperature: 1.0
11-23 13:07:17 class_temperature: 10.0
11-23 13:07:17 lambda_src: 0.0
11-23 13:07:17 lambda_src_decay_patience: 5
11-23 13:07:17 lambda_src_decay_factor: 0.5
11-23 13:07:17 lambda_src_min: 0.0
11-23 13:07:17 lambda_src_warmup: 0
11-23 13:07:17 improvement_metric: common
11-23 13:07:17 auto_select: True
11-23 13:07:17 llm_compare: True
11-23 13:07:17 llm_backend: openai
11-23 13:07:17 llm_model: None
11-23 13:07:17 llm_context: 
11-23 13:07:17 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: (all available); target cathodes: (none specified).\nLabel column 'eol_class' with ~None classes; sequence length 32; 7 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': [], 'target_cathodes': [], 'label_column': 'eol_class'}}
11-23 13:07:17 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 0.0, 'warmup_epochs': 5, 'rationale': 'The dataset has short sequences (32 steps) and 7 channels, making a lightweight CNN with self-attention ideal to capture local and some global dependencies without excessive compute. Label inconsistency suggests avoiding openmax to reduce tuning complexity, while dropout at 0.3 helps prevent overfitting. Moderate bottleneck size balances capacity and regularization, and a learning rate of 0.001 with warmup stabilizes training on this battery aging data. Backbone frozen for 5 epoch(s) before full fine-tuning to stabilise feature reuse. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness. Enabled SNGP to stabilise open-set transfer risk detected in metadata.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": false,\n  "openmax": false,\n  "use_unknown_head": false,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 0.0,\n  "warmup_epochs": 5,\n  "rationale": "The dataset has short sequences (32 steps) and 7 channels, making a lightweight CNN with self-attention ideal to capture local and some global dependencies without excessive compute. Label inconsistency suggests avoiding openmax to reduce tuning complexity, while dropout at 0.3 helps prevent overfitting. Moderate bottleneck size balances capacity and regularization, and a learning rate of 0.001 with warmup stabilizes training on this battery aging data."\n}'}
11-23 13:07:17 llm_cfg_stamp: 20251123_130533
11-23 13:07:17 pretrained_model_path: None
11-23 13:07:19 using 1 cpu

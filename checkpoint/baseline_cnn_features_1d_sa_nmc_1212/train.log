12-12 14:13:29 data_name: Battery_inconsistent
12-12 14:13:29 data_dir: ./my_datasets/Battery
12-12 14:13:29 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
12-12 14:13:29 normlizetype: mean-std
12-12 14:13:29 method: deterministic
12-12 14:13:29 gp_hidden_dim: 2048
12-12 14:13:29 spectral_norm_bound: 0.95
12-12 14:13:29 n_power_iterations: 1
12-12 14:13:29 nesterov: True
12-12 14:13:29 print_freq: 10
12-12 14:13:29 layers: 16
12-12 14:13:29 widen_factor: 1
12-12 14:13:29 droprate: 0.3
12-12 14:13:29 cuda_device: 0
12-12 14:13:29 checkpoint_dir: ./checkpoint
12-12 14:13:29 pretrained: False
12-12 14:13:29 batch_size: 8
12-12 14:13:29 warmup_epochs: 3
12-12 14:13:29 num_workers: 0
12-12 14:13:29 bottleneck: True
12-12 14:13:29 bottleneck_num: 256
12-12 14:13:29 last_batch: False
12-12 14:13:29 hidden_size: 1024
12-12 14:13:29 trade_off_adversarial: Step
12-12 14:13:29 lam_adversarial: 1
12-12 14:13:29 opt: adam
12-12 14:13:29 lr: 0.0003
12-12 14:13:29 momentum: 0.9
12-12 14:13:29 weight_decay: 1e-05
12-12 14:13:29 lr_scheduler: step
12-12 14:13:29 gamma: 0.1
12-12 14:13:29 steps: 150, 250
12-12 14:13:29 middle_epoch: 15
12-12 14:13:29 max_epoch: 50
12-12 14:13:29 print_step: 25
12-12 14:13:29 inconsistent: UAN
12-12 14:13:29 model_name: cnn_features_1d
12-12 14:13:29 th: 0.5
12-12 14:13:29 input_channels: 7
12-12 14:13:29 classification_label: eol_class
12-12 14:13:29 sequence_length: 32
12-12 14:13:29 cycles_per_file: 15
12-12 14:13:29 source_cycles_per_file: None
12-12 14:13:29 target_cycles_per_file: None
12-12 14:13:29 cycle_ablation: False
12-12 14:13:29 cycle_ablation_start: 5
12-12 14:13:29 cycle_ablation_step: 10
12-12 14:13:29 cycle_ablation_max: None
12-12 14:13:29 literature_context_file: ./references/joule_s2542-4351-22-00409-3.md
12-12 14:13:29 sample_random_state: 42
12-12 14:13:29 transfer_task: [[[0], [1]], [[0], [2]], [[0], [3]], [[1], [0]], [[1], [2]], [[1], [3]], [[2], [0]], [[2], [1]], [[2], [3]], [[3], [0]], [[3], [1]], [[3], [2]]]
12-12 14:13:29 source_cathode: ['HE5050', 'NMC111', 'NMC532', 'NMC622', 'NMC811']
12-12 14:13:29 target_cathode: []
12-12 14:13:29 num_classes: None
12-12 14:13:29 domain_temperature: 1.0
12-12 14:13:29 class_temperature: 10.0
12-12 14:13:29 lambda_src: 0.0
12-12 14:13:29 lambda_src_decay_patience: 5
12-12 14:13:29 lambda_src_decay_factor: 0.5
12-12 14:13:29 lambda_src_min: 0.0
12-12 14:13:29 lambda_src_warmup: 0
12-12 14:13:29 improvement_metric: accuracy
12-12 14:13:29 skip_retry: False
12-12 14:13:29 auto_select: True
12-12 14:13:29 llm_compare: True
12-12 14:13:29 llm_backend: openai
12-12 14:13:29 llm_model: None
12-12 14:13:29 llm_context: 
12-12 14:13:29 llm_ablation: False
12-12 14:13:29 llm_per_transfer: True
12-12 14:13:29 ablation_cycle_limits: 
12-12 14:13:29 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: 5Vspinel; target cathodes: HE5050, NMC111, NMC532, NMC622, NMC811.\nLabel column 'eol_class' with ~None classes; sequence length 32; 21 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.\nLiterature cues (Joule S2542-4351(22)00409-3, chemistry-sensitive ablations):\nJoule 2022 (S2542-4351(22)00409-3) key cues for chemistry-aware ablation:\n- Early-cycle capacity fade slopes and coulombic-efficiency stabilization highlight whether aging is lithium-inventory loss (graphite/anode-electrolyte limited) versus cathode structural decay (e.g., Ni-rich NMC cathodes with electrolyte oxidation risk).\n- High initial impedance growth, thick SEI signatures, or electrolyte oxidation markers flag chemistries that benefit from uncertainty-aware heads (SNGP/OpenMax) to avoid overconfident extrapolation on outlier cycles.\n- LFP/graphite pairs tend to show flatter voltage plateaus and slower early fade; Ni-rich NMC chemistries degrade faster under aggressive cycling and need stronger regularization or shorter cycle horizons in ablations.\n- Transfer between mismatched chemistries (e.g., NMC → LFP) should down-weight source loss and favor architectures that adapt quickly with self-attention or wider bottlenecks; closely matched chemistries can rely more on convolutional baselines.\nsource_train: class counts 0:19, 1:18, 2:2, 3:1, 4:1 (example label 1).\nsource_train sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, -0.016899999231100082, 0.0012000000569969416, -0.030400000512599945, -0.03880000114440918, -0.04360000044107437, -0.05829999968409538, -0.0729999989271164, -0.074…\nsource_train flattened row glimpses: [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408]]\ntarget_train: class counts 0:22, 1:22, 2:38, 3:34, 4:39 (example label 1).\ntarget_train sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, 0.3743000030517578, 0.37630000710487366, 0.3709999918937683, 0.3490999937057495, 0.34459999203681946, 0.34619998931884766, 0.32109999656677246, 0.3179000020027160…\ntarget_train flattened row glimpses: [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408]]\nsource_val: class counts 0:5, 1:5, 2:1, 4:1 (example label 0).\nsource_val sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, -0.04390000179409981, -0.06509999930858612, -0.11779999732971191, -0.15219999849796295, -0.17170000076293945, -0.19760000705718994, -0.21850000321865082, -0.22759…\nsource_val flattened row glimpses: [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408]]\ntarget_val: class counts 0:9, 1:10, 2:19, 3:17, 4:19 (example label 3).\ntarget_val sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.174100041389…\ntarget_val flattened row glimpses: [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408]]", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.0005, 'dropout_hint': 0.35, 'num_classes_hint': None, 'splits': {'source_train': {'batch_shape': [8, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 1, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, -0.016899999231100082, 0.0012000000569969416, -0.030400000512599945, -0.03880000114440918, -0.04360000044107437, -0.05829999968409538, -0.0729999989271164, -0.07419999688863754, -0.07940000295639038, -0.10239999741315842, -0.11169999837875366], [-1.13100004196167, -0.17839999496936798, -0.15639999508857727, -0.18359999358654022, -0.19140000641345978, -0.195700004696846, -0.20919999480247498, -0.22220000624656677, -0.22349999845027924, -0.22930000722408295, -0.2498999983072281, -0.25760000944137573]]}, 'batch_channel_mean': [0.8607000112533569, -0.19619999825954437, -0.3012000024318695, -0.15970000624656677, -0.301800012588501, 0.9627000093460083, -0.1062999963760376, 0.14169999957084656], 'batch_channel_std': [1.059499979019165, 0.24040000140666962, 0.17249999940395355, 0.25209999084472656, 0.17270000278949738, 1.1347999572753906, 0.824999988079071, 0.6575999855995178], 'class_distribution': {'0': 19, '1': 18, '2': 2, '3': 1, '4': 1}, 'feature_range': [-3.203935665790699, 5.1401425697203385], 'feature_global_mean': -0.016921664518018544, 'flattened_rows_head': [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408]]}, 'target_train': {'batch_shape': [8, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 1, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, 0.3743000030517578, 0.37630000710487366, 0.3709999918937683, 0.3490999937057495, 0.34459999203681946, 0.34619998931884766, 0.32109999656677246, 0.31790000200271606, 0.29190000891685486, 0.24529999494552612, 0.24169999361038208], [-1.13100004196167, 0.4562999904155731, 0.46230000257492065, 0.45500001311302185, 0.4316999912261963, 0.4268999993801117, 0.42480000853538513, 0.397599995136261, 0.39410001039505005, 0.36640000343322754, 0.31610000133514404, 0.3122999966144562]]}, 'batch_channel_mean': [0.8607000112533569, -0.04560000076889992, -0.01269999984651804, -0.06199999898672104, -0.009600000455975533, 0.1704999953508377, -0.36579999327659607, -0.0020000000949949026], 'batch_channel_std': [1.059499979019165, 0.5946999788284302, 0.5917999744415283, 0.5709999799728394, 0.5871999859809875, 0.9681000113487244, 0.6819999814033508, 0.7301999926567078], 'class_distribution': {'0': 22, '1': 22, '2': 38, '3': 34, '4': 39}, 'feature_range': [-9.981793547102113, 31.866555053073082], 'feature_global_mean': 0.043367023845691036, 'flattened_rows_head': [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408]]}, 'source_val': {'batch_shape': [8, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 0, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, -0.04390000179409981, -0.06509999930858612, -0.11779999732971191, -0.15219999849796295, -0.17170000076293945, -0.19760000705718994, -0.21850000321865082, -0.22759999334812164, -0.2379000037908554, -0.2531999945640564, -0.2678999900817871], [-1.13100004196167, -0.19740000367164612, -0.21279999613761902, -0.25780001282691956, -0.28760001063346863, -0.3043000102043152, -0.32760000228881836, -0.3458000123500824, -0.3540000021457672, -0.3650999963283539, -0.37959998846054077, -0.3926999866962433]]}, 'batch_channel_mean': [0.8607000112533569, -0.21899999678134918, -0.3481999933719635, -0.1915999948978424, -0.34689998626708984, 1.118899941444397, -0.11599999666213989, 0.016499999910593033], 'batch_channel_std': [1.059499979019165, 0.21160000562667847, 0.17630000412464142, 0.2303999960422516, 0.17829999327659607, 0.9266999959945679, 0.8331000208854675, 0.7063000202178955], 'class_distribution': {'0': 5, '1': 5, '2': 1, '4': 1}, 'feature_range': [-3.203935665790699, 5.1560300164083985], 'feature_global_mean': -0.010901597356338598, 'flattened_rows_head': [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408]]}, 'target_val': {'batch_shape': [8, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 3, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653], [-1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167]]}, 'batch_channel_mean': [0.8607000112533569, 0.27559998631477356, 0.2630000114440918, 0.1655000001192093, 0.2653000056743622, 0.005200000014156103, -0.3278999924659729, -0.6205999851226807], 'batch_channel_std': [1.059499979019165, 0.4851999878883362, 0.47620001435279846, 0.46309998631477356, 0.47749999165534973, 0.9384999871253967, 0.8022000193595886, 0.9699000120162964], 'class_distribution': {'0': 9, '1': 10, '2': 19, '3': 17, '4': 19}, 'feature_range': [-3.426709644883315, 32.206823553376644], 'feature_global_mean': 0.014365731412246173, 'flattened_rows_head': [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408]]}}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': ['5Vspinel'], 'target_cathodes': ['HE5050', 'NMC111', 'NMC532', 'NMC622', 'NMC811'], 'label_column': 'eol_class', 'split_used': 'source_train', 'batch_size_seen': 8, 'channels': 21, 'seq_len': 32}}
12-12 14:13:29 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 256, 'dropout': 0.35, 'learning_rate': 0.0005, 'batch_size': 8, 'lambda_src': 0.6, 'warmup_epochs': 10, 'rationale': 'The dataset has 21 channels and short sequence length (32), favoring CNN with self-attention to capture local and some global dependencies. The domain shift from 5Vspinel to HE5050 and label inconsistency suggest using self-attention for adaptability without the complexity of openmax or SNGP. Moderate bottleneck and dropout balance capacity and regularization. Batch size 8 matches seen data, and warmup helps stabilize transfer learning. Target cathode: HE5050 (high-energy experimental blend). Source cathodes: 5Vspinel (high-voltage spinel (manganese rich)). Source/target chemistries differ → expect domain shift. Evaluated 21 channels × 32-step windows and matched them with cnn_1d_sa capacity. Target fine-tune spans ≈155 cycles versus 41 source cycles, guiding lr=5.00e-04 and dropout=0.35.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": false,\n  "openmax": false,\n  "use_unknown_head": false,\n  "bottleneck": 256,\n  "dropout": 0.35,\n  "learning_rate": 0.0005,\n  "batch_size": 8,\n  "lambda_src": 1.0,\n  "warmup_epochs": 10,\n  "rationale": "The dataset has 21 channels and short sequence length (32), favoring CNN with self-attention to capture local and some global dependencies. The domain shift from 5Vspinel to HE5050 and label inconsistency suggest using self-attention for adaptability without the complexity of openmax or SNGP. Moderate bottleneck and dropout balance capacity and regularization. Batch size 8 matches seen data, and warmup helps stabilize transfer learning."\n}'}
12-12 14:13:29 llm_cfg_stamp: 20251212_131825
12-12 14:13:29 sngp: False
12-12 14:13:29 openmax: False
12-12 14:13:29 use_unknown_head: False
12-12 14:13:29 pretrained_model_path: None
12-12 14:13:29 using 1 cpu
12-12 14:15:29 data_name: Battery_inconsistent
12-12 14:15:29 data_dir: ./my_datasets/Battery
12-12 14:15:29 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
12-12 14:15:29 normlizetype: mean-std
12-12 14:15:29 method: deterministic
12-12 14:15:29 gp_hidden_dim: 2048
12-12 14:15:29 spectral_norm_bound: 0.95
12-12 14:15:29 n_power_iterations: 1
12-12 14:15:29 nesterov: True
12-12 14:15:29 print_freq: 10
12-12 14:15:29 layers: 16
12-12 14:15:29 widen_factor: 1
12-12 14:15:29 droprate: 0.3
12-12 14:15:29 cuda_device: 0
12-12 14:15:29 checkpoint_dir: ./checkpoint
12-12 14:15:29 pretrained: False
12-12 14:15:29 batch_size: 16
12-12 14:15:29 warmup_epochs: 3
12-12 14:15:29 num_workers: 0
12-12 14:15:29 bottleneck: True
12-12 14:15:29 bottleneck_num: 256
12-12 14:15:29 last_batch: False
12-12 14:15:29 hidden_size: 1024
12-12 14:15:29 trade_off_adversarial: Step
12-12 14:15:29 lam_adversarial: 1
12-12 14:15:29 opt: adam
12-12 14:15:29 lr: 0.0003
12-12 14:15:29 momentum: 0.9
12-12 14:15:29 weight_decay: 1e-05
12-12 14:15:29 lr_scheduler: step
12-12 14:15:29 gamma: 0.1
12-12 14:15:29 steps: 150, 250
12-12 14:15:29 middle_epoch: 15
12-12 14:15:29 max_epoch: 50
12-12 14:15:29 print_step: 25
12-12 14:15:29 inconsistent: UAN
12-12 14:15:29 model_name: cnn_features_1d
12-12 14:15:29 th: 0.5
12-12 14:15:29 input_channels: 7
12-12 14:15:29 classification_label: eol_class
12-12 14:15:29 sequence_length: 32
12-12 14:15:29 cycles_per_file: 15
12-12 14:15:29 source_cycles_per_file: None
12-12 14:15:29 target_cycles_per_file: None
12-12 14:15:29 cycle_ablation: False
12-12 14:15:29 cycle_ablation_start: 5
12-12 14:15:29 cycle_ablation_step: 10
12-12 14:15:29 cycle_ablation_max: None
12-12 14:15:29 literature_context_file: ./references/joule_s2542-4351-22-00409-3.md
12-12 14:15:29 sample_random_state: 42
12-12 14:15:29 transfer_task: [[[0], [1]], [[0], [2]], [[0], [3]], [[1], [0]], [[1], [2]], [[1], [3]], [[2], [0]], [[2], [1]], [[2], [3]], [[3], [0]], [[3], [1]], [[3], [2]]]
12-12 14:15:29 source_cathode: ['HE5050', 'NMC111', 'NMC532', 'NMC622', 'NMC811']
12-12 14:15:29 target_cathode: []
12-12 14:15:29 num_classes: None
12-12 14:15:29 domain_temperature: 1.0
12-12 14:15:29 class_temperature: 10.0
12-12 14:15:29 lambda_src: 0.0
12-12 14:15:29 lambda_src_decay_patience: 5
12-12 14:15:29 lambda_src_decay_factor: 0.5
12-12 14:15:29 lambda_src_min: 0.0
12-12 14:15:29 lambda_src_warmup: 0
12-12 14:15:29 improvement_metric: accuracy
12-12 14:15:29 skip_retry: False
12-12 14:15:29 auto_select: True
12-12 14:15:29 llm_compare: True
12-12 14:15:29 llm_backend: openai
12-12 14:15:29 llm_model: None
12-12 14:15:29 llm_context: 
12-12 14:15:29 llm_ablation: False
12-12 14:15:29 llm_per_transfer: True
12-12 14:15:29 ablation_cycle_limits: 
12-12 14:15:29 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: FCG; target cathodes: HE5050, NMC111, NMC532, NMC622, NMC811.\nLabel column 'eol_class' with ~None classes; sequence length 32; 21 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.\nLiterature cues (Joule S2542-4351(22)00409-3, chemistry-sensitive ablations):\nJoule 2022 (S2542-4351(22)00409-3) key cues for chemistry-aware ablation:\n- Early-cycle capacity fade slopes and coulombic-efficiency stabilization highlight whether aging is lithium-inventory loss (graphite/anode-electrolyte limited) versus cathode structural decay (e.g., Ni-rich NMC cathodes with electrolyte oxidation risk).\n- High initial impedance growth, thick SEI signatures, or electrolyte oxidation markers flag chemistries that benefit from uncertainty-aware heads (SNGP/OpenMax) to avoid overconfident extrapolation on outlier cycles.\n- LFP/graphite pairs tend to show flatter voltage plateaus and slower early fade; Ni-rich NMC chemistries degrade faster under aggressive cycling and need stronger regularization or shorter cycle horizons in ablations.\n- Transfer between mismatched chemistries (e.g., NMC → LFP) should down-weight source loss and favor architectures that adapt quickly with self-attention or wider bottlenecks; closely matched chemistries can rely more on convolutional baselines.\nsource_train: class counts 3:6 (example label 3).\nsource_train sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, 0.595300018787384, 0.6348000168800354, 0.625, 0.6118999719619751, 0.6071000099182129, 0.5910000205039978, 0.5741000175476074, 0.5752000212669373, 0.55400002002716…\nsource_train flattened row glimpses: [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]\ntarget_train: class counts 0:22, 1:22, 2:38, 3:34, 4:39 (example label 0).\ntarget_train sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, 0.20640000700950623, 0.23579999804496765, 0.22679999470710754, 0.26030001044273376, 0.04879999905824661, 0.05339999869465828, 0.09780000150203705, -0.050999999046…\ntarget_train flattened row glimpses: [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]\nsource_val: class counts 3:2 (example label 3).\nsource_val sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, 0.6047000288963318, 0.6389999985694885, 0.6284000277519226, 0.6177999973297119, 0.6114000082015991, 0.5934000015258789, 0.5813999772071838, 0.5791000127792358, 0.…\nsource_val flattened row glimpses: [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]\ntarget_val: class counts 0:9, 1:10, 2:19, 3:17, 4:19 (example label 3).\ntarget_val sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.140300035476…\ntarget_val flattened row glimpses: [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.0005, 'dropout_hint': 0.35, 'num_classes_hint': None, 'splits': {'source_train': {'batch_shape': [6, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 3, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, 0.595300018787384, 0.6348000168800354, 0.625, 0.6118999719619751, 0.6071000099182129, 0.5910000205039978, 0.5741000175476074, 0.5752000212669373, 0.5540000200271606, 0.5228000283241272, 0.5182999968528748], [-1.1371999979019165, 0.605400025844574, 0.6553000211715698, 0.6427000164985657, 0.628000020980835, 0.6229000091552734, 0.6029000282287598, 0.5839999914169312, 0.5852000117301941, 0.5598999857902527, 0.5235999822616577, 0.5192999839782715]]}, 'batch_channel_mean': [0.8607000112533569, 0.48170000314712524, 0.48350000381469727, 0.5385000109672546, 0.5008000135421753, 0.8583999872207642, -0.19740000367164612, 0.20200000703334808], 'batch_channel_std': [1.059499979019165, 0.2962000072002411, 0.29739999771118164, 0.3061000108718872, 0.2996000051498413, 0.7976999878883362, 0.7239000201225281, 0.6255999803543091], 'class_distribution': {'3': 6}, 'feature_range': [-3.2106635401609402, 3.5002185783545934], 'feature_global_mean': 0.19591510889964595, 'flattened_rows_head': [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]}, 'target_train': {'batch_shape': [8, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 0, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, 0.20640000700950623, 0.23579999804496765, 0.22679999470710754, 0.26030001044273376, 0.04879999905824661, 0.05339999869465828, 0.09780000150203705, -0.050999999046325684, -0.038100000470876694, -1.1403000354766846, 0.1395999938249588], [-1.1371999979019165, 0.1460999995470047, 0.2013999968767166, 0.19509999454021454, 0.20659999549388885, -0.00430000014603138, 0.002899999963119626, 0.026499999687075615, -0.11969999969005585, -0.10379999876022339, -1.1371999979019165, 0.09149999916553497]]}, 'batch_channel_mean': [0.8607000112533569, -0.6585999727249146, -0.6636999845504761, -0.6561999917030334, -0.6572999954223633, 0.4447000026702881, -0.22990000247955322, 0.045099999755620956], 'batch_channel_std': [1.059499979019165, 0.5873000025749207, 0.5734000205993652, 0.5383999943733215, 0.5719000101089478, 0.991599977016449, 0.7060999870300293, 0.7912999987602234], 'class_distribution': {'0': 22, '1': 22, '2': 38, '3': 34, '4': 39}, 'feature_range': [-9.20385614937911, 28.87870113204206], 'feature_global_mean': 0.026973963171381392, 'flattened_rows_head': [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]}, 'source_val': {'batch_shape': [2, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 3, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, 0.6047000288963318, 0.6389999985694885, 0.6284000277519226, 0.6177999973297119, 0.6114000082015991, 0.5934000015258789, 0.5813999772071838, 0.5791000127792358, 0.5594000220298767, 0.5275999903678894, 0.5281000137329102], [-1.1371999979019165, 0.6155999898910522, 0.6596999764442444, 0.6460999846458435, 0.6338000297546387, 0.6272000074386597, 0.6050000190734863, 0.590399980545044, 0.5888000130653381, 0.5641999840736389, 0.5278000235557556, 0.5281000137329102]]}, 'batch_channel_mean': [0.8607000112533569, 0.4747999906539917, 0.4763000011444092, 0.5282999873161316, 0.49230000376701355, 0.847100019454956, -0.2003999948501587, 0.2013999968767166], 'batch_channel_std': [1.059499979019165, 0.2944999933242798, 0.2957000136375427, 0.3041999936103821, 0.2978000044822693, 0.7935000061988831, 0.7203999757766724, 0.6255000233650208], 'class_distribution': {'3': 2}, 'feature_range': [-3.2106635401609402, 3.44913117424402], 'feature_global_mean': 0.1921846091904075, 'flattened_rows_head': [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]}, 'target_val': {'batch_shape': [8, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 3, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846], [-1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165]]}, 'batch_channel_mean': [0.8607000112533569, 0.18389999866485596, 0.14309999346733093, 0.08900000154972076, 0.14380000531673431, 0.11050000041723251, -0.2827000021934509, -0.5985999703407288], 'batch_channel_std': [1.059499979019165, 0.4431999921798706, 0.4374000132083893, 0.42160001397132874, 0.438400000333786, 1.0058000087738037, 0.8294000029563904, 0.9807000160217285], 'class_distribution': {'0': 9, '1': 10, '2': 19, '3': 17, '4': 19}, 'feature_range': [-3.484281475443279, 29.189441485357083], 'feature_global_mean': -0.0008096915043016523, 'flattened_rows_head': [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]}}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': ['FCG'], 'target_cathodes': ['HE5050', 'NMC111', 'NMC532', 'NMC622', 'NMC811'], 'label_column': 'eol_class', 'split_used': 'source_train', 'batch_size_seen': 6, 'channels': 21, 'seq_len': 32}}
12-12 14:15:29 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 256, 'dropout': 0.35, 'learning_rate': 0.0005, 'batch_size': 16, 'lambda_src': 0.6, 'warmup_epochs': 10, 'rationale': 'The dataset has 21 channels and short sequences (32 steps), with label inconsistency and domain shift between source and target chemistries. A CNN with self-attention balances local feature extraction and global context, helping capture multi-cycle correlations and adapt to domain shifts. The moderate bottleneck and dropout prevent overfitting given the small batch size and label noise. OpenMax or SNGP are less favored since explicit unknown rejection is not mandatory, and the dataset benefits more from attention-based adaptation. Target cathode: HE5050 (high-energy experimental blend). Source cathodes: FCG (graphite focused). Source/target chemistries differ → expect domain shift. Evaluated 21 channels × 32-step windows and matched them with cnn_1d_sa capacity. Target fine-tune spans ≈155 cycles versus 6 source cycles, guiding lr=5.00e-04 and dropout=0.35.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": false,\n  "openmax": false,\n  "use_unknown_head": false,\n  "bottleneck": 256,\n  "dropout": 0.35,\n  "learning_rate": 0.0005,\n  "batch_size": 16,\n  "lambda_src": 1.0,\n  "warmup_epochs": 10,\n  "rationale": "The dataset has 21 channels and short sequences (32 steps), with label inconsistency and domain shift between source and target chemistries. A CNN with self-attention balances local feature extraction and global context, helping capture multi-cycle correlations and adapt to domain shifts. The moderate bottleneck and dropout prevent overfitting given the small batch size and label noise. OpenMax or SNGP are less favored since explicit unknown rejection is not mandatory, and the dataset benefits more from attention-based adaptation."\n}'}
12-12 14:15:29 llm_cfg_stamp: 20251212_131825
12-12 14:15:29 sngp: False
12-12 14:15:29 openmax: False
12-12 14:15:29 use_unknown_head: False
12-12 14:15:29 pretrained_model_path: None
12-12 14:15:29 using 1 cpu
12-12 14:28:40 data_name: Battery_inconsistent
12-12 14:28:40 data_dir: ./my_datasets/Battery
12-12 14:28:40 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
12-12 14:28:40 normlizetype: mean-std
12-12 14:28:40 method: deterministic
12-12 14:28:40 gp_hidden_dim: 2048
12-12 14:28:40 spectral_norm_bound: 0.95
12-12 14:28:40 n_power_iterations: 1
12-12 14:28:40 nesterov: True
12-12 14:28:40 print_freq: 10
12-12 14:28:40 layers: 16
12-12 14:28:40 widen_factor: 1
12-12 14:28:40 droprate: 0.3
12-12 14:28:40 cuda_device: 0
12-12 14:28:40 checkpoint_dir: ./checkpoint
12-12 14:28:40 pretrained: False
12-12 14:28:40 batch_size: 8
12-12 14:28:40 warmup_epochs: 3
12-12 14:28:40 num_workers: 0
12-12 14:28:40 bottleneck: True
12-12 14:28:40 bottleneck_num: 256
12-12 14:28:40 last_batch: False
12-12 14:28:40 hidden_size: 1024
12-12 14:28:40 trade_off_adversarial: Step
12-12 14:28:40 lam_adversarial: 1
12-12 14:28:40 opt: adam
12-12 14:28:40 lr: 0.0003
12-12 14:28:40 momentum: 0.9
12-12 14:28:40 weight_decay: 1e-05
12-12 14:28:40 lr_scheduler: step
12-12 14:28:40 gamma: 0.1
12-12 14:28:40 steps: 150, 250
12-12 14:28:40 middle_epoch: 15
12-12 14:28:40 max_epoch: 50
12-12 14:28:40 print_step: 25
12-12 14:28:40 inconsistent: UAN
12-12 14:28:40 model_name: cnn_features_1d
12-12 14:28:40 th: 0.5
12-12 14:28:40 input_channels: 7
12-12 14:28:40 classification_label: eol_class
12-12 14:28:40 sequence_length: 32
12-12 14:28:40 cycles_per_file: 15
12-12 14:28:40 source_cycles_per_file: None
12-12 14:28:40 target_cycles_per_file: None
12-12 14:28:40 cycle_ablation: False
12-12 14:28:40 cycle_ablation_start: 5
12-12 14:28:40 cycle_ablation_step: 10
12-12 14:28:40 cycle_ablation_max: None
12-12 14:28:40 literature_context_file: ./references/joule_s2542-4351-22-00409-3.md
12-12 14:28:40 sample_random_state: 42
12-12 14:28:40 transfer_task: [[[0], [1]], [[0], [2]], [[0], [3]], [[1], [0]], [[1], [2]], [[1], [3]], [[2], [0]], [[2], [1]], [[2], [3]], [[3], [0]], [[3], [1]], [[3], [2]]]
12-12 14:28:40 source_cathode: ['HE5050', 'NMC111', 'NMC532', 'NMC622', 'NMC811']
12-12 14:28:40 target_cathode: []
12-12 14:28:40 num_classes: None
12-12 14:28:40 domain_temperature: 1.0
12-12 14:28:40 class_temperature: 10.0
12-12 14:28:40 lambda_src: 0.0
12-12 14:28:40 lambda_src_decay_patience: 5
12-12 14:28:40 lambda_src_decay_factor: 0.5
12-12 14:28:40 lambda_src_min: 0.0
12-12 14:28:40 lambda_src_warmup: 0
12-12 14:28:40 improvement_metric: accuracy
12-12 14:28:40 skip_retry: False
12-12 14:28:40 auto_select: True
12-12 14:28:40 llm_compare: True
12-12 14:28:40 llm_backend: openai
12-12 14:28:40 llm_model: None
12-12 14:28:40 llm_context: 
12-12 14:28:40 llm_ablation: False
12-12 14:28:40 llm_per_transfer: True
12-12 14:28:40 ablation_cycle_limits: 
12-12 14:28:40 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: 5Vspinel; target cathodes: HE5050, NMC111, NMC532, NMC622, NMC811.\nLabel column 'eol_class' with ~None classes; sequence length 32; 21 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.\nLiterature cues (Joule S2542-4351(22)00409-3, chemistry-sensitive ablations):\nJoule 2022 (S2542-4351(22)00409-3) key cues for chemistry-aware ablation:\n- Early-cycle capacity fade slopes and coulombic-efficiency stabilization highlight whether aging is lithium-inventory loss (graphite/anode-electrolyte limited) versus cathode structural decay (e.g., Ni-rich NMC cathodes with electrolyte oxidation risk).\n- High initial impedance growth, thick SEI signatures, or electrolyte oxidation markers flag chemistries that benefit from uncertainty-aware heads (SNGP/OpenMax) to avoid overconfident extrapolation on outlier cycles.\n- LFP/graphite pairs tend to show flatter voltage plateaus and slower early fade; Ni-rich NMC chemistries degrade faster under aggressive cycling and need stronger regularization or shorter cycle horizons in ablations.\n- Transfer between mismatched chemistries (e.g., NMC → LFP) should down-weight source loss and favor architectures that adapt quickly with self-attention or wider bottlenecks; closely matched chemistries can rely more on convolutional baselines.\nsource_train: class counts 0:19, 1:18, 2:2, 3:1, 4:1 (example label 1).\nsource_train sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, 0.03319999948143959, 0.03240000084042549, 0.010200000368058681, 0.0005000000237487257, -0.00279999990016222, -0.009200000204145908, -0.019200000911951065, -0.0222…\nsource_train flattened row glimpses: [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408]]\ntarget_train: class counts 0:22, 1:22, 2:38, 3:34, 4:39 (example label 1).\ntarget_train sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, -1.0851999521255493, -1.0836000442504883, -1.0839999914169312, -1.0845999717712402, -1.0846999883651733, -1.0848000049591064, -1.0857000350952148, -1.085700035095…\ntarget_train flattened row glimpses: [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408]]\nsource_val: class counts 0:5, 1:5, 2:1, 4:1 (example label 0).\nsource_val sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, -0.04390000179409981, -0.06509999930858612, -0.11779999732971191, -0.15219999849796295, -0.17170000076293945, -0.19760000705718994, -0.21850000321865082, -0.22759…\nsource_val flattened row glimpses: [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408]]\ntarget_val: class counts 0:9, 1:10, 2:19, 3:17, 4:19 (example label 3).\ntarget_val sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.174100041389…\ntarget_val flattened row glimpses: [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408]]", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.0005, 'dropout_hint': 0.35, 'num_classes_hint': None, 'splits': {'source_train': {'batch_shape': [8, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 1, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, 0.03319999948143959, 0.03240000084042549, 0.010200000368058681, 0.0005000000237487257, -0.00279999990016222, -0.009200000204145908, -0.019200000911951065, -0.022299999371170998, -0.024399999529123306, -0.038600001484155655, -0.041200000792741776], [-1.13100004196167, -0.13539999723434448, -0.13030000030994415, -0.149399995803833, -0.15809999406337738, -0.16099999845027924, -0.16779999434947968, -0.17669999599456787, -0.17949999868869781, -0.18289999663829803, -0.195700004696846, -0.1979999989271164]]}, 'batch_channel_mean': [0.8607000112533569, -0.2655999958515167, -0.3587999939918518, -0.2363000065088272, -0.3619999885559082, 0.9797999858856201, -0.1062999963760376, 0.09290000051259995], 'batch_channel_std': [1.059499979019165, 0.2797999978065491, 0.21699999272823334, 0.2978000044822693, 0.21809999644756317, 1.1097999811172485, 0.8382999897003174, 0.9832000136375427], 'class_distribution': {'0': 19, '1': 18, '2': 2, '3': 1, '4': 1}, 'feature_range': [-3.203935665790699, 5.1401425697203385], 'feature_global_mean': -0.016921664518018544, 'flattened_rows_head': [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408]]}, 'target_train': {'batch_shape': [8, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 1, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, -1.0851999521255493, -1.0836000442504883, -1.0839999914169312, -1.0845999717712402, -1.0846999883651733, -1.0848000049591064, -1.0857000350952148, -1.0857000350952148, -1.086400032043457, -1.0885000228881836, -1.0885000228881836], [-1.13100004196167, -1.0427000522613525, -1.0407999753952026, -1.041200041770935, -1.0419000387191772, -1.0420000553131104, -1.042199969291687, -1.0432000160217285, -1.0433000326156616, -1.0440000295639038, -1.0463000535964966, -1.0463999509811401]]}, 'batch_channel_mean': [0.8607000112533569, 0.427700012922287, 0.4577000141143799, 0.4221999943256378, 0.46560001373291016, 0.6165000200271606, -0.2732999920845032, -0.026900000870227814], 'batch_channel_std': [1.059499979019165, 1.77839994430542, 1.783400058746338, 1.8186999559402466, 1.8009999990463257, 0.8043000102043152, 0.7856000065803528, 0.6467000246047974], 'class_distribution': {'0': 22, '1': 22, '2': 38, '3': 34, '4': 39}, 'feature_range': [-9.981793547102113, 31.866555053073082], 'feature_global_mean': 0.043367023845691036, 'flattened_rows_head': [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408]]}, 'source_val': {'batch_shape': [8, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 0, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, -0.04390000179409981, -0.06509999930858612, -0.11779999732971191, -0.15219999849796295, -0.17170000076293945, -0.19760000705718994, -0.21850000321865082, -0.22759999334812164, -0.2379000037908554, -0.2531999945640564, -0.2678999900817871], [-1.13100004196167, -0.19740000367164612, -0.21279999613761902, -0.25780001282691956, -0.28760001063346863, -0.3043000102043152, -0.32760000228881836, -0.3458000123500824, -0.3540000021457672, -0.3650999963283539, -0.37959998846054077, -0.3926999866962433]]}, 'batch_channel_mean': [0.8607000112533569, -0.21899999678134918, -0.3481999933719635, -0.1915999948978424, -0.34689998626708984, 1.118899941444397, -0.11599999666213989, 0.016499999910593033], 'batch_channel_std': [1.059499979019165, 0.21160000562667847, 0.17630000412464142, 0.2303999960422516, 0.17829999327659607, 0.9266999959945679, 0.8331000208854675, 0.7063000202178955], 'class_distribution': {'0': 5, '1': 5, '2': 1, '4': 1}, 'feature_range': [-3.203935665790699, 5.1560300164083985], 'feature_global_mean': -0.010901597356338598, 'flattened_rows_head': [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408]]}, 'target_val': {'batch_shape': [8, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 3, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653], [-1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167]]}, 'batch_channel_mean': [0.8607000112533569, 0.27559998631477356, 0.2630000114440918, 0.1655000001192093, 0.2653000056743622, 0.005200000014156103, -0.3278999924659729, -0.6205999851226807], 'batch_channel_std': [1.059499979019165, 0.4851999878883362, 0.47620001435279846, 0.46309998631477356, 0.47749999165534973, 0.9384999871253967, 0.8022000193595886, 0.9699000120162964], 'class_distribution': {'0': 9, '1': 10, '2': 19, '3': 17, '4': 19}, 'feature_range': [-3.426709644883315, 32.206823553376644], 'feature_global_mean': 0.014365731412246173, 'flattened_rows_head': [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408]]}}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': ['5Vspinel'], 'target_cathodes': ['HE5050', 'NMC111', 'NMC532', 'NMC622', 'NMC811'], 'label_column': 'eol_class', 'split_used': 'source_train', 'batch_size_seen': 8, 'channels': 21, 'seq_len': 32}}
12-12 14:28:40 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 256, 'dropout': 0.35, 'learning_rate': 0.0005, 'batch_size': 8, 'lambda_src': 0.6, 'warmup_epochs': 10, 'rationale': 'The dataset has 21 channels and short sequence length (32), favoring a CNN backbone with self-attention to capture multi-cycle correlations and domain shifts between chemistries. Self-attention balances expressivity and efficiency without excessive memory use. Dropout at 0.35 mitigates overfitting on the small source domain. OpenMax or SNGP are less critical here since the label inconsistency is moderate and the focus is on transfer learning, so a standard unknown head is not used. A moderate bottleneck (256) supports adaptation to domain shift while maintaining capacity. Target cathode: HE5050 (high-energy experimental blend). Source cathodes: 5Vspinel (high-voltage spinel (manganese rich)). Source/target chemistries differ → expect domain shift. Evaluated 21 channels × 32-step windows and matched them with cnn_1d_sa capacity. Target fine-tune spans ≈155 cycles versus 41 source cycles, guiding lr=5.00e-04 and dropout=0.35.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": false,\n  "openmax": false,\n  "use_unknown_head": false,\n  "bottleneck": 256,\n  "dropout": 0.35,\n  "learning_rate": 0.0005,\n  "batch_size": 8,\n  "lambda_src": 1.0,\n  "warmup_epochs": 10,\n  "rationale": "The dataset has 21 channels and short sequence length (32), favoring a CNN backbone with self-attention to capture multi-cycle correlations and domain shifts between chemistries. Self-attention balances expressivity and efficiency without excessive memory use. Dropout at 0.35 mitigates overfitting on the small source domain. OpenMax or SNGP are less critical here since the label inconsistency is moderate and the focus is on transfer learning, so a standard unknown head is not used. A moderate bottleneck (256) supports adaptation to domain shift while maintaining capacity."\n}'}
12-12 14:28:40 llm_cfg_stamp: 20251212_131825
12-12 14:28:40 tag: no_sa_20251212_131825
12-12 14:28:40 sngp: False
12-12 14:28:40 openmax: False
12-12 14:28:40 use_unknown_head: False
12-12 14:28:40 pretrained_model_path: None
12-12 14:28:40 using 1 cpu
12-12 14:33:53 data_name: Battery_inconsistent
12-12 14:33:53 data_dir: ./my_datasets/Battery
12-12 14:33:53 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
12-12 14:33:53 normlizetype: mean-std
12-12 14:33:53 method: deterministic
12-12 14:33:53 gp_hidden_dim: 2048
12-12 14:33:53 spectral_norm_bound: 0.95
12-12 14:33:53 n_power_iterations: 1
12-12 14:33:53 nesterov: True
12-12 14:33:53 print_freq: 10
12-12 14:33:53 layers: 16
12-12 14:33:53 widen_factor: 1
12-12 14:33:53 droprate: 0.3
12-12 14:33:53 cuda_device: 0
12-12 14:33:53 checkpoint_dir: ./checkpoint
12-12 14:33:53 pretrained: False
12-12 14:33:53 batch_size: 16
12-12 14:33:53 warmup_epochs: 3
12-12 14:33:53 num_workers: 0
12-12 14:33:53 bottleneck: True
12-12 14:33:53 bottleneck_num: 256
12-12 14:33:53 last_batch: False
12-12 14:33:53 hidden_size: 1024
12-12 14:33:53 trade_off_adversarial: Step
12-12 14:33:53 lam_adversarial: 1
12-12 14:33:53 opt: adam
12-12 14:33:53 lr: 0.0003
12-12 14:33:53 momentum: 0.9
12-12 14:33:53 weight_decay: 1e-05
12-12 14:33:53 lr_scheduler: step
12-12 14:33:53 gamma: 0.1
12-12 14:33:53 steps: 150, 250
12-12 14:33:53 middle_epoch: 15
12-12 14:33:53 max_epoch: 50
12-12 14:33:53 print_step: 25
12-12 14:33:53 inconsistent: UAN
12-12 14:33:53 model_name: cnn_features_1d
12-12 14:33:53 th: 0.5
12-12 14:33:53 input_channels: 7
12-12 14:33:53 classification_label: eol_class
12-12 14:33:53 sequence_length: 32
12-12 14:33:53 cycles_per_file: 15
12-12 14:33:53 source_cycles_per_file: None
12-12 14:33:53 target_cycles_per_file: None
12-12 14:33:53 cycle_ablation: False
12-12 14:33:53 cycle_ablation_start: 5
12-12 14:33:53 cycle_ablation_step: 10
12-12 14:33:53 cycle_ablation_max: None
12-12 14:33:53 literature_context_file: ./references/joule_s2542-4351-22-00409-3.md
12-12 14:33:53 sample_random_state: 42
12-12 14:33:53 transfer_task: [[[0], [1]], [[0], [2]], [[0], [3]], [[1], [0]], [[1], [2]], [[1], [3]], [[2], [0]], [[2], [1]], [[2], [3]], [[3], [0]], [[3], [1]], [[3], [2]]]
12-12 14:33:53 source_cathode: ['HE5050', 'NMC111', 'NMC532', 'NMC622', 'NMC811']
12-12 14:33:53 target_cathode: []
12-12 14:33:53 num_classes: None
12-12 14:33:53 domain_temperature: 1.0
12-12 14:33:53 class_temperature: 10.0
12-12 14:33:53 lambda_src: 0.0
12-12 14:33:53 lambda_src_decay_patience: 5
12-12 14:33:53 lambda_src_decay_factor: 0.5
12-12 14:33:53 lambda_src_min: 0.0
12-12 14:33:53 lambda_src_warmup: 0
12-12 14:33:53 improvement_metric: accuracy
12-12 14:33:53 skip_retry: False
12-12 14:33:53 auto_select: True
12-12 14:33:53 llm_compare: True
12-12 14:33:53 llm_backend: openai
12-12 14:33:53 llm_model: None
12-12 14:33:53 llm_context: 
12-12 14:33:53 llm_ablation: False
12-12 14:33:53 llm_per_transfer: True
12-12 14:33:53 ablation_cycle_limits: 
12-12 14:33:53 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: FCG; target cathodes: HE5050, NMC111, NMC532, NMC622, NMC811.\nLabel column 'eol_class' with ~None classes; sequence length 32; 21 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.\nLiterature cues (Joule S2542-4351(22)00409-3, chemistry-sensitive ablations):\nJoule 2022 (S2542-4351(22)00409-3) key cues for chemistry-aware ablation:\n- Early-cycle capacity fade slopes and coulombic-efficiency stabilization highlight whether aging is lithium-inventory loss (graphite/anode-electrolyte limited) versus cathode structural decay (e.g., Ni-rich NMC cathodes with electrolyte oxidation risk).\n- High initial impedance growth, thick SEI signatures, or electrolyte oxidation markers flag chemistries that benefit from uncertainty-aware heads (SNGP/OpenMax) to avoid overconfident extrapolation on outlier cycles.\n- LFP/graphite pairs tend to show flatter voltage plateaus and slower early fade; Ni-rich NMC chemistries degrade faster under aggressive cycling and need stronger regularization or shorter cycle horizons in ablations.\n- Transfer between mismatched chemistries (e.g., NMC → LFP) should down-weight source loss and favor architectures that adapt quickly with self-attention or wider bottlenecks; closely matched chemistries can rely more on convolutional baselines.\nsource_train: class counts 3:6 (example label 3).\nsource_train sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, 0.6254000067710876, 0.6679999828338623, 0.6567999720573425, 0.6449999809265137, 0.6392999887466431, 0.6205999851226807, 0.6075000166893005, 0.6062999963760376, 0.…\nsource_train flattened row glimpses: [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]\ntarget_train: class counts 0:22, 1:22, 2:38, 3:34, 4:39 (example label 1).\ntarget_train sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, -0.27320000529289246, -0.2777000069618225, -0.2948000133037567, -0.3199999928474426, -0.3249000012874603, -0.3255999982357025, -0.3427000045776367, -0.35010001063…\ntarget_train flattened row glimpses: [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]\nsource_val: class counts 3:2 (example label 3).\nsource_val sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, 0.6047000288963318, 0.6389999985694885, 0.6284000277519226, 0.6177999973297119, 0.6114000082015991, 0.5934000015258789, 0.5813999772071838, 0.5791000127792358, 0.…\nsource_val flattened row glimpses: [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]\ntarget_val: class counts 0:9, 1:10, 2:19, 3:17, 4:19 (example label 3).\ntarget_val sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.140300035476…\ntarget_val flattened row glimpses: [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.0005, 'dropout_hint': 0.35, 'num_classes_hint': None, 'splits': {'source_train': {'batch_shape': [6, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 3, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, 0.6254000067710876, 0.6679999828338623, 0.6567999720573425, 0.6449999809265137, 0.6392999887466431, 0.6205999851226807, 0.6075000166893005, 0.6062999963760376, 0.589900016784668, 0.5564000010490417, 0.5540000200271606], [-1.1371999979019165, 0.6347000002861023, 0.6883999705314636, 0.6744999885559082, 0.6610000133514404, 0.6547999978065491, 0.6324999928474426, 0.6172999739646912, 0.6164000034332275, 0.595300018787384, 0.5572999715805054, 0.5551000237464905]]}, 'batch_channel_mean': [0.8607000112533569, 0.48170000314712524, 0.48350000381469727, 0.5385000109672546, 0.5008000135421753, 0.8583999872207642, -0.19740000367164612, 0.20200000703334808], 'batch_channel_std': [1.059499979019165, 0.2962000072002411, 0.29739999771118164, 0.3061000108718872, 0.2996000051498413, 0.7976999878883362, 0.7239000201225281, 0.6255999803543091], 'class_distribution': {'3': 6}, 'feature_range': [-3.2106635401609402, 3.5002185783545934], 'feature_global_mean': 0.19591510889964595, 'flattened_rows_head': [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]}, 'target_train': {'batch_shape': [8, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 1, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, -0.27320000529289246, -0.2777000069618225, -0.2948000133037567, -0.3199999928474426, -0.3249000012874603, -0.3255999982357025, -0.3427000045776367, -0.35010001063346863, -0.33809998631477356, -0.36039999127388, -0.3621000051498413], [-1.1371999979019165, -0.24789999425411224, -0.24869999289512634, -0.2685999870300293, -0.2953999936580658, -0.3005000054836273, -0.3046000003814697, -0.3230000138282776, -0.3301999866962433, -0.323199987411499, -0.34610000252723694, -0.3479999899864197]]}, 'batch_channel_mean': [0.8607000112533569, 0.22910000383853912, 0.22540000081062317, 0.23350000381469727, 0.23589999973773956, 0.2689000070095062, -0.2867000102996826, -0.015699999406933784], 'batch_channel_std': [1.059499979019165, 1.6269999742507935, 1.6416000127792358, 1.6531000137329102, 1.6542999744415283, 1.1289000511169434, 0.7103000283241272, 0.619700014591217], 'class_distribution': {'0': 22, '1': 22, '2': 38, '3': 34, '4': 39}, 'feature_range': [-9.20385614937911, 28.87870113204206], 'feature_global_mean': 0.026973963171381392, 'flattened_rows_head': [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]}, 'source_val': {'batch_shape': [2, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 3, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, 0.6047000288963318, 0.6389999985694885, 0.6284000277519226, 0.6177999973297119, 0.6114000082015991, 0.5934000015258789, 0.5813999772071838, 0.5791000127792358, 0.5594000220298767, 0.5275999903678894, 0.5281000137329102], [-1.1371999979019165, 0.6155999898910522, 0.6596999764442444, 0.6460999846458435, 0.6338000297546387, 0.6272000074386597, 0.6050000190734863, 0.590399980545044, 0.5888000130653381, 0.5641999840736389, 0.5278000235557556, 0.5281000137329102]]}, 'batch_channel_mean': [0.8607000112533569, 0.4747999906539917, 0.4763000011444092, 0.5282999873161316, 0.49230000376701355, 0.847100019454956, -0.2003999948501587, 0.2013999968767166], 'batch_channel_std': [1.059499979019165, 0.2944999933242798, 0.2957000136375427, 0.3041999936103821, 0.2978000044822693, 0.7935000061988831, 0.7203999757766724, 0.6255000233650208], 'class_distribution': {'3': 2}, 'feature_range': [-3.2106635401609402, 3.44913117424402], 'feature_global_mean': 0.1921846091904075, 'flattened_rows_head': [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]}, 'target_val': {'batch_shape': [8, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 3, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846], [-1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165]]}, 'batch_channel_mean': [0.8607000112533569, 0.18389999866485596, 0.14309999346733093, 0.08900000154972076, 0.14380000531673431, 0.11050000041723251, -0.2827000021934509, -0.5985999703407288], 'batch_channel_std': [1.059499979019165, 0.4431999921798706, 0.4374000132083893, 0.42160001397132874, 0.438400000333786, 1.0058000087738037, 0.8294000029563904, 0.9807000160217285], 'class_distribution': {'0': 9, '1': 10, '2': 19, '3': 17, '4': 19}, 'feature_range': [-3.484281475443279, 29.189441485357083], 'feature_global_mean': -0.0008096915043016523, 'flattened_rows_head': [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]}}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': ['FCG'], 'target_cathodes': ['HE5050', 'NMC111', 'NMC532', 'NMC622', 'NMC811'], 'label_column': 'eol_class', 'split_used': 'source_train', 'batch_size_seen': 6, 'channels': 21, 'seq_len': 32}}
12-12 14:33:53 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 256, 'dropout': 0.35, 'learning_rate': 0.0005, 'batch_size': 16, 'lambda_src': 0.6, 'warmup_epochs': 10, 'rationale': 'The dataset has 21 channels and short sequence length (32), favoring CNN with self-attention to capture multi-cycle correlations and chemistry-sensitive features. The domain shift between source and target cathodes suggests using SNGP and unknown heads to handle uncertainty and avoid overconfident extrapolation on outlier cycles. Dropout at 0.35 balances regularization given label inconsistency, and a moderate bottleneck of 256 provides sufficient capacity without overfitting. Batch size 16 balances memory and stable training, with warmup to adapt to transfer. Target cathode: HE5050 (high-energy experimental blend). Source cathodes: FCG (graphite focused). Source/target chemistries differ → expect domain shift. Evaluated 21 channels × 32-step windows and matched them with cnn_1d_sa capacity. Target fine-tune spans ≈155 cycles versus 6 source cycles, guiding lr=5.00e-04 and dropout=0.35.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": true,\n  "openmax": false,\n  "use_unknown_head": true,\n  "bottleneck": 256,\n  "dropout": 0.35,\n  "learning_rate": 0.0005,\n  "batch_size": 16,\n  "lambda_src": 1.5,\n  "warmup_epochs": 10,\n  "rationale": "The dataset has 21 channels and short sequence length (32), favoring CNN with self-attention to capture multi-cycle correlations and chemistry-sensitive features. The domain shift between source and target cathodes suggests using SNGP and unknown heads to handle uncertainty and avoid overconfident extrapolation on outlier cycles. Dropout at 0.35 balances regularization given label inconsistency, and a moderate bottleneck of 256 provides sufficient capacity without overfitting. Batch size 16 balances memory and stable training, with warmup to adapt to transfer."\n}'}
12-12 14:33:53 llm_cfg_stamp: 20251212_131825
12-12 14:33:53 tag: no_sa_20251212_131825
12-12 14:33:53 sngp: False
12-12 14:33:53 openmax: False
12-12 14:33:53 use_unknown_head: False
12-12 14:33:53 pretrained_model_path: None
12-12 14:33:53 using 1 cpu
12-12 15:18:03 data_name: Battery_inconsistent
12-12 15:18:03 data_dir: ./my_datasets/Battery
12-12 15:18:03 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
12-12 15:18:03 normlizetype: mean-std
12-12 15:18:03 method: deterministic
12-12 15:18:03 gp_hidden_dim: 2048
12-12 15:18:03 spectral_norm_bound: 0.95
12-12 15:18:03 n_power_iterations: 1
12-12 15:18:03 nesterov: True
12-12 15:18:03 print_freq: 10
12-12 15:18:03 layers: 16
12-12 15:18:03 widen_factor: 1
12-12 15:18:03 droprate: 0.3
12-12 15:18:03 cuda_device: 0
12-12 15:18:03 checkpoint_dir: ./checkpoint
12-12 15:18:03 pretrained: False
12-12 15:18:03 batch_size: 16
12-12 15:18:03 warmup_epochs: 3
12-12 15:18:03 num_workers: 0
12-12 15:18:03 bottleneck: True
12-12 15:18:03 bottleneck_num: 256
12-12 15:18:03 last_batch: False
12-12 15:18:03 hidden_size: 1024
12-12 15:18:03 trade_off_adversarial: Step
12-12 15:18:03 lam_adversarial: 1
12-12 15:18:03 opt: adam
12-12 15:18:03 lr: 0.0003
12-12 15:18:03 momentum: 0.9
12-12 15:18:03 weight_decay: 1e-05
12-12 15:18:03 lr_scheduler: step
12-12 15:18:03 gamma: 0.1
12-12 15:18:03 steps: 150, 250
12-12 15:18:03 middle_epoch: 15
12-12 15:18:03 max_epoch: 50
12-12 15:18:03 print_step: 25
12-12 15:18:03 inconsistent: UAN
12-12 15:18:03 model_name: cnn_features_1d
12-12 15:18:03 th: 0.5
12-12 15:18:03 input_channels: 7
12-12 15:18:03 classification_label: eol_class
12-12 15:18:03 sequence_length: 32
12-12 15:18:03 cycles_per_file: 15
12-12 15:18:03 source_cycles_per_file: None
12-12 15:18:03 target_cycles_per_file: None
12-12 15:18:03 cycle_ablation: False
12-12 15:18:03 cycle_ablation_start: 5
12-12 15:18:03 cycle_ablation_step: 10
12-12 15:18:03 cycle_ablation_max: None
12-12 15:18:03 literature_context_file: ./references/joule_s2542-4351-22-00409-3.md
12-12 15:18:03 sample_random_state: 42
12-12 15:18:03 transfer_task: [[[0], [1]], [[0], [2]], [[0], [3]], [[1], [0]], [[1], [2]], [[1], [3]], [[2], [0]], [[2], [1]], [[2], [3]], [[3], [0]], [[3], [1]], [[3], [2]]]
12-12 15:18:03 source_cathode: ['HE5050', 'NMC111', 'NMC532', 'NMC622', 'NMC811']
12-12 15:18:03 target_cathode: []
12-12 15:18:03 num_classes: None
12-12 15:18:03 domain_temperature: 1.0
12-12 15:18:03 class_temperature: 10.0
12-12 15:18:03 lambda_src: 0.0
12-12 15:18:03 lambda_src_decay_patience: 5
12-12 15:18:03 lambda_src_decay_factor: 0.5
12-12 15:18:03 lambda_src_min: 0.0
12-12 15:18:03 lambda_src_warmup: 0
12-12 15:18:03 improvement_metric: accuracy
12-12 15:18:03 skip_retry: False
12-12 15:18:03 auto_select: True
12-12 15:18:03 llm_compare: True
12-12 15:18:03 llm_backend: openai
12-12 15:18:03 llm_model: None
12-12 15:18:03 llm_context: 
12-12 15:18:03 llm_ablation: False
12-12 15:18:03 llm_per_transfer: True
12-12 15:18:03 ablation_cycle_limits: 
12-12 15:18:03 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: FCG; target cathodes: HE5050, NMC111, NMC532, NMC622, NMC811.\nLabel column 'eol_class' with ~None classes; sequence length 32; 21 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.\nLiterature cues (Joule S2542-4351(22)00409-3, chemistry-sensitive ablations):\nJoule 2022 (S2542-4351(22)00409-3) key cues for chemistry-aware ablation:\n- Early-cycle capacity fade slopes and coulombic-efficiency stabilization highlight whether aging is lithium-inventory loss (graphite/anode-electrolyte limited) versus cathode structural decay (e.g., Ni-rich NMC cathodes with electrolyte oxidation risk).\n- High initial impedance growth, thick SEI signatures, or electrolyte oxidation markers flag chemistries that benefit from uncertainty-aware heads (SNGP/OpenMax) to avoid overconfident extrapolation on outlier cycles.\n- LFP/graphite pairs tend to show flatter voltage plateaus and slower early fade; Ni-rich NMC chemistries degrade faster under aggressive cycling and need stronger regularization or shorter cycle horizons in ablations.\n- Transfer between mismatched chemistries (e.g., NMC → LFP) should down-weight source loss and favor architectures that adapt quickly with self-attention or wider bottlenecks; closely matched chemistries can rely more on convolutional baselines.\nsource_train: class counts 3:6 (example label 3).\nsource_train sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, 0.595300018787384, 0.6348000168800354, 0.625, 0.6118999719619751, 0.6071000099182129, 0.5910000205039978, 0.5741000175476074, 0.5752000212669373, 0.55400002002716…\nsource_train flattened row glimpses: [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]\ntarget_train: class counts 0:22, 1:22, 2:38, 3:34, 4:39 (example label 3).\ntarget_train sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, 0.6445000171661377, 0.6406000256538391, 0.6349999904632568, 0.6608999967575073, 0.5705999732017517, 0.5623999834060669, 0.5824000239372253, 0.478300005197525, 0.4…\ntarget_train flattened row glimpses: [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]\nsource_val: class counts 3:2 (example label 3).\nsource_val sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, 0.6047000288963318, 0.6389999985694885, 0.6284000277519226, 0.6177999973297119, 0.6114000082015991, 0.5934000015258789, 0.5813999772071838, 0.5791000127792358, 0.…\nsource_val flattened row glimpses: [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]\ntarget_val: class counts 0:9, 1:10, 2:19, 3:17, 4:19 (example label 3).\ntarget_val sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.140300035476…\ntarget_val flattened row glimpses: [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.0005, 'dropout_hint': 0.35, 'num_classes_hint': None, 'splits': {'source_train': {'batch_shape': [6, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 3, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, 0.595300018787384, 0.6348000168800354, 0.625, 0.6118999719619751, 0.6071000099182129, 0.5910000205039978, 0.5741000175476074, 0.5752000212669373, 0.5540000200271606, 0.5228000283241272, 0.5182999968528748], [-1.1371999979019165, 0.605400025844574, 0.6553000211715698, 0.6427000164985657, 0.628000020980835, 0.6229000091552734, 0.6029000282287598, 0.5839999914169312, 0.5852000117301941, 0.5598999857902527, 0.5235999822616577, 0.5192999839782715]]}, 'batch_channel_mean': [0.8607000112533569, 0.48170000314712524, 0.48350000381469727, 0.5385000109672546, 0.5008000135421753, 0.8583999872207642, -0.19740000367164612, 0.20200000703334808], 'batch_channel_std': [1.059499979019165, 0.2962000072002411, 0.29739999771118164, 0.3061000108718872, 0.2996000051498413, 0.7976999878883362, 0.7239000201225281, 0.6255999803543091], 'class_distribution': {'3': 6}, 'feature_range': [-3.2106635401609402, 3.5002185783545934], 'feature_global_mean': 0.19591510889964595, 'flattened_rows_head': [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]}, 'target_train': {'batch_shape': [8, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 3, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, 0.6445000171661377, 0.6406000256538391, 0.6349999904632568, 0.6608999967575073, 0.5705999732017517, 0.5623999834060669, 0.5824000239372253, 0.478300005197525, 0.4749000072479248, 0.5198000073432922, 0.3831999897956848], [-1.1371999979019165, 0.6136000156402588, 0.6353999972343445, 0.6359999775886536, 0.646399974822998, 0.5543000102043152, 0.54830002784729, 0.546999990940094, 0.43939998745918274, 0.43709999322891235, 0.446399986743927, 0.3077000081539154]]}, 'batch_channel_mean': [0.8607000112533569, -0.2752000093460083, -0.2874000072479248, -0.28780001401901245, -0.27959999442100525, 0.3610000014305115, -0.21279999613761902, -0.01720000058412552], 'batch_channel_std': [1.059499979019165, 0.5958999991416931, 0.583299994468689, 0.554099977016449, 0.5823000073432922, 1.042199969291687, 0.7246000170707703, 0.774399995803833], 'class_distribution': {'0': 22, '1': 22, '2': 38, '3': 34, '4': 39}, 'feature_range': [-9.20385614937911, 28.87870113204206], 'feature_global_mean': 0.026973963171381392, 'flattened_rows_head': [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]}, 'source_val': {'batch_shape': [2, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 3, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, 0.6047000288963318, 0.6389999985694885, 0.6284000277519226, 0.6177999973297119, 0.6114000082015991, 0.5934000015258789, 0.5813999772071838, 0.5791000127792358, 0.5594000220298767, 0.5275999903678894, 0.5281000137329102], [-1.1371999979019165, 0.6155999898910522, 0.6596999764442444, 0.6460999846458435, 0.6338000297546387, 0.6272000074386597, 0.6050000190734863, 0.590399980545044, 0.5888000130653381, 0.5641999840736389, 0.5278000235557556, 0.5281000137329102]]}, 'batch_channel_mean': [0.8607000112533569, 0.4747999906539917, 0.4763000011444092, 0.5282999873161316, 0.49230000376701355, 0.847100019454956, -0.2003999948501587, 0.2013999968767166], 'batch_channel_std': [1.059499979019165, 0.2944999933242798, 0.2957000136375427, 0.3041999936103821, 0.2978000044822693, 0.7935000061988831, 0.7203999757766724, 0.6255000233650208], 'class_distribution': {'3': 2}, 'feature_range': [-3.2106635401609402, 3.44913117424402], 'feature_global_mean': 0.1921846091904075, 'flattened_rows_head': [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]}, 'target_val': {'batch_shape': [8, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 3, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846], [-1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165]]}, 'batch_channel_mean': [0.8607000112533569, 0.18389999866485596, 0.14309999346733093, 0.08900000154972076, 0.14380000531673431, 0.11050000041723251, -0.2827000021934509, -0.5985999703407288], 'batch_channel_std': [1.059499979019165, 0.4431999921798706, 0.4374000132083893, 0.42160001397132874, 0.438400000333786, 1.0058000087738037, 0.8294000029563904, 0.9807000160217285], 'class_distribution': {'0': 9, '1': 10, '2': 19, '3': 17, '4': 19}, 'feature_range': [-3.484281475443279, 29.189441485357083], 'feature_global_mean': -0.0008096915043016523, 'flattened_rows_head': [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]}}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': ['FCG'], 'target_cathodes': ['HE5050', 'NMC111', 'NMC532', 'NMC622', 'NMC811'], 'label_column': 'eol_class', 'split_used': 'source_train', 'batch_size_seen': 6, 'channels': 21, 'seq_len': 32}}
12-12 15:18:03 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 256, 'dropout': 0.35, 'learning_rate': 0.0005, 'batch_size': 16, 'lambda_src': 0.6, 'warmup_epochs': 10, 'rationale': 'The dataset has 21 channels and short sequence length (32), with label inconsistency and domain shift between source and target cathodes. A CNN with self-attention balances local feature extraction and long-range dependencies, aiding transfer across chemistries. The dropout of 0.35 mitigates overfitting given the small batch size and label noise. OpenMax and SNGP are avoided due to the partial cycle windows and label inconsistency, favoring stable feature extraction over explicit open-set calibration. A moderate bottleneck (256) supports capacity without overfitting, and a warmup of 10 epochs helps stabilize transfer learning. Target cathode: HE5050 (high-energy experimental blend). Source cathodes: FCG (graphite focused). Source/target chemistries differ → expect domain shift. Evaluated 21 channels × 32-step windows and matched them with cnn_1d_sa capacity. Target fine-tune spans ≈155 cycles versus 6 source cycles, guiding lr=5.00e-04 and dropout=0.35.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": false,\n  "openmax": false,\n  "use_unknown_head": false,\n  "bottleneck": 256,\n  "dropout": 0.35,\n  "learning_rate": 0.0005,\n  "batch_size": 16,\n  "lambda_src": 1.0,\n  "warmup_epochs": 10,\n  "rationale": "The dataset has 21 channels and short sequence length (32), with label inconsistency and domain shift between source and target cathodes. A CNN with self-attention balances local feature extraction and long-range dependencies, aiding transfer across chemistries. The dropout of 0.35 mitigates overfitting given the small batch size and label noise. OpenMax and SNGP are avoided due to the partial cycle windows and label inconsistency, favoring stable feature extraction over explicit open-set calibration. A moderate bottleneck (256) supports capacity without overfitting, and a warmup of 10 epochs helps stabilize transfer learning."\n}'}
12-12 15:18:03 llm_cfg_stamp: 20251212_131825
12-12 15:18:03 tag: no_sngp_20251212_131825
12-12 15:18:03 sngp: False
12-12 15:18:03 openmax: False
12-12 15:18:03 use_unknown_head: False
12-12 15:18:03 pretrained_model_path: None
12-12 15:18:03 using 1 cpu
12-12 15:29:33 data_name: Battery_inconsistent
12-12 15:29:33 data_dir: ./my_datasets/Battery
12-12 15:29:33 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
12-12 15:29:33 normlizetype: mean-std
12-12 15:29:33 method: deterministic
12-12 15:29:33 gp_hidden_dim: 2048
12-12 15:29:33 spectral_norm_bound: 0.95
12-12 15:29:33 n_power_iterations: 1
12-12 15:29:33 nesterov: True
12-12 15:29:33 print_freq: 10
12-12 15:29:33 layers: 16
12-12 15:29:33 widen_factor: 1
12-12 15:29:33 droprate: 0.3
12-12 15:29:33 cuda_device: 0
12-12 15:29:33 checkpoint_dir: ./checkpoint
12-12 15:29:33 pretrained: False
12-12 15:29:33 batch_size: 8
12-12 15:29:33 warmup_epochs: 3
12-12 15:29:33 num_workers: 0
12-12 15:29:33 bottleneck: True
12-12 15:29:33 bottleneck_num: 256
12-12 15:29:33 last_batch: False
12-12 15:29:33 hidden_size: 1024
12-12 15:29:33 trade_off_adversarial: Step
12-12 15:29:33 lam_adversarial: 1
12-12 15:29:33 opt: adam
12-12 15:29:33 lr: 0.0003
12-12 15:29:33 momentum: 0.9
12-12 15:29:33 weight_decay: 1e-05
12-12 15:29:33 lr_scheduler: step
12-12 15:29:33 gamma: 0.1
12-12 15:29:33 steps: 150, 250
12-12 15:29:33 middle_epoch: 15
12-12 15:29:33 max_epoch: 50
12-12 15:29:33 print_step: 25
12-12 15:29:33 inconsistent: UAN
12-12 15:29:33 model_name: cnn_features_1d
12-12 15:29:33 th: 0.5
12-12 15:29:33 input_channels: 7
12-12 15:29:33 classification_label: eol_class
12-12 15:29:33 sequence_length: 32
12-12 15:29:33 cycles_per_file: 15
12-12 15:29:33 source_cycles_per_file: None
12-12 15:29:33 target_cycles_per_file: None
12-12 15:29:33 cycle_ablation: False
12-12 15:29:33 cycle_ablation_start: 5
12-12 15:29:33 cycle_ablation_step: 10
12-12 15:29:33 cycle_ablation_max: None
12-12 15:29:33 literature_context_file: ./references/joule_s2542-4351-22-00409-3.md
12-12 15:29:33 sample_random_state: 42
12-12 15:29:33 transfer_task: [[[0], [1]], [[0], [2]], [[0], [3]], [[1], [0]], [[1], [2]], [[1], [3]], [[2], [0]], [[2], [1]], [[2], [3]], [[3], [0]], [[3], [1]], [[3], [2]]]
12-12 15:29:33 source_cathode: ['HE5050', 'NMC111', 'NMC532', 'NMC622', 'NMC811']
12-12 15:29:33 target_cathode: []
12-12 15:29:33 num_classes: None
12-12 15:29:33 domain_temperature: 1.0
12-12 15:29:33 class_temperature: 10.0
12-12 15:29:33 lambda_src: 0.0
12-12 15:29:33 lambda_src_decay_patience: 5
12-12 15:29:33 lambda_src_decay_factor: 0.5
12-12 15:29:33 lambda_src_min: 0.0
12-12 15:29:33 lambda_src_warmup: 0
12-12 15:29:33 improvement_metric: accuracy
12-12 15:29:33 skip_retry: False
12-12 15:29:33 auto_select: True
12-12 15:29:33 llm_compare: True
12-12 15:29:33 llm_backend: openai
12-12 15:29:33 llm_model: None
12-12 15:29:33 llm_context: 
12-12 15:29:33 llm_ablation: False
12-12 15:29:33 llm_per_transfer: True
12-12 15:29:33 ablation_cycle_limits: 
12-12 15:29:33 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: 5Vspinel; target cathodes: HE5050, NMC111, NMC532, NMC622, NMC811.\nLabel column 'eol_class' with ~None classes; sequence length 32; 21 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.\nLiterature cues (Joule S2542-4351(22)00409-3, chemistry-sensitive ablations):\nJoule 2022 (S2542-4351(22)00409-3) key cues for chemistry-aware ablation:\n- Early-cycle capacity fade slopes and coulombic-efficiency stabilization highlight whether aging is lithium-inventory loss (graphite/anode-electrolyte limited) versus cathode structural decay (e.g., Ni-rich NMC cathodes with electrolyte oxidation risk).\n- High initial impedance growth, thick SEI signatures, or electrolyte oxidation markers flag chemistries that benefit from uncertainty-aware heads (SNGP/OpenMax) to avoid overconfident extrapolation on outlier cycles.\n- LFP/graphite pairs tend to show flatter voltage plateaus and slower early fade; Ni-rich NMC chemistries degrade faster under aggressive cycling and need stronger regularization or shorter cycle horizons in ablations.\n- Transfer between mismatched chemistries (e.g., NMC → LFP) should down-weight source loss and favor architectures that adapt quickly with self-attention or wider bottlenecks; closely matched chemistries can rely more on convolutional baselines.\nsource_train: class counts 0:19, 1:18, 2:2, 3:1, 4:1 (example label 1).\nsource_train sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, -0.016899999231100082, 0.0012000000569969416, -0.030400000512599945, -0.03880000114440918, -0.04360000044107437, -0.05829999968409538, -0.0729999989271164, -0.074…\nsource_train flattened row glimpses: [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408]]\ntarget_train: class counts 0:22, 1:22, 2:38, 3:34, 4:39 (example label 1).\ntarget_train sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, 0.3743000030517578, 0.37630000710487366, 0.3709999918937683, 0.3490999937057495, 0.34459999203681946, 0.34619998931884766, 0.32109999656677246, 0.3179000020027160…\ntarget_train flattened row glimpses: [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408]]\nsource_val: class counts 0:5, 1:5, 2:1, 4:1 (example label 0).\nsource_val sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, -0.04390000179409981, -0.06509999930858612, -0.11779999732971191, -0.15219999849796295, -0.17170000076293945, -0.19760000705718994, -0.21850000321865082, -0.22759…\nsource_val flattened row glimpses: [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408]]\ntarget_val: class counts 0:9, 1:10, 2:19, 3:17, 4:19 (example label 3).\ntarget_val sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.174100041389…\ntarget_val flattened row glimpses: [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408]]", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.0005, 'dropout_hint': 0.35, 'num_classes_hint': None, 'splits': {'source_train': {'batch_shape': [8, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 1, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, -0.016899999231100082, 0.0012000000569969416, -0.030400000512599945, -0.03880000114440918, -0.04360000044107437, -0.05829999968409538, -0.0729999989271164, -0.07419999688863754, -0.07940000295639038, -0.10239999741315842, -0.11169999837875366], [-1.13100004196167, -0.17839999496936798, -0.15639999508857727, -0.18359999358654022, -0.19140000641345978, -0.195700004696846, -0.20919999480247498, -0.22220000624656677, -0.22349999845027924, -0.22930000722408295, -0.2498999983072281, -0.25760000944137573]]}, 'batch_channel_mean': [0.8607000112533569, -0.19619999825954437, -0.3012000024318695, -0.15970000624656677, -0.301800012588501, 0.9627000093460083, -0.1062999963760376, 0.14169999957084656], 'batch_channel_std': [1.059499979019165, 0.24040000140666962, 0.17249999940395355, 0.25209999084472656, 0.17270000278949738, 1.1347999572753906, 0.824999988079071, 0.6575999855995178], 'class_distribution': {'0': 19, '1': 18, '2': 2, '3': 1, '4': 1}, 'feature_range': [-3.203935665790699, 5.1401425697203385], 'feature_global_mean': -0.016921664518018544, 'flattened_rows_head': [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408]]}, 'target_train': {'batch_shape': [8, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 1, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, 0.3743000030517578, 0.37630000710487366, 0.3709999918937683, 0.3490999937057495, 0.34459999203681946, 0.34619998931884766, 0.32109999656677246, 0.31790000200271606, 0.29190000891685486, 0.24529999494552612, 0.24169999361038208], [-1.13100004196167, 0.4562999904155731, 0.46230000257492065, 0.45500001311302185, 0.4316999912261963, 0.4268999993801117, 0.42480000853538513, 0.397599995136261, 0.39410001039505005, 0.36640000343322754, 0.31610000133514404, 0.3122999966144562]]}, 'batch_channel_mean': [0.8607000112533569, -0.04560000076889992, -0.01269999984651804, -0.06199999898672104, -0.009600000455975533, 0.1704999953508377, -0.36579999327659607, -0.0020000000949949026], 'batch_channel_std': [1.059499979019165, 0.5946999788284302, 0.5917999744415283, 0.5709999799728394, 0.5871999859809875, 0.9681000113487244, 0.6819999814033508, 0.7301999926567078], 'class_distribution': {'0': 22, '1': 22, '2': 38, '3': 34, '4': 39}, 'feature_range': [-9.981793547102113, 31.866555053073082], 'feature_global_mean': 0.043367023845691036, 'flattened_rows_head': [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408]]}, 'source_val': {'batch_shape': [8, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 0, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, -0.04390000179409981, -0.06509999930858612, -0.11779999732971191, -0.15219999849796295, -0.17170000076293945, -0.19760000705718994, -0.21850000321865082, -0.22759999334812164, -0.2379000037908554, -0.2531999945640564, -0.2678999900817871], [-1.13100004196167, -0.19740000367164612, -0.21279999613761902, -0.25780001282691956, -0.28760001063346863, -0.3043000102043152, -0.32760000228881836, -0.3458000123500824, -0.3540000021457672, -0.3650999963283539, -0.37959998846054077, -0.3926999866962433]]}, 'batch_channel_mean': [0.8607000112533569, -0.21899999678134918, -0.3481999933719635, -0.1915999948978424, -0.34689998626708984, 1.118899941444397, -0.11599999666213989, 0.016499999910593033], 'batch_channel_std': [1.059499979019165, 0.21160000562667847, 0.17630000412464142, 0.2303999960422516, 0.17829999327659607, 0.9266999959945679, 0.8331000208854675, 0.7063000202178955], 'class_distribution': {'0': 5, '1': 5, '2': 1, '4': 1}, 'feature_range': [-3.203935665790699, 5.1560300164083985], 'feature_global_mean': -0.010901597356338598, 'flattened_rows_head': [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8328, -3.2039, -3.1735, -0.1408]]}, 'target_val': {'batch_shape': [8, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 3, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653, -1.1741000413894653], [-1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167, -1.13100004196167]]}, 'batch_channel_mean': [0.8607000112533569, 0.27559998631477356, 0.2630000114440918, 0.1655000001192093, 0.2653000056743622, 0.005200000014156103, -0.3278999924659729, -0.6205999851226807], 'batch_channel_std': [1.059499979019165, 0.4851999878883362, 0.47620001435279846, 0.46309998631477356, 0.47749999165534973, 0.9384999871253967, 0.8022000193595886, 0.9699000120162964], 'class_distribution': {'0': 9, '1': 10, '2': 19, '3': 17, '4': 19}, 'feature_range': [-3.426709644883315, 32.206823553376644], 'feature_global_mean': 0.014365731412246173, 'flattened_rows_head': [[-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408], [-1.6202, -1.1741, -1.131, -1.1495, -1.1239, -1.459, -0.8317, -3.2039, -3.1735, -0.1408]]}}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': ['5Vspinel'], 'target_cathodes': ['HE5050', 'NMC111', 'NMC532', 'NMC622', 'NMC811'], 'label_column': 'eol_class', 'split_used': 'source_train', 'batch_size_seen': 8, 'channels': 21, 'seq_len': 32}}
12-12 15:29:33 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 256, 'dropout': 0.35, 'learning_rate': 0.0005, 'batch_size': 8, 'lambda_src': 0.6, 'warmup_epochs': 10, 'rationale': 'The dataset has 21 channels and short sequence length (32), favoring a CNN with self-attention to capture local and some global dependencies without excessive compute. The domain shift between source and target chemistries suggests benefit from self-attention for adaptation. Dropout at 0.35 balances overfitting risk given label inconsistency. OpenMax or SNGP are less critical since explicit unknown rejection is not emphasized, and the baseline is a deterministic CNN. Moderate bottleneck (256) supports capacity for multi-channel features while maintaining efficiency. Target cathode: HE5050 (high-energy experimental blend). Source cathodes: 5Vspinel (high-voltage spinel (manganese rich)). Source/target chemistries differ → expect domain shift. Evaluated 21 channels × 32-step windows and matched them with cnn_1d_sa capacity. Target fine-tune spans ≈155 cycles versus 41 source cycles, guiding lr=5.00e-04 and dropout=0.35.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": false,\n  "openmax": false,\n  "use_unknown_head": false,\n  "bottleneck": 256,\n  "dropout": 0.35,\n  "learning_rate": 0.0005,\n  "batch_size": 8,\n  "lambda_src": 1.0,\n  "warmup_epochs": 10,\n  "rationale": "The dataset has 21 channels and short sequence length (32), favoring a CNN with self-attention to capture local and some global dependencies without excessive compute. The domain shift between source and target chemistries suggests benefit from self-attention for adaptation. Dropout at 0.35 balances overfitting risk given label inconsistency. OpenMax or SNGP are less critical since explicit unknown rejection is not emphasized, and the baseline is a deterministic CNN. Moderate bottleneck (256) supports capacity for multi-channel features while maintaining efficiency."\n}'}
12-12 15:29:33 llm_cfg_stamp: 20251212_131825
12-12 15:29:33 tag: detcnn_20251212_131825
12-12 15:29:33 sngp: False
12-12 15:29:33 openmax: False
12-12 15:29:33 use_unknown_head: False
12-12 15:29:33 pretrained_model_path: None
12-12 15:29:33 using 1 cpu
12-12 16:46:50 data_name: Battery_inconsistent
12-12 16:46:50 data_dir: ./my_datasets/Battery
12-12 16:46:50 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
12-12 16:46:50 normlizetype: mean-std
12-12 16:46:50 method: deterministic
12-12 16:46:50 gp_hidden_dim: 2048
12-12 16:46:50 spectral_norm_bound: 0.95
12-12 16:46:50 n_power_iterations: 1
12-12 16:46:50 nesterov: True
12-12 16:46:50 print_freq: 10
12-12 16:46:50 layers: 16
12-12 16:46:50 widen_factor: 1
12-12 16:46:50 droprate: 0.3
12-12 16:46:50 cuda_device: 0
12-12 16:46:50 checkpoint_dir: ./checkpoint
12-12 16:46:50 pretrained: False
12-12 16:46:50 batch_size: 8
12-12 16:46:50 warmup_epochs: 3
12-12 16:46:50 num_workers: 0
12-12 16:46:50 bottleneck: True
12-12 16:46:50 bottleneck_num: 256
12-12 16:46:50 last_batch: False
12-12 16:46:50 hidden_size: 1024
12-12 16:46:50 trade_off_adversarial: Step
12-12 16:46:50 lam_adversarial: 1
12-12 16:46:50 opt: adam
12-12 16:46:50 lr: 0.0003
12-12 16:46:50 momentum: 0.9
12-12 16:46:50 weight_decay: 1e-05
12-12 16:46:50 lr_scheduler: step
12-12 16:46:50 gamma: 0.1
12-12 16:46:50 steps: 150, 250
12-12 16:46:50 middle_epoch: 15
12-12 16:46:50 max_epoch: 50
12-12 16:46:50 print_step: 25
12-12 16:46:50 inconsistent: UAN
12-12 16:46:50 model_name: cnn_features_1d
12-12 16:46:50 th: 0.5
12-12 16:46:50 input_channels: 7
12-12 16:46:50 classification_label: eol_class
12-12 16:46:50 sequence_length: 32
12-12 16:46:50 cycles_per_file: 15
12-12 16:46:50 source_cycles_per_file: None
12-12 16:46:50 target_cycles_per_file: None
12-12 16:46:50 cycle_ablation: False
12-12 16:46:50 cycle_ablation_start: 5
12-12 16:46:50 cycle_ablation_step: 10
12-12 16:46:50 cycle_ablation_max: None
12-12 16:46:50 literature_context_file: ./references/joule_s2542-4351-22-00409-3.md
12-12 16:46:50 sample_random_state: 42
12-12 16:46:50 transfer_task: [[[0], [1]], [[0], [2]], [[0], [3]], [[1], [0]], [[1], [2]], [[1], [3]], [[2], [0]], [[2], [1]], [[2], [3]], [[3], [0]], [[3], [1]], [[3], [2]]]
12-12 16:46:50 source_cathode: ['HE5050', 'NMC111', 'NMC532', 'NMC622', 'NMC811']
12-12 16:46:50 target_cathode: []
12-12 16:46:50 num_classes: None
12-12 16:46:50 domain_temperature: 1.0
12-12 16:46:50 class_temperature: 10.0
12-12 16:46:50 lambda_src: 0.0
12-12 16:46:50 lambda_src_decay_patience: 5
12-12 16:46:50 lambda_src_decay_factor: 0.5
12-12 16:46:50 lambda_src_min: 0.0
12-12 16:46:50 lambda_src_warmup: 0
12-12 16:46:50 improvement_metric: accuracy
12-12 16:46:50 skip_retry: False
12-12 16:46:50 auto_select: True
12-12 16:46:50 llm_compare: True
12-12 16:46:50 llm_backend: openai
12-12 16:46:50 llm_model: None
12-12 16:46:50 llm_context: 
12-12 16:46:50 llm_ablation: False
12-12 16:46:50 llm_per_transfer: True
12-12 16:46:50 ablation_cycle_limits: 
12-12 16:46:50 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: FCG; target cathodes: HE5050, NMC111, NMC532, NMC622, NMC811.\nLabel column 'eol_class' with ~None classes; sequence length 32; 21 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.\nLiterature cues (Joule S2542-4351(22)00409-3, chemistry-sensitive ablations):\nJoule 2022 (S2542-4351(22)00409-3) key cues for chemistry-aware ablation:\n- Early-cycle capacity fade slopes and coulombic-efficiency stabilization highlight whether aging is lithium-inventory loss (graphite/anode-electrolyte limited) versus cathode structural decay (e.g., Ni-rich NMC cathodes with electrolyte oxidation risk).\n- High initial impedance growth, thick SEI signatures, or electrolyte oxidation markers flag chemistries that benefit from uncertainty-aware heads (SNGP/OpenMax) to avoid overconfident extrapolation on outlier cycles.\n- LFP/graphite pairs tend to show flatter voltage plateaus and slower early fade; Ni-rich NMC chemistries degrade faster under aggressive cycling and need stronger regularization or shorter cycle horizons in ablations.\n- Transfer between mismatched chemistries (e.g., NMC → LFP) should down-weight source loss and favor architectures that adapt quickly with self-attention or wider bottlenecks; closely matched chemistries can rely more on convolutional baselines.\nsource_train: class counts 3:6 (example label 3).\nsource_train sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, 0.6254000067710876, 0.6679999828338623, 0.6567999720573425, 0.6449999809265137, 0.6392999887466431, 0.6205999851226807, 0.6075000166893005, 0.6062999963760376, 0.…\nsource_train flattened row glimpses: [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]\ntarget_train: class counts 0:22, 1:22, 2:38, 3:34, 4:39 (example label 1).\ntarget_train sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, -0.27320000529289246, -0.2777000069618225, -0.2948000133037567, -0.3199999928474426, -0.3249000012874603, -0.3255999982357025, -0.3427000045776367, -0.35010001063…\ntarget_train flattened row glimpses: [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]\nsource_val: class counts 3:2 (example label 3).\nsource_val sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, 0.6047000288963318, 0.6389999985694885, 0.6284000277519226, 0.6177999973297119, 0.6114000082015991, 0.5934000015258789, 0.5813999772071838, 0.5791000127792358, 0.…\nsource_val flattened row glimpses: [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]\ntarget_val: class counts 0:9, 1:10, 2:19, 3:17, 4:19 (example label 3).\ntarget_val sample window (first 3 ch × 12 steps): [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.140300035476…\ntarget_val flattened row glimpses: [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.0005, 'dropout_hint': 0.35, 'num_classes_hint': None, 'splits': {'source_train': {'batch_shape': [6, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 3, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, 0.6254000067710876, 0.6679999828338623, 0.6567999720573425, 0.6449999809265137, 0.6392999887466431, 0.6205999851226807, 0.6075000166893005, 0.6062999963760376, 0.589900016784668, 0.5564000010490417, 0.5540000200271606], [-1.1371999979019165, 0.6347000002861023, 0.6883999705314636, 0.6744999885559082, 0.6610000133514404, 0.6547999978065491, 0.6324999928474426, 0.6172999739646912, 0.6164000034332275, 0.595300018787384, 0.5572999715805054, 0.5551000237464905]]}, 'batch_channel_mean': [0.8607000112533569, 0.48170000314712524, 0.48350000381469727, 0.5385000109672546, 0.5008000135421753, 0.8583999872207642, -0.19740000367164612, 0.20200000703334808], 'batch_channel_std': [1.059499979019165, 0.2962000072002411, 0.29739999771118164, 0.3061000108718872, 0.2996000051498413, 0.7976999878883362, 0.7239000201225281, 0.6255999803543091], 'class_distribution': {'3': 6}, 'feature_range': [-3.2106635401609402, 3.5002185783545934], 'feature_global_mean': 0.19591510889964595, 'flattened_rows_head': [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]}, 'target_train': {'batch_shape': [8, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 1, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, -0.27320000529289246, -0.2777000069618225, -0.2948000133037567, -0.3199999928474426, -0.3249000012874603, -0.3255999982357025, -0.3427000045776367, -0.35010001063346863, -0.33809998631477356, -0.36039999127388, -0.3621000051498413], [-1.1371999979019165, -0.24789999425411224, -0.24869999289512634, -0.2685999870300293, -0.2953999936580658, -0.3005000054836273, -0.3046000003814697, -0.3230000138282776, -0.3301999866962433, -0.323199987411499, -0.34610000252723694, -0.3479999899864197]]}, 'batch_channel_mean': [0.8607000112533569, 0.22910000383853912, 0.22540000081062317, 0.23350000381469727, 0.23589999973773956, 0.2689000070095062, -0.2867000102996826, -0.015699999406933784], 'batch_channel_std': [1.059499979019165, 1.6269999742507935, 1.6416000127792358, 1.6531000137329102, 1.6542999744415283, 1.1289000511169434, 0.7103000283241272, 0.619700014591217], 'class_distribution': {'0': 22, '1': 22, '2': 38, '3': 34, '4': 39}, 'feature_range': [-9.20385614937911, 28.87870113204206], 'feature_global_mean': 0.026973963171381392, 'flattened_rows_head': [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]}, 'source_val': {'batch_shape': [2, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 3, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, 0.6047000288963318, 0.6389999985694885, 0.6284000277519226, 0.6177999973297119, 0.6114000082015991, 0.5934000015258789, 0.5813999772071838, 0.5791000127792358, 0.5594000220298767, 0.5275999903678894, 0.5281000137329102], [-1.1371999979019165, 0.6155999898910522, 0.6596999764442444, 0.6460999846458435, 0.6338000297546387, 0.6272000074386597, 0.6050000190734863, 0.590399980545044, 0.5888000130653381, 0.5641999840736389, 0.5278000235557556, 0.5281000137329102]]}, 'batch_channel_mean': [0.8607000112533569, 0.4747999906539917, 0.4763000011444092, 0.5282999873161316, 0.49230000376701355, 0.847100019454956, -0.2003999948501587, 0.2013999968767166], 'batch_channel_std': [1.059499979019165, 0.2944999933242798, 0.2957000136375427, 0.3041999936103821, 0.2978000044822693, 0.7935000061988831, 0.7203999757766724, 0.6255000233650208], 'class_distribution': {'3': 2}, 'feature_range': [-3.2106635401609402, 3.44913117424402], 'feature_global_mean': 0.1921846091904075, 'flattened_rows_head': [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]}, 'target_val': {'batch_shape': [8, 21, 32], 'channels': 21, 'seq_len': 32, 'example_label': 3, 'preview': {'channels': 3, 'timesteps': 12, 'values': [[-1.6202000379562378, -1.388700008392334, -1.1572999954223633, -0.9258000254631042, -0.6944000124931335, -0.4629000127315521, -0.23149999976158142, 0.0, 0.23149999976158142, 0.4629000127315521, 0.6944000124931335, 0.9258000254631042], [-1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846, -1.1403000354766846], [-1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165, -1.1371999979019165]]}, 'batch_channel_mean': [0.8607000112533569, 0.18389999866485596, 0.14309999346733093, 0.08900000154972076, 0.14380000531673431, 0.11050000041723251, -0.2827000021934509, -0.5985999703407288], 'batch_channel_std': [1.059499979019165, 0.4431999921798706, 0.4374000132083893, 0.42160001397132874, 0.438400000333786, 1.0058000087738037, 0.8294000029563904, 0.9807000160217285], 'class_distribution': {'0': 9, '1': 10, '2': 19, '3': 17, '4': 19}, 'feature_range': [-3.484281475443279, 29.189441485357083], 'feature_global_mean': -0.0008096915043016523, 'flattened_rows_head': [[-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409], [-1.6202, -1.1403, -1.1372, -1.1082, -1.132, -1.4587, -0.8036, -3.2107, -3.179, -0.1409]]}}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': ['FCG'], 'target_cathodes': ['HE5050', 'NMC111', 'NMC532', 'NMC622', 'NMC811'], 'label_column': 'eol_class', 'split_used': 'source_train', 'batch_size_seen': 6, 'channels': 21, 'seq_len': 32}}
12-12 16:46:50 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 256, 'dropout': 0.35, 'learning_rate': 0.0005, 'batch_size': 8, 'lambda_src': 0.6, 'warmup_epochs': 10, 'rationale': 'The dataset has 21 channels and short sequences (32 steps) with label inconsistency and domain shift between source and target cathodes. WideResNet with self-attention balances strong representation for multi-channel inputs and captures global context needed for chemistry-aware aging patterns. Using SNGP and an unknown head addresses uncertainty and outlier detection due to domain shift and label inconsistency. Dropout at 0.35 prevents overfitting on small source data, and moderate bottleneck size (256) balances capacity and regularization. Batch size 8 fits memory constraints while enabling stable training. Warmup epochs and lambda_src help gradual adaptation to target domain. Target cathode: HE5050 (high-energy experimental blend). Source cathodes: FCG (graphite focused). Source/target chemistries differ → expect domain shift. Evaluated 21 channels × 32-step windows and matched them with cnn_1d_sa capacity. Target fine-tune spans ≈155 cycles versus 6 source cycles, guiding lr=5.00e-04 and dropout=0.35.', '_provider': 'openai', '_raw': '{\n  "model_name": "WideResNet_sa",\n  "self_attention": true,\n  "sngp": true,\n  "openmax": false,\n  "use_unknown_head": true,\n  "bottleneck": 256,\n  "dropout": 0.35,\n  "learning_rate": 0.0005,\n  "batch_size": 8,\n  "lambda_src": 1.5,\n  "warmup_epochs": 10,\n  "rationale": "The dataset has 21 channels and short sequences (32 steps) with label inconsistency and domain shift between source and target cathodes. WideResNet with self-attention balances strong representation for multi-channel inputs and captures global context needed for chemistry-aware aging patterns. Using SNGP and an unknown head addresses uncertainty and outlier detection due to domain shift and label inconsistency. Dropout at 0.35 prevents overfitting on small source data, and moderate bottleneck size (256) balances capacity and regularization. Batch size 8 fits memory constraints while enabling stable training. Warmup epochs and lambda_src help gradual adaptation to target domain."\n}'}
12-12 16:46:50 llm_cfg_stamp: 20251212_131825
12-12 16:46:50 tag: sngp_wrn_sa_20251212_131825
12-12 16:46:50 sngp: False
12-12 16:46:50 openmax: False
12-12 16:46:50 use_unknown_head: False
12-12 16:46:50 pretrained_model_path: None
12-12 16:46:50 using 1 cpu

11-16 14:53:30 data_name: Battery_inconsistent
11-16 14:53:30 data_dir: ./my_datasets/Battery
11-16 14:53:30 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
11-16 14:53:30 normlizetype: mean-std
11-16 14:53:30 method: sngp
11-16 14:53:30 gp_hidden_dim: 2048
11-16 14:53:30 spectral_norm_bound: 0.95
11-16 14:53:30 n_power_iterations: 1
11-16 14:53:30 nesterov: True
11-16 14:53:30 print_freq: 10
11-16 14:53:30 layers: 16
11-16 14:53:30 widen_factor: 1
11-16 14:53:30 droprate: 0.3
11-16 14:53:30 cuda_device: 0
11-16 14:53:30 checkpoint_dir: ./checkpoint
11-16 14:53:30 pretrained: False
11-16 14:53:30 batch_size: 64
11-16 14:53:30 num_workers: 0
11-16 14:53:30 bottleneck: True
11-16 14:53:30 bottleneck_num: 128
11-16 14:53:30 last_batch: False
11-16 14:53:30 hidden_size: 1024
11-16 14:53:30 trade_off_adversarial: Step
11-16 14:53:30 lam_adversarial: 1
11-16 14:53:30 opt: adam
11-16 14:53:30 lr: 0.001
11-16 14:53:30 momentum: 0.9
11-16 14:53:30 weight_decay: 1e-05
11-16 14:53:30 lr_scheduler: step
11-16 14:53:30 gamma: 0.1
11-16 14:53:30 steps: 150, 250
11-16 14:53:30 middle_epoch: 15
11-16 14:53:30 max_epoch: 50
11-16 14:53:30 print_step: 25
11-16 14:53:30 inconsistent: UAN
11-16 14:53:30 model_name: cnn_features_1d_sa
11-16 14:53:30 th: 0.5
11-16 14:53:30 input_channels: 7
11-16 14:53:30 classification_label: eol_class
11-16 14:53:30 sequence_length: 32
11-16 14:53:30 cycles_per_file: 50
11-16 14:53:30 sample_random_state: 42
11-16 14:53:30 transfer_task: [[0], [1]]
11-16 14:53:30 source_cathode: ['5Vspinel']
11-16 14:53:30 target_cathode: []
11-16 14:53:30 num_classes: None
11-16 14:53:30 domain_temperature: 1.0
11-16 14:53:30 class_temperature: 10.0
11-16 14:53:30 lambda_src: 0.0
11-16 14:53:30 lambda_src_decay_patience: 5
11-16 14:53:30 lambda_src_decay_factor: 0.5
11-16 14:53:30 lambda_src_min: 0.0
11-16 14:53:30 lambda_src_warmup: 0
11-16 14:53:30 improvement_metric: common
11-16 14:53:30 auto_select: True
11-16 14:53:30 llm_compare: True
11-16 14:53:30 llm_backend: openai
11-16 14:53:30 llm_model: None
11-16 14:53:30 llm_context: 
11-16 14:53:30 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: (all available); target cathodes: (none specified).\nLabel column 'eol_class' with ~None classes; sequence length 32; 7 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': [], 'target_cathodes': [], 'label_column': 'eol_class'}}
11-16 14:53:30 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 0.0, 'rationale': 'The dataset has short sequences (32 steps) and 7 channels, making a 1-D CNN with self-attention ideal to capture local and some longer-range dependencies without excessive memory use. Label inconsistency suggests avoiding openmax or unknown heads to reduce tuning complexity. Moderate dropout (0.3) helps prevent overfitting given small data and attention. Batch size 64 balances GPU efficiency and stable training. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness. Enabled SNGP to stabilise open-set transfer risk detected in metadata.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": false,\n  "openmax": false,\n  "use_unknown_head": false,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 0.0,\n  "rationale": "The dataset has short sequences (32 steps) and 7 channels, making a 1-D CNN with self-attention ideal to capture local and some longer-range dependencies without excessive memory use. Label inconsistency suggests avoiding openmax or unknown heads to reduce tuning complexity. Moderate dropout (0.3) helps prevent overfitting given small data and attention. Batch size 64 balances GPU efficiency and stable training."\n}'}
11-16 14:53:30 llm_cfg_stamp: 20251116_144741
11-16 14:53:30 pretrained_model_path: None
11-16 14:53:32 using 1 cpu
11-16 14:57:46 data_name: Battery_inconsistent
11-16 14:57:46 data_dir: ./my_datasets/Battery
11-16 14:57:46 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
11-16 14:57:46 normlizetype: mean-std
11-16 14:57:46 method: sngp
11-16 14:57:46 gp_hidden_dim: 2048
11-16 14:57:46 spectral_norm_bound: 0.95
11-16 14:57:46 n_power_iterations: 1
11-16 14:57:46 nesterov: True
11-16 14:57:46 print_freq: 10
11-16 14:57:46 layers: 16
11-16 14:57:46 widen_factor: 1
11-16 14:57:46 droprate: 0.3
11-16 14:57:46 cuda_device: 0
11-16 14:57:46 checkpoint_dir: ./checkpoint
11-16 14:57:46 pretrained: False
11-16 14:57:46 batch_size: 64
11-16 14:57:46 num_workers: 0
11-16 14:57:46 bottleneck: True
11-16 14:57:46 bottleneck_num: 128
11-16 14:57:46 last_batch: False
11-16 14:57:46 hidden_size: 1024
11-16 14:57:46 trade_off_adversarial: Step
11-16 14:57:46 lam_adversarial: 1
11-16 14:57:46 opt: adam
11-16 14:57:46 lr: 0.001
11-16 14:57:46 momentum: 0.9
11-16 14:57:46 weight_decay: 1e-05
11-16 14:57:46 lr_scheduler: step
11-16 14:57:46 gamma: 0.1
11-16 14:57:46 steps: 150, 250
11-16 14:57:46 middle_epoch: 15
11-16 14:57:46 max_epoch: 50
11-16 14:57:46 print_step: 25
11-16 14:57:46 inconsistent: UAN
11-16 14:57:46 model_name: cnn_features_1d_sa
11-16 14:57:46 th: 0.5
11-16 14:57:46 input_channels: 7
11-16 14:57:46 classification_label: eol_class
11-16 14:57:46 sequence_length: 32
11-16 14:57:46 cycles_per_file: 50
11-16 14:57:46 sample_random_state: 42
11-16 14:57:46 transfer_task: [[0], [1]]
11-16 14:57:46 source_cathode: ['5Vspinel']
11-16 14:57:46 target_cathode: []
11-16 14:57:46 num_classes: None
11-16 14:57:46 domain_temperature: 1.0
11-16 14:57:46 class_temperature: 10.0
11-16 14:57:46 lambda_src: 0.0
11-16 14:57:46 lambda_src_decay_patience: 5
11-16 14:57:46 lambda_src_decay_factor: 0.5
11-16 14:57:46 lambda_src_min: 0.0
11-16 14:57:46 lambda_src_warmup: 0
11-16 14:57:46 improvement_metric: common
11-16 14:57:46 auto_select: True
11-16 14:57:46 llm_compare: True
11-16 14:57:46 llm_backend: openai
11-16 14:57:46 llm_model: None
11-16 14:57:46 llm_context: 
11-16 14:57:46 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: (all available); target cathodes: (none specified).\nLabel column 'eol_class' with ~None classes; sequence length 32; 7 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': [], 'target_cathodes': [], 'label_column': 'eol_class'}}
11-16 14:57:46 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 0.0, 'rationale': 'The dataset has short sequences (32 steps) and 7 channels, making a 1-D CNN with self-attention ideal to capture local and some longer-range dependencies without excessive memory use. Label inconsistency suggests avoiding openmax or unknown heads to reduce tuning complexity. Moderate dropout (0.3) helps prevent overfitting given small data and attention. Batch size 64 balances GPU efficiency and stable training. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness. Enabled SNGP to stabilise open-set transfer risk detected in metadata.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": false,\n  "openmax": false,\n  "use_unknown_head": false,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 0.0,\n  "rationale": "The dataset has short sequences (32 steps) and 7 channels, making a 1-D CNN with self-attention ideal to capture local and some longer-range dependencies without excessive memory use. Label inconsistency suggests avoiding openmax or unknown heads to reduce tuning complexity. Moderate dropout (0.3) helps prevent overfitting given small data and attention. Batch size 64 balances GPU efficiency and stable training."\n}'}
11-16 14:57:46 llm_cfg_stamp: 20251116_144741
11-16 14:57:46 pretrained_model_path: None
11-16 14:57:50 using 1 cpu
11-16 19:28:40 data_name: Battery_inconsistent
11-16 19:28:40 data_dir: ./my_datasets/Battery
11-16 19:28:40 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
11-16 19:28:40 normlizetype: mean-std
11-16 19:28:40 method: sngp
11-16 19:28:40 gp_hidden_dim: 2048
11-16 19:28:40 spectral_norm_bound: 0.95
11-16 19:28:40 n_power_iterations: 1
11-16 19:28:40 nesterov: True
11-16 19:28:40 print_freq: 10
11-16 19:28:40 layers: 16
11-16 19:28:40 widen_factor: 1
11-16 19:28:40 droprate: 0.3
11-16 19:28:40 cuda_device: 0
11-16 19:28:40 checkpoint_dir: ./checkpoint
11-16 19:28:40 pretrained: False
11-16 19:28:40 batch_size: 64
11-16 19:28:40 num_workers: 0
11-16 19:28:40 bottleneck: True
11-16 19:28:40 bottleneck_num: 128
11-16 19:28:40 last_batch: False
11-16 19:28:40 hidden_size: 1024
11-16 19:28:40 trade_off_adversarial: Step
11-16 19:28:40 lam_adversarial: 1
11-16 19:28:40 opt: adam
11-16 19:28:40 lr: 0.001
11-16 19:28:40 momentum: 0.9
11-16 19:28:40 weight_decay: 1e-05
11-16 19:28:40 lr_scheduler: step
11-16 19:28:40 gamma: 0.1
11-16 19:28:40 steps: 150, 250
11-16 19:28:40 middle_epoch: 15
11-16 19:28:40 max_epoch: 50
11-16 19:28:40 print_step: 25
11-16 19:28:40 inconsistent: UAN
11-16 19:28:40 model_name: cnn_features_1d_sa
11-16 19:28:40 th: 0.5
11-16 19:28:40 input_channels: 7
11-16 19:28:40 classification_label: eol_class
11-16 19:28:40 sequence_length: 32
11-16 19:28:40 cycles_per_file: 20
11-16 19:28:40 sample_random_state: 42
11-16 19:28:40 transfer_task: [[0], [1]]
11-16 19:28:40 source_cathode: ['5Vspinel']
11-16 19:28:40 target_cathode: []
11-16 19:28:40 num_classes: None
11-16 19:28:40 domain_temperature: 1.0
11-16 19:28:40 class_temperature: 10.0
11-16 19:28:40 lambda_src: 0.0
11-16 19:28:40 lambda_src_decay_patience: 5
11-16 19:28:40 lambda_src_decay_factor: 0.5
11-16 19:28:40 lambda_src_min: 0.0
11-16 19:28:40 lambda_src_warmup: 0
11-16 19:28:40 improvement_metric: common
11-16 19:28:40 auto_select: True
11-16 19:28:40 llm_compare: True
11-16 19:28:40 llm_backend: openai
11-16 19:28:40 llm_model: None
11-16 19:28:40 llm_context: 
11-16 19:28:40 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: (all available); target cathodes: (none specified).\nLabel column 'eol_class' with ~None classes; sequence length 32; 7 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': [], 'target_cathodes': [], 'label_column': 'eol_class'}}
11-16 19:28:40 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 0.0, 'rationale': 'The dataset has short sequences (32 steps) and 7 channels, making a lightweight CNN with self-attention ideal to capture local features and some long-range dependencies. Label inconsistency suggests avoiding openmax or unknown heads to reduce complexity. Dropout at 0.3 balances overfitting risk from attention layers on a small dataset. Batch size 64 and learning rate 0.001 align with hints and ensure stable training. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness. Enabled SNGP to stabilise open-set transfer risk detected in metadata.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": false,\n  "openmax": false,\n  "use_unknown_head": false,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 0.0,\n  "rationale": "The dataset has short sequences (32 steps) and 7 channels, making a lightweight CNN with self-attention ideal to capture local features and some long-range dependencies. Label inconsistency suggests avoiding openmax or unknown heads to reduce complexity. Dropout at 0.3 balances overfitting risk from attention layers on a small dataset. Batch size 64 and learning rate 0.001 align with hints and ensure stable training."\n}'}
11-16 19:28:40 llm_cfg_stamp: 20251116_192719
11-16 19:28:40 pretrained_model_path: None
11-16 19:28:42 using 1 cpu
11-16 19:29:02 data_name: Battery_inconsistent
11-16 19:29:02 data_dir: ./my_datasets/Battery
11-16 19:29:02 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
11-16 19:29:02 normlizetype: mean-std
11-16 19:29:02 method: sngp
11-16 19:29:02 gp_hidden_dim: 2048
11-16 19:29:02 spectral_norm_bound: 0.95
11-16 19:29:02 n_power_iterations: 1
11-16 19:29:02 nesterov: True
11-16 19:29:02 print_freq: 10
11-16 19:29:02 layers: 16
11-16 19:29:02 widen_factor: 1
11-16 19:29:02 droprate: 0.3
11-16 19:29:02 cuda_device: 0
11-16 19:29:02 checkpoint_dir: ./checkpoint
11-16 19:29:02 pretrained: False
11-16 19:29:02 batch_size: 64
11-16 19:29:02 num_workers: 0
11-16 19:29:02 bottleneck: True
11-16 19:29:02 bottleneck_num: 128
11-16 19:29:02 last_batch: False
11-16 19:29:02 hidden_size: 1024
11-16 19:29:02 trade_off_adversarial: Step
11-16 19:29:02 lam_adversarial: 1
11-16 19:29:02 opt: adam
11-16 19:29:02 lr: 0.001
11-16 19:29:02 momentum: 0.9
11-16 19:29:02 weight_decay: 1e-05
11-16 19:29:02 lr_scheduler: step
11-16 19:29:02 gamma: 0.1
11-16 19:29:02 steps: 150, 250
11-16 19:29:02 middle_epoch: 15
11-16 19:29:02 max_epoch: 50
11-16 19:29:02 print_step: 25
11-16 19:29:02 inconsistent: UAN
11-16 19:29:02 model_name: cnn_features_1d_sa
11-16 19:29:02 th: 0.5
11-16 19:29:02 input_channels: 7
11-16 19:29:02 classification_label: eol_class
11-16 19:29:02 sequence_length: 32
11-16 19:29:02 cycles_per_file: 20
11-16 19:29:02 sample_random_state: 42
11-16 19:29:02 transfer_task: [[0], [1]]
11-16 19:29:02 source_cathode: ['5Vspinel']
11-16 19:29:02 target_cathode: []
11-16 19:29:02 num_classes: None
11-16 19:29:02 domain_temperature: 1.0
11-16 19:29:02 class_temperature: 10.0
11-16 19:29:02 lambda_src: 0.0
11-16 19:29:02 lambda_src_decay_patience: 5
11-16 19:29:02 lambda_src_decay_factor: 0.5
11-16 19:29:02 lambda_src_min: 0.0
11-16 19:29:02 lambda_src_warmup: 0
11-16 19:29:02 improvement_metric: common
11-16 19:29:02 auto_select: True
11-16 19:29:02 llm_compare: True
11-16 19:29:02 llm_backend: openai
11-16 19:29:02 llm_model: None
11-16 19:29:02 llm_context: 
11-16 19:29:02 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: (all available); target cathodes: (none specified).\nLabel column 'eol_class' with ~None classes; sequence length 32; 7 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': [], 'target_cathodes': [], 'label_column': 'eol_class'}}
11-16 19:29:02 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 0.0, 'rationale': 'The dataset has short sequences (32 steps) and 7 channels, making a lightweight CNN with self-attention ideal to capture local features and some long-range dependencies. Label inconsistency suggests avoiding openmax or unknown heads to reduce complexity. Dropout at 0.3 balances overfitting risk from attention layers on a small dataset. Batch size 64 and learning rate 0.001 align with hints and ensure stable training. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness. Enabled SNGP to stabilise open-set transfer risk detected in metadata.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": false,\n  "openmax": false,\n  "use_unknown_head": false,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 0.0,\n  "rationale": "The dataset has short sequences (32 steps) and 7 channels, making a lightweight CNN with self-attention ideal to capture local features and some long-range dependencies. Label inconsistency suggests avoiding openmax or unknown heads to reduce complexity. Dropout at 0.3 balances overfitting risk from attention layers on a small dataset. Batch size 64 and learning rate 0.001 align with hints and ensure stable training."\n}'}
11-16 19:29:02 llm_cfg_stamp: 20251116_192719
11-16 19:29:02 pretrained_model_path: None
11-16 19:29:05 using 1 cpu
11-16 20:23:58 data_name: Battery_inconsistent
11-16 20:23:58 data_dir: ./my_datasets/Battery
11-16 20:23:58 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
11-16 20:23:58 normlizetype: mean-std
11-16 20:23:58 method: sngp
11-16 20:23:58 gp_hidden_dim: 2048
11-16 20:23:58 spectral_norm_bound: 0.95
11-16 20:23:58 n_power_iterations: 1
11-16 20:23:58 nesterov: True
11-16 20:23:58 print_freq: 10
11-16 20:23:58 layers: 16
11-16 20:23:58 widen_factor: 1
11-16 20:23:58 droprate: 0.3
11-16 20:23:58 cuda_device: 0
11-16 20:23:58 checkpoint_dir: ./checkpoint
11-16 20:23:58 pretrained: False
11-16 20:23:58 batch_size: 64
11-16 20:23:58 num_workers: 0
11-16 20:23:58 bottleneck: True
11-16 20:23:58 bottleneck_num: 128
11-16 20:23:58 last_batch: False
11-16 20:23:58 hidden_size: 1024
11-16 20:23:58 trade_off_adversarial: Step
11-16 20:23:58 lam_adversarial: 1
11-16 20:23:58 opt: adam
11-16 20:23:58 lr: 0.001
11-16 20:23:58 momentum: 0.9
11-16 20:23:58 weight_decay: 1e-05
11-16 20:23:58 lr_scheduler: step
11-16 20:23:58 gamma: 0.1
11-16 20:23:58 steps: 150, 250
11-16 20:23:58 middle_epoch: 15
11-16 20:23:58 max_epoch: 50
11-16 20:23:58 print_step: 25
11-16 20:23:58 inconsistent: UAN
11-16 20:23:58 model_name: cnn_features_1d_sa
11-16 20:23:58 th: 0.5
11-16 20:23:58 input_channels: 7
11-16 20:23:58 classification_label: eol_class
11-16 20:23:58 sequence_length: 32
11-16 20:23:58 cycles_per_file: 5
11-16 20:23:58 sample_random_state: 42
11-16 20:23:58 transfer_task: [[0], [1]]
11-16 20:23:58 source_cathode: ['5Vspinel']
11-16 20:23:58 target_cathode: []
11-16 20:23:58 num_classes: None
11-16 20:23:58 domain_temperature: 1.0
11-16 20:23:58 class_temperature: 10.0
11-16 20:23:58 lambda_src: 0.0
11-16 20:23:58 lambda_src_decay_patience: 5
11-16 20:23:58 lambda_src_decay_factor: 0.5
11-16 20:23:58 lambda_src_min: 0.0
11-16 20:23:58 lambda_src_warmup: 0
11-16 20:23:58 improvement_metric: common
11-16 20:23:58 auto_select: True
11-16 20:23:58 llm_compare: True
11-16 20:23:58 llm_backend: openai
11-16 20:23:58 llm_model: None
11-16 20:23:58 llm_context: 
11-16 20:23:58 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: (all available); target cathodes: (none specified).\nLabel column 'eol_class' with ~None classes; sequence length 32; 7 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': [], 'target_cathodes': [], 'label_column': 'eol_class'}}
11-16 20:23:58 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 0.0, 'rationale': 'The dataset has short sequences (32 steps) and 7 channels, making a 1-D CNN with self-attention ideal to capture local features and some long-range dependencies. Label inconsistency suggests avoiding openmax or unknown heads to reduce complexity and overfitting risk. Moderate bottleneck and dropout balance capacity and regularization. The learning rate and batch size align with hints for stable training on this battery dataset. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness. Enabled SNGP to stabilise open-set transfer risk detected in metadata.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": false,\n  "openmax": false,\n  "use_unknown_head": false,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 0.0,\n  "rationale": "The dataset has short sequences (32 steps) and 7 channels, making a 1-D CNN with self-attention ideal to capture local features and some long-range dependencies. Label inconsistency suggests avoiding openmax or unknown heads to reduce complexity and overfitting risk. Moderate bottleneck and dropout balance capacity and regularization. The learning rate and batch size align with hints for stable training on this battery dataset."\n}'}
11-16 20:23:58 llm_cfg_stamp: 20251116_202238
11-16 20:23:58 pretrained_model_path: None
11-16 20:24:01 using 1 cpu
11-16 20:24:14 data_name: Battery_inconsistent
11-16 20:24:14 data_dir: ./my_datasets/Battery
11-16 20:24:14 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
11-16 20:24:14 normlizetype: mean-std
11-16 20:24:14 method: sngp
11-16 20:24:14 gp_hidden_dim: 2048
11-16 20:24:14 spectral_norm_bound: 0.95
11-16 20:24:14 n_power_iterations: 1
11-16 20:24:14 nesterov: True
11-16 20:24:14 print_freq: 10
11-16 20:24:14 layers: 16
11-16 20:24:14 widen_factor: 1
11-16 20:24:14 droprate: 0.3
11-16 20:24:14 cuda_device: 0
11-16 20:24:14 checkpoint_dir: ./checkpoint
11-16 20:24:14 pretrained: False
11-16 20:24:14 batch_size: 64
11-16 20:24:14 num_workers: 0
11-16 20:24:14 bottleneck: True
11-16 20:24:14 bottleneck_num: 128
11-16 20:24:14 last_batch: False
11-16 20:24:14 hidden_size: 1024
11-16 20:24:14 trade_off_adversarial: Step
11-16 20:24:14 lam_adversarial: 1
11-16 20:24:14 opt: adam
11-16 20:24:14 lr: 0.001
11-16 20:24:14 momentum: 0.9
11-16 20:24:14 weight_decay: 1e-05
11-16 20:24:14 lr_scheduler: step
11-16 20:24:14 gamma: 0.1
11-16 20:24:14 steps: 150, 250
11-16 20:24:14 middle_epoch: 15
11-16 20:24:14 max_epoch: 50
11-16 20:24:14 print_step: 25
11-16 20:24:14 inconsistent: UAN
11-16 20:24:14 model_name: cnn_features_1d_sa
11-16 20:24:14 th: 0.5
11-16 20:24:14 input_channels: 7
11-16 20:24:14 classification_label: eol_class
11-16 20:24:14 sequence_length: 32
11-16 20:24:14 cycles_per_file: 5
11-16 20:24:14 sample_random_state: 42
11-16 20:24:14 transfer_task: [[0], [1]]
11-16 20:24:14 source_cathode: ['5Vspinel']
11-16 20:24:14 target_cathode: []
11-16 20:24:14 num_classes: None
11-16 20:24:14 domain_temperature: 1.0
11-16 20:24:14 class_temperature: 10.0
11-16 20:24:14 lambda_src: 0.0
11-16 20:24:14 lambda_src_decay_patience: 5
11-16 20:24:14 lambda_src_decay_factor: 0.5
11-16 20:24:14 lambda_src_min: 0.0
11-16 20:24:14 lambda_src_warmup: 0
11-16 20:24:14 improvement_metric: common
11-16 20:24:14 auto_select: True
11-16 20:24:14 llm_compare: True
11-16 20:24:14 llm_backend: openai
11-16 20:24:14 llm_model: None
11-16 20:24:14 llm_context: 
11-16 20:24:14 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: (all available); target cathodes: (none specified).\nLabel column 'eol_class' with ~None classes; sequence length 32; 7 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': [], 'target_cathodes': [], 'label_column': 'eol_class'}}
11-16 20:24:14 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 0.0, 'rationale': 'The dataset has short sequences (32 steps) and 7 channels, making a 1-D CNN with self-attention ideal to capture local features and some long-range dependencies. Label inconsistency suggests avoiding openmax or unknown heads to reduce complexity and overfitting risk. Moderate bottleneck and dropout balance capacity and regularization. The learning rate and batch size align with hints for stable training on this battery dataset. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness. Enabled SNGP to stabilise open-set transfer risk detected in metadata.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": false,\n  "openmax": false,\n  "use_unknown_head": false,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 0.0,\n  "rationale": "The dataset has short sequences (32 steps) and 7 channels, making a 1-D CNN with self-attention ideal to capture local features and some long-range dependencies. Label inconsistency suggests avoiding openmax or unknown heads to reduce complexity and overfitting risk. Moderate bottleneck and dropout balance capacity and regularization. The learning rate and batch size align with hints for stable training on this battery dataset."\n}'}
11-16 20:24:14 llm_cfg_stamp: 20251116_202238
11-16 20:24:14 pretrained_model_path: None
11-16 20:24:16 using 1 cpu
11-16 21:34:23 data_name: Battery_inconsistent
11-16 21:34:23 data_dir: ./my_datasets/Battery
11-16 21:34:23 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
11-16 21:34:23 normlizetype: mean-std
11-16 21:34:23 method: sngp
11-16 21:34:23 gp_hidden_dim: 2048
11-16 21:34:23 spectral_norm_bound: 0.95
11-16 21:34:23 n_power_iterations: 1
11-16 21:34:23 nesterov: True
11-16 21:34:23 print_freq: 10
11-16 21:34:23 layers: 16
11-16 21:34:23 widen_factor: 1
11-16 21:34:23 droprate: 0.3
11-16 21:34:23 cuda_device: 0
11-16 21:34:23 checkpoint_dir: ./checkpoint
11-16 21:34:23 pretrained: False
11-16 21:34:23 batch_size: 64
11-16 21:34:23 num_workers: 0
11-16 21:34:23 bottleneck: True
11-16 21:34:23 bottleneck_num: 128
11-16 21:34:23 last_batch: False
11-16 21:34:23 hidden_size: 1024
11-16 21:34:23 trade_off_adversarial: Step
11-16 21:34:23 lam_adversarial: 1
11-16 21:34:23 opt: adam
11-16 21:34:23 lr: 0.001
11-16 21:34:23 momentum: 0.9
11-16 21:34:23 weight_decay: 1e-05
11-16 21:34:23 lr_scheduler: step
11-16 21:34:23 gamma: 0.1
11-16 21:34:23 steps: 150, 250
11-16 21:34:23 middle_epoch: 15
11-16 21:34:23 max_epoch: 50
11-16 21:34:23 print_step: 25
11-16 21:34:23 inconsistent: UAN
11-16 21:34:23 model_name: cnn_features_1d_sa
11-16 21:34:23 th: 0.5
11-16 21:34:23 input_channels: 7
11-16 21:34:23 classification_label: eol_class
11-16 21:34:23 sequence_length: 32
11-16 21:34:23 cycles_per_file: 5
11-16 21:34:23 sample_random_state: 42
11-16 21:34:23 transfer_task: [[0], [1]]
11-16 21:34:23 source_cathode: ['5Vspinel']
11-16 21:34:23 target_cathode: []
11-16 21:34:23 num_classes: None
11-16 21:34:23 domain_temperature: 1.0
11-16 21:34:23 class_temperature: 10.0
11-16 21:34:23 lambda_src: 0.0
11-16 21:34:23 lambda_src_decay_patience: 5
11-16 21:34:23 lambda_src_decay_factor: 0.5
11-16 21:34:23 lambda_src_min: 0.0
11-16 21:34:23 lambda_src_warmup: 0
11-16 21:34:23 improvement_metric: common
11-16 21:34:23 auto_select: True
11-16 21:34:23 llm_compare: True
11-16 21:34:23 llm_backend: openai
11-16 21:34:23 llm_model: None
11-16 21:34:23 llm_context: 
11-16 21:34:23 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: (all available); target cathodes: (none specified).\nLabel column 'eol_class' with ~None classes; sequence length 32; 7 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': [], 'target_cathodes': [], 'label_column': 'eol_class'}}
11-16 21:34:23 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 0.0, 'rationale': 'The dataset has short sequences (32 steps) and 7 channels, suitable for a 1-D CNN with self-attention to capture local and some longer-range dependencies. Label inconsistency suggests avoiding openmax or unknown heads to reduce complexity and tuning overhead. Dropout at 0.3 balances overfitting risk given the small sequence length and label noise. Batch size 64 and learning rate 0.001 align with typical stable training for this domain. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness. Enabled SNGP to stabilise open-set transfer risk detected in metadata.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": false,\n  "openmax": false,\n  "use_unknown_head": false,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 0.0,\n  "rationale": "The dataset has short sequences (32 steps) and 7 channels, suitable for a 1-D CNN with self-attention to capture local and some longer-range dependencies. Label inconsistency suggests avoiding openmax or unknown heads to reduce complexity and tuning overhead. Dropout at 0.3 balances overfitting risk given the small sequence length and label noise. Batch size 64 and learning rate 0.001 align with typical stable training for this domain."\n}'}
11-16 21:34:23 llm_cfg_stamp: 20251116_213306
11-16 21:34:23 pretrained_model_path: None
11-16 21:34:25 using 1 cpu
11-16 21:34:38 data_name: Battery_inconsistent
11-16 21:34:38 data_dir: ./my_datasets/Battery
11-16 21:34:38 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
11-16 21:34:38 normlizetype: mean-std
11-16 21:34:38 method: sngp
11-16 21:34:38 gp_hidden_dim: 2048
11-16 21:34:38 spectral_norm_bound: 0.95
11-16 21:34:38 n_power_iterations: 1
11-16 21:34:38 nesterov: True
11-16 21:34:38 print_freq: 10
11-16 21:34:38 layers: 16
11-16 21:34:38 widen_factor: 1
11-16 21:34:38 droprate: 0.3
11-16 21:34:38 cuda_device: 0
11-16 21:34:38 checkpoint_dir: ./checkpoint
11-16 21:34:38 pretrained: False
11-16 21:34:38 batch_size: 64
11-16 21:34:38 num_workers: 0
11-16 21:34:38 bottleneck: True
11-16 21:34:38 bottleneck_num: 128
11-16 21:34:38 last_batch: False
11-16 21:34:38 hidden_size: 1024
11-16 21:34:38 trade_off_adversarial: Step
11-16 21:34:38 lam_adversarial: 1
11-16 21:34:38 opt: adam
11-16 21:34:38 lr: 0.001
11-16 21:34:38 momentum: 0.9
11-16 21:34:38 weight_decay: 1e-05
11-16 21:34:38 lr_scheduler: step
11-16 21:34:38 gamma: 0.1
11-16 21:34:38 steps: 150, 250
11-16 21:34:38 middle_epoch: 15
11-16 21:34:38 max_epoch: 50
11-16 21:34:38 print_step: 25
11-16 21:34:38 inconsistent: UAN
11-16 21:34:38 model_name: cnn_features_1d_sa
11-16 21:34:38 th: 0.5
11-16 21:34:38 input_channels: 7
11-16 21:34:38 classification_label: eol_class
11-16 21:34:38 sequence_length: 32
11-16 21:34:38 cycles_per_file: 5
11-16 21:34:38 sample_random_state: 42
11-16 21:34:38 transfer_task: [[0], [1]]
11-16 21:34:38 source_cathode: ['5Vspinel']
11-16 21:34:38 target_cathode: []
11-16 21:34:38 num_classes: None
11-16 21:34:38 domain_temperature: 1.0
11-16 21:34:38 class_temperature: 10.0
11-16 21:34:38 lambda_src: 0.0
11-16 21:34:38 lambda_src_decay_patience: 5
11-16 21:34:38 lambda_src_decay_factor: 0.5
11-16 21:34:38 lambda_src_min: 0.0
11-16 21:34:38 lambda_src_warmup: 0
11-16 21:34:38 improvement_metric: common
11-16 21:34:38 auto_select: True
11-16 21:34:38 llm_compare: True
11-16 21:34:38 llm_backend: openai
11-16 21:34:38 llm_model: None
11-16 21:34:38 llm_context: 
11-16 21:34:38 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: (all available); target cathodes: (none specified).\nLabel column 'eol_class' with ~None classes; sequence length 32; 7 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': [], 'target_cathodes': [], 'label_column': 'eol_class'}}
11-16 21:34:38 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 0.0, 'rationale': 'The dataset has short sequences (32 steps) and 7 channels, suitable for a 1-D CNN with self-attention to capture local and some longer-range dependencies. Label inconsistency suggests avoiding openmax or unknown heads to reduce complexity and tuning overhead. Dropout at 0.3 balances overfitting risk given the small sequence length and label noise. Batch size 64 and learning rate 0.001 align with typical stable training for this domain. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness. Enabled SNGP to stabilise open-set transfer risk detected in metadata.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": false,\n  "openmax": false,\n  "use_unknown_head": false,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 0.0,\n  "rationale": "The dataset has short sequences (32 steps) and 7 channels, suitable for a 1-D CNN with self-attention to capture local and some longer-range dependencies. Label inconsistency suggests avoiding openmax or unknown heads to reduce complexity and tuning overhead. Dropout at 0.3 balances overfitting risk given the small sequence length and label noise. Batch size 64 and learning rate 0.001 align with typical stable training for this domain."\n}'}
11-16 21:34:38 llm_cfg_stamp: 20251116_213306
11-16 21:34:38 pretrained_model_path: None
11-16 21:34:40 using 1 cpu
11-16 22:21:58 data_name: Battery_inconsistent
11-16 22:21:58 data_dir: ./my_datasets/Battery
11-16 22:21:58 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
11-16 22:21:58 normlizetype: mean-std
11-16 22:21:58 method: sngp
11-16 22:21:58 gp_hidden_dim: 2048
11-16 22:21:58 spectral_norm_bound: 0.95
11-16 22:21:58 n_power_iterations: 1
11-16 22:21:58 nesterov: True
11-16 22:21:58 print_freq: 10
11-16 22:21:58 layers: 16
11-16 22:21:58 widen_factor: 1
11-16 22:21:58 droprate: 0.3
11-16 22:21:58 cuda_device: 0
11-16 22:21:58 checkpoint_dir: ./checkpoint
11-16 22:21:58 pretrained: False
11-16 22:21:58 batch_size: 64
11-16 22:21:58 num_workers: 0
11-16 22:21:58 bottleneck: True
11-16 22:21:58 bottleneck_num: 128
11-16 22:21:58 last_batch: False
11-16 22:21:58 hidden_size: 1024
11-16 22:21:58 trade_off_adversarial: Step
11-16 22:21:58 lam_adversarial: 1
11-16 22:21:58 opt: adam
11-16 22:21:58 lr: 0.001
11-16 22:21:58 momentum: 0.9
11-16 22:21:58 weight_decay: 1e-05
11-16 22:21:58 lr_scheduler: step
11-16 22:21:58 gamma: 0.1
11-16 22:21:58 steps: 150, 250
11-16 22:21:58 middle_epoch: 15
11-16 22:21:58 max_epoch: 50
11-16 22:21:58 print_step: 25
11-16 22:21:58 inconsistent: UAN
11-16 22:21:58 model_name: cnn_features_1d_sa
11-16 22:21:58 th: 0.5
11-16 22:21:58 input_channels: 7
11-16 22:21:58 classification_label: eol_class
11-16 22:21:58 sequence_length: 32
11-16 22:21:58 cycles_per_file: 5
11-16 22:21:58 sample_random_state: 42
11-16 22:21:58 transfer_task: [[0], [1]]
11-16 22:21:58 source_cathode: ['5Vspinel']
11-16 22:21:58 target_cathode: []
11-16 22:21:58 num_classes: None
11-16 22:21:58 domain_temperature: 1.0
11-16 22:21:58 class_temperature: 10.0
11-16 22:21:58 lambda_src: 0.0
11-16 22:21:58 lambda_src_decay_patience: 5
11-16 22:21:58 lambda_src_decay_factor: 0.5
11-16 22:21:58 lambda_src_min: 0.0
11-16 22:21:58 lambda_src_warmup: 0
11-16 22:21:58 improvement_metric: common
11-16 22:21:58 auto_select: True
11-16 22:21:58 llm_compare: True
11-16 22:21:58 llm_backend: openai
11-16 22:21:58 llm_model: None
11-16 22:21:58 llm_context: 
11-16 22:21:58 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: (all available); target cathodes: (none specified).\nLabel column 'eol_class' with ~None classes; sequence length 32; 7 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': [], 'target_cathodes': [], 'label_column': 'eol_class'}}
11-16 22:21:58 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 0.0, 'rationale': 'The dataset has short sequences (32 steps) and 7 channels, making a 1-D CNN with self-attention suitable to capture local and some longer-range dependencies without excessive compute. Label inconsistency suggests avoiding openmax or unknown heads to reduce tuning complexity. Dropout at 0.3 balances overfitting risk due to label noise. Batch size 64 and learning rate 0.001 align with hints and stable training. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness. Enabled SNGP to stabilise open-set transfer risk detected in metadata.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": false,\n  "openmax": false,\n  "use_unknown_head": false,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 0.0,\n  "rationale": "The dataset has short sequences (32 steps) and 7 channels, making a 1-D CNN with self-attention suitable to capture local and some longer-range dependencies without excessive compute. Label inconsistency suggests avoiding openmax or unknown heads to reduce tuning complexity. Dropout at 0.3 balances overfitting risk due to label noise. Batch size 64 and learning rate 0.001 align with hints and stable training."\n}'}
11-16 22:21:58 llm_cfg_stamp: 20251116_222040
11-16 22:21:58 pretrained_model_path: None
11-16 22:22:00 using 1 cpu
11-16 22:22:12 data_name: Battery_inconsistent
11-16 22:22:12 data_dir: ./my_datasets/Battery
11-16 22:22:12 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
11-16 22:22:12 normlizetype: mean-std
11-16 22:22:12 method: sngp
11-16 22:22:12 gp_hidden_dim: 2048
11-16 22:22:12 spectral_norm_bound: 0.95
11-16 22:22:12 n_power_iterations: 1
11-16 22:22:12 nesterov: True
11-16 22:22:12 print_freq: 10
11-16 22:22:12 layers: 16
11-16 22:22:12 widen_factor: 1
11-16 22:22:12 droprate: 0.3
11-16 22:22:12 cuda_device: 0
11-16 22:22:12 checkpoint_dir: ./checkpoint
11-16 22:22:12 pretrained: False
11-16 22:22:12 batch_size: 64
11-16 22:22:12 num_workers: 0
11-16 22:22:12 bottleneck: True
11-16 22:22:12 bottleneck_num: 128
11-16 22:22:12 last_batch: False
11-16 22:22:12 hidden_size: 1024
11-16 22:22:12 trade_off_adversarial: Step
11-16 22:22:12 lam_adversarial: 1
11-16 22:22:12 opt: adam
11-16 22:22:12 lr: 0.001
11-16 22:22:12 momentum: 0.9
11-16 22:22:12 weight_decay: 1e-05
11-16 22:22:12 lr_scheduler: step
11-16 22:22:12 gamma: 0.1
11-16 22:22:12 steps: 150, 250
11-16 22:22:12 middle_epoch: 15
11-16 22:22:12 max_epoch: 50
11-16 22:22:12 print_step: 25
11-16 22:22:12 inconsistent: UAN
11-16 22:22:12 model_name: cnn_features_1d_sa
11-16 22:22:12 th: 0.5
11-16 22:22:12 input_channels: 7
11-16 22:22:12 classification_label: eol_class
11-16 22:22:12 sequence_length: 32
11-16 22:22:12 cycles_per_file: 5
11-16 22:22:12 sample_random_state: 42
11-16 22:22:12 transfer_task: [[0], [1]]
11-16 22:22:12 source_cathode: ['5Vspinel']
11-16 22:22:12 target_cathode: []
11-16 22:22:12 num_classes: None
11-16 22:22:12 domain_temperature: 1.0
11-16 22:22:12 class_temperature: 10.0
11-16 22:22:12 lambda_src: 0.0
11-16 22:22:12 lambda_src_decay_patience: 5
11-16 22:22:12 lambda_src_decay_factor: 0.5
11-16 22:22:12 lambda_src_min: 0.0
11-16 22:22:12 lambda_src_warmup: 0
11-16 22:22:12 improvement_metric: common
11-16 22:22:12 auto_select: True
11-16 22:22:12 llm_compare: True
11-16 22:22:13 llm_backend: openai
11-16 22:22:13 llm_model: None
11-16 22:22:13 llm_context: 
11-16 22:22:13 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: (all available); target cathodes: (none specified).\nLabel column 'eol_class' with ~None classes; sequence length 32; 7 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': [], 'target_cathodes': [], 'label_column': 'eol_class'}}
11-16 22:22:13 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 0.0, 'rationale': 'The dataset has short sequences (32 steps) and 7 channels, making a 1-D CNN with self-attention suitable to capture local and some longer-range dependencies without excessive compute. Label inconsistency suggests avoiding openmax or unknown heads to reduce tuning complexity. Dropout at 0.3 balances overfitting risk due to label noise. Batch size 64 and learning rate 0.001 align with hints and stable training. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness. Enabled SNGP to stabilise open-set transfer risk detected in metadata.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": false,\n  "openmax": false,\n  "use_unknown_head": false,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 0.0,\n  "rationale": "The dataset has short sequences (32 steps) and 7 channels, making a 1-D CNN with self-attention suitable to capture local and some longer-range dependencies without excessive compute. Label inconsistency suggests avoiding openmax or unknown heads to reduce tuning complexity. Dropout at 0.3 balances overfitting risk due to label noise. Batch size 64 and learning rate 0.001 align with hints and stable training."\n}'}
11-16 22:22:13 llm_cfg_stamp: 20251116_222040
11-16 22:22:13 pretrained_model_path: None
11-16 22:22:15 using 1 cpu
11-16 22:56:16 data_name: Battery_inconsistent
11-16 22:56:16 data_dir: ./my_datasets/Battery
11-16 22:56:16 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
11-16 22:56:16 normlizetype: mean-std
11-16 22:56:16 method: sngp
11-16 22:56:16 gp_hidden_dim: 2048
11-16 22:56:16 spectral_norm_bound: 0.95
11-16 22:56:16 n_power_iterations: 1
11-16 22:56:16 nesterov: True
11-16 22:56:16 print_freq: 10
11-16 22:56:16 layers: 16
11-16 22:56:16 widen_factor: 1
11-16 22:56:16 droprate: 0.3
11-16 22:56:16 cuda_device: 0
11-16 22:56:16 checkpoint_dir: ./checkpoint
11-16 22:56:16 pretrained: False
11-16 22:56:16 batch_size: 64
11-16 22:56:16 num_workers: 0
11-16 22:56:16 bottleneck: True
11-16 22:56:16 bottleneck_num: 128
11-16 22:56:16 last_batch: False
11-16 22:56:16 hidden_size: 1024
11-16 22:56:16 trade_off_adversarial: Step
11-16 22:56:16 lam_adversarial: 1
11-16 22:56:16 opt: adam
11-16 22:56:16 lr: 0.001
11-16 22:56:16 momentum: 0.9
11-16 22:56:16 weight_decay: 1e-05
11-16 22:56:16 lr_scheduler: step
11-16 22:56:16 gamma: 0.1
11-16 22:56:16 steps: 150, 250
11-16 22:56:16 middle_epoch: 15
11-16 22:56:16 max_epoch: 50
11-16 22:56:16 print_step: 25
11-16 22:56:16 inconsistent: UAN
11-16 22:56:16 model_name: cnn_features_1d_sa
11-16 22:56:16 th: 0.5
11-16 22:56:16 input_channels: 7
11-16 22:56:16 classification_label: eol_class
11-16 22:56:16 sequence_length: 32
11-16 22:56:16 cycles_per_file: 30
11-16 22:56:16 sample_random_state: 42
11-16 22:56:16 transfer_task: [[0], [1]]
11-16 22:56:16 source_cathode: ['5Vspinel']
11-16 22:56:16 target_cathode: []
11-16 22:56:16 num_classes: None
11-16 22:56:16 domain_temperature: 1.0
11-16 22:56:16 class_temperature: 10.0
11-16 22:56:16 lambda_src: 0.0
11-16 22:56:16 lambda_src_decay_patience: 5
11-16 22:56:16 lambda_src_decay_factor: 0.5
11-16 22:56:16 lambda_src_min: 0.0
11-16 22:56:16 lambda_src_warmup: 0
11-16 22:56:16 improvement_metric: common
11-16 22:56:16 auto_select: True
11-16 22:56:16 llm_compare: True
11-16 22:56:16 llm_backend: openai
11-16 22:56:16 llm_model: None
11-16 22:56:16 llm_context: 
11-16 22:56:16 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: (all available); target cathodes: (none specified).\nLabel column 'eol_class' with ~None classes; sequence length 32; 7 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': [], 'target_cathodes': [], 'label_column': 'eol_class'}}
11-16 22:56:16 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 0.0, 'rationale': 'The dataset has short sequences (32) and 7 channels, making a lightweight CNN with self-attention ideal to capture local features and some longer-range dependencies. The label inconsistency suggests avoiding openmax or unknown heads to reduce complexity and tuning overhead. Dropout at 0.3 balances overfitting risk due to label noise. Batch size 64 fits memory and training stability. This config should outperform the baseline deterministic CNN by leveraging attention without excessive capacity or open-set complexity. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness. Enabled SNGP to stabilise open-set transfer risk detected in metadata.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": false,\n  "openmax": false,\n  "use_unknown_head": false,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 0.0,\n  "rationale": "The dataset has short sequences (32) and 7 channels, making a lightweight CNN with self-attention ideal to capture local features and some longer-range dependencies. The label inconsistency suggests avoiding openmax or unknown heads to reduce complexity and tuning overhead. Dropout at 0.3 balances overfitting risk due to label noise. Batch size 64 fits memory and training stability. This config should outperform the baseline deterministic CNN by leveraging attention without excessive capacity or open-set complexity."\n}'}
11-16 22:56:16 llm_cfg_stamp: 20251116_225503
11-16 22:56:16 pretrained_model_path: None
11-16 22:56:18 using 1 cpu
11-16 22:56:31 data_name: Battery_inconsistent
11-16 22:56:31 data_dir: ./my_datasets/Battery
11-16 22:56:31 csv: ./my_datasets/Battery/battery_cycles_labeled.csv
11-16 22:56:31 normlizetype: mean-std
11-16 22:56:31 method: sngp
11-16 22:56:31 gp_hidden_dim: 2048
11-16 22:56:31 spectral_norm_bound: 0.95
11-16 22:56:31 n_power_iterations: 1
11-16 22:56:31 nesterov: True
11-16 22:56:31 print_freq: 10
11-16 22:56:31 layers: 16
11-16 22:56:31 widen_factor: 1
11-16 22:56:31 droprate: 0.3
11-16 22:56:31 cuda_device: 0
11-16 22:56:31 checkpoint_dir: ./checkpoint
11-16 22:56:31 pretrained: False
11-16 22:56:31 batch_size: 64
11-16 22:56:31 num_workers: 0
11-16 22:56:31 bottleneck: True
11-16 22:56:31 bottleneck_num: 128
11-16 22:56:31 last_batch: False
11-16 22:56:31 hidden_size: 1024
11-16 22:56:31 trade_off_adversarial: Step
11-16 22:56:31 lam_adversarial: 1
11-16 22:56:31 opt: adam
11-16 22:56:31 lr: 0.001
11-16 22:56:31 momentum: 0.9
11-16 22:56:31 weight_decay: 1e-05
11-16 22:56:31 lr_scheduler: step
11-16 22:56:31 gamma: 0.1
11-16 22:56:31 steps: 150, 250
11-16 22:56:31 middle_epoch: 15
11-16 22:56:31 max_epoch: 50
11-16 22:56:31 print_step: 25
11-16 22:56:31 inconsistent: UAN
11-16 22:56:31 model_name: cnn_features_1d_sa
11-16 22:56:31 th: 0.5
11-16 22:56:31 input_channels: 7
11-16 22:56:31 classification_label: eol_class
11-16 22:56:31 sequence_length: 32
11-16 22:56:31 cycles_per_file: 30
11-16 22:56:31 sample_random_state: 42
11-16 22:56:31 transfer_task: [[0], [1]]
11-16 22:56:31 source_cathode: ['5Vspinel']
11-16 22:56:31 target_cathode: []
11-16 22:56:31 num_classes: None
11-16 22:56:31 domain_temperature: 1.0
11-16 22:56:31 class_temperature: 10.0
11-16 22:56:31 lambda_src: 0.0
11-16 22:56:31 lambda_src_decay_patience: 5
11-16 22:56:31 lambda_src_decay_factor: 0.5
11-16 22:56:31 lambda_src_min: 0.0
11-16 22:56:31 lambda_src_warmup: 0
11-16 22:56:31 improvement_metric: common
11-16 22:56:31 auto_select: True
11-16 22:56:31 llm_compare: True
11-16 22:56:31 llm_backend: openai
11-16 22:56:31 llm_model: None
11-16 22:56:31 llm_context: 
11-16 22:56:31 llm_cfg_inputs: {'text_context': "Dataset: Argonne National Laboratory battery aging time-series with partial cycle windows.\nSource cathodes: (all available); target cathodes: (none specified).\nLabel column 'eol_class' with ~None classes; sequence length 32; 7 channels covering cycle_number, energy_charge, capacity_charge, energy_discharge, capacity_discharge, cycle_start, cycle_duration.", 'numeric_summary': {'dataset': 'Battery_inconsistent', 'sequence_length_requested': 32, 'notes': 'label_inconsistent', 'lr_hint': 0.001, 'dropout_hint': 0.3, 'num_classes_hint': None, 'splits': {}, 'dataset_variant': 'argonne_battery', 'feature_names': ['cycle_number', 'energy_charge', 'capacity_charge', 'energy_discharge', 'capacity_discharge', 'cycle_start', 'cycle_duration'], 'source_cathodes': [], 'target_cathodes': [], 'label_column': 'eol_class'}}
11-16 22:56:31 llm_cfg: {'architecture': 'cnn_1d_sa', 'model_name': 'cnn_features_1d_sa', 'self_attention': True, 'sngp': True, 'openmax': False, 'use_unknown_head': False, 'bottleneck': 128, 'dropout': 0.3, 'learning_rate': 0.001, 'batch_size': 64, 'lambda_src': 0.0, 'rationale': 'The dataset has short sequences (32) and 7 channels, making a lightweight CNN with self-attention ideal to capture local features and some longer-range dependencies. The label inconsistency suggests avoiding openmax or unknown heads to reduce complexity and tuning overhead. Dropout at 0.3 balances overfitting risk due to label noise. Batch size 64 fits memory and training stability. This config should outperform the baseline deterministic CNN by leveraging attention without excessive capacity or open-set complexity. Label inconsistency flagged → calibrated heads (SNGP/OpenMax) stay enabled for open-set robustness. Enabled SNGP to stabilise open-set transfer risk detected in metadata.', '_provider': 'openai', '_raw': '{\n  "model_name": "cnn_features_1d_sa",\n  "self_attention": true,\n  "sngp": false,\n  "openmax": false,\n  "use_unknown_head": false,\n  "bottleneck": 128,\n  "dropout": 0.3,\n  "learning_rate": 0.001,\n  "batch_size": 64,\n  "lambda_src": 0.0,\n  "rationale": "The dataset has short sequences (32) and 7 channels, making a lightweight CNN with self-attention ideal to capture local features and some longer-range dependencies. The label inconsistency suggests avoiding openmax or unknown heads to reduce complexity and tuning overhead. Dropout at 0.3 balances overfitting risk due to label noise. Batch size 64 fits memory and training stability. This config should outperform the baseline deterministic CNN by leveraging attention without excessive capacity or open-set complexity."\n}'}
11-16 22:56:31 llm_cfg_stamp: 20251116_225503
11-16 22:56:31 pretrained_model_path: None
11-16 22:56:34 using 1 cpu
